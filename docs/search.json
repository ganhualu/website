[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Information",
    "section": "",
    "text": "Ganhua Lu, PhD\nganhulu@gmail.com\n\nThe best way to contact me is at the email address above."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ganhua Lu",
    "section": "",
    "text": "Hello! My name is Ganhua Lu, and I am a Data Scientist enthusiastic about data analytics, machine learning, AI, and creating impactful data visualizations.\nI am currently a Data Scientist on the Data & Analytics Team for the County Health Rankings & Roadmaps at University of Wisconsin Population Health Institute.\nI have an M.S. in Data Science from Marquette University and Ph.D. in Mechanical Engineering from the University of Wisconsin-Milwaukee.\nYou can see my posts on data analytics using R, SAS, and Python on Posts page and find more information about my academic publications on the Publications page and my contact information."
  },
  {
    "objectID": "posts/2022-06-23-ways-to-run-sas/index.html",
    "href": "posts/2022-06-23-ways-to-run-sas/index.html",
    "title": "Ways to Run SAS",
    "section": "",
    "text": "“Even when walking in a party of no more than three, I can always be certain of learning from those I am with.” - Confucius\n\n\n\nHave a space to share code and learn from/with each other\n\n\n\n\n\nFlexible\nRelaxing\nWide range of topics"
  },
  {
    "objectID": "posts/2022-06-23-ways-to-run-sas/index.html#purpose",
    "href": "posts/2022-06-23-ways-to-run-sas/index.html#purpose",
    "title": "Ways to Run SAS",
    "section": "",
    "text": "Have a space to share code and learn from/with each other"
  },
  {
    "objectID": "posts/2022-06-23-ways-to-run-sas/index.html#format",
    "href": "posts/2022-06-23-ways-to-run-sas/index.html#format",
    "title": "Ways to Run SAS",
    "section": "",
    "text": "Flexible\nRelaxing\nWide range of topics"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html",
    "href": "posts/2022-07-07-sas-macor-1/index.html",
    "title": "SAS Macor (1)",
    "section": "",
    "text": "Don’t Repeat Yourself (DRY)\n\nMacros automatically generate SAS code\nMake more dynamic, complex, and reusable SAS programs\nReduce the effort required to read, write, and modify SAS Code"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#why-use-macros",
    "href": "posts/2022-07-07-sas-macor-1/index.html#why-use-macros",
    "title": "SAS Macor (1)",
    "section": "",
    "text": "Don’t Repeat Yourself (DRY)\n\nMacros automatically generate SAS code\nMake more dynamic, complex, and reusable SAS programs\nReduce the effort required to read, write, and modify SAS Code"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#two-building-blocks",
    "href": "posts/2022-07-07-sas-macor-1/index.html#two-building-blocks",
    "title": "SAS Macor (1)",
    "section": "Two building blocks",
    "text": "Two building blocks\n\nmacro variables\n&name - refers to a macro variable - like a standard data variable except that it does not belong to a data set and has only a single value which is always character\nmacros\n%name - refers to a macro - a piece of a program that can contain complex logic including DATA and PROC steps, and macro statements (e.g., %IF-%THEN/%ELSE and %DO-%END) - Macros often —but not always—contain macro variables"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#built-in-macro-variables",
    "href": "posts/2022-07-07-sas-macor-1/index.html#built-in-macro-variables",
    "title": "SAS Macor (1)",
    "section": "Built-In Macro Variables",
    "text": "Built-In Macro Variables\n\n\n\n\n\n\n\n\ndescription\n\n\n\n\n&SYSDATE9\n:stores the date the SAS session started (in DATE9. format, e.g., 02JUL2022)\n\n\n&SYSTIME\n:stores the time the SAS session started\n\n\n&SYSDAY\n:stores day of the week the session started\n\n\n…\n…\n\n\n\n\nproc print data = sashelp.cars;\n    where make = 'Audi' and type = 'Sports' ;\n    TITLE \"Sales as of &SYSDAY &SYSDATE\";  #&lt;&lt;\nrun;\n\n\n%PUT _AUTOMATIC_;\n\nlists all automatic macro variables created by SAS\nIt instructs the macro processor to write information to the SAS log"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#double-vs.-single-quotes",
    "href": "posts/2022-07-07-sas-macor-1/index.html#double-vs.-single-quotes",
    "title": "SAS Macor (1)",
    "section": "Double vs. Single Quotes",
    "text": "Double vs. Single Quotes\n\nIf you place a macro variable inside quotation marks, SAS resolves only macro variables that are inside double quotation marks\n\n\nproc print data = sashelp.cars;\n    where make = 'Audi' and type = 'Sports' ;\n    TITLE \"Sales as of &SYSDAY &SYSDATE\";  #&lt;&lt;\nrun;\n\n\nA macro variable reference enclosed within single quotation marks is not resolved\n\n\nproc print data = sashelp.cars;\n    where make = 'Audi' and type = 'Sports' ;\n    TITLE 'Sales as of &SYSDAY &SYSDATE';  #&lt;&lt;\nrun;"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#creating-macro-variables",
    "href": "posts/2022-07-07-sas-macor-1/index.html#creating-macro-variables",
    "title": "SAS Macor (1)",
    "section": "Creating Macro Variables",
    "text": "Creating Macro Variables\n\n%LET statement\n\n\n%let year = 2022; #&lt;&lt;\n\n\nPROC SQL\n\n\nproc sql;\nselect count(*)\n    into :total  #&lt;&lt;\n    from sashelp.cars;\nquit;\n\n\nDATA step\n\n\ndata _null_;\nCALL SYMPUTX ('today', PUT(\"&sysdate\"d,worddate22.));  #&lt;&lt;\nrun;\n%put &today;"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#assign-values-to-macro-variables",
    "href": "posts/2022-07-07-sas-macor-1/index.html#assign-values-to-macro-variables",
    "title": "SAS Macor (1)",
    "section": "Assign Values to Macro Variables",
    "text": "Assign Values to Macro Variables\n\nuse a %LET Statement\n\n\n%let n = 5;\n\ndata want;\n    do i = 1 to &n;\n    x = int(100*ranuni(0) + 1);\n    output;\n    end;\nrun;\n\ntitle \"Data Set with &n Random Numbers\";\nproc print data=want noobs;\nrun;\n\nNotice that the %LET statement comes before the DATA step. When this program runs, each occurrence of &n is replaced with a 5."
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#transfer-a-value-between-data-steps",
    "href": "posts/2022-07-07-sas-macor-1/index.html#transfer-a-value-between-data-steps",
    "title": "SAS Macor (1)",
    "section": "Transfer a Value between DATA Steps",
    "text": "Transfer a Value between DATA Steps\n\nCALL SYMPUT\nCALL SYMPUTX: Assigns a value to a macro variable, and removes both leading and trailing blanks.\n\nSuppose we want to compute the mean of MSRP from CARS data set and compare each individual value of MSRP against the mean\n\ndata have; set sashelp.cars (keep=make type origin msrp);\n  where type = 'Sports' and origin = \"Europe\";\nrun;\n\nproc means data=have noprint;\n  var msrp;\n  output out=means mean= m_msrp;\nrun;\n\ndata _null_; set means;\n  call symputx('AveMSRP', m_msrp); #&lt;&lt;\nrun;\n\ndata want; set have;\n  Per_mspr = msrp / &AveMSRP; #&lt;&lt;\n  format Per_mspr percent8.;\nrun;"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#use-proc-sql",
    "href": "posts/2022-07-07-sas-macor-1/index.html#use-proc-sql",
    "title": "SAS Macor (1)",
    "section": "Use PROC SQL",
    "text": "Use PROC SQL\n\ndata have; set sashelp.cars (keep=make type origin msrp);\n  where type = 'Sports' and origin = \"Europe\";\nrun;\n\nproc sql;\nselect avg(msrp)\n    into :AveMSRP2  #&lt;&lt;\nfrom have;\nquit;\n\ndata want2; set have;\n  Per_mspr = msrp / &AveMSRP2; #&lt;&lt;\n  format Per_mspr percent8.;\nrun;"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#macros",
    "href": "posts/2022-07-07-sas-macor-1/index.html#macros",
    "title": "SAS Macor (1)",
    "section": "Macros",
    "text": "Macros\n\nDefining a Macro\n\n\n%MACRO macro-name;\n  macro-text\n%MEND &lt;macro-name&gt;;\n\n\nCalling a Macro\n\n\n%macro-name\n\n\nExample\n\n\n%macro today;\n  %put Today is &sysday &sysdate9;\n%mend;\n\n%today"
  },
  {
    "objectID": "posts/2022-07-07-sas-macor-1/index.html#macros---example",
    "href": "posts/2022-07-07-sas-macor-1/index.html#macros---example",
    "title": "SAS Macor (1)",
    "section": "Macros - example",
    "text": "Macros - example\n\n%macro gen_rand(n, Start, End);\n  data rand_data;\n  do i = 1 to &n;\n    x = int((&End - &Start + 1)*ranuni(0) + &Start);\n    output;\n  end;\n  run;\n  proc print data=rand_data noobs;\n    title \"Randomly Generated Data Set with &n Obs\";\n    title2 \"Values are from &Start to &End\";\n  run;\n%mend gen_rand;\n\n%gen_rand(5,1,10)"
  },
  {
    "objectID": "posts/2022-07-21-sas-macro-2/index.html",
    "href": "posts/2022-07-21-sas-macro-2/index.html",
    "title": "SAS Macro (2)",
    "section": "",
    "text": "v140 Social Associations\nAssign Libref’s\n\n\nlibname datalib 'P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification\\SAS data sets';\nlibname geo 'P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification';\nlibname vintage 'P:\\CH-Ranking\\Data\\2022\\2 Cleaned Data ready for Calculation or Verification';\nlibname calcs 'P:\\CH-Ranking\\Data\\2023\\3 Data calculated needs checking';\n\n\nUse two macro variables to replace 2023 and 2022, respectively\n\n\n%let curYr = 2023; /* current year */\n%let preYr = 2022; /* previous year */\n\nlibname datalib \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or Verification\\SAS data sets\";\nlibname geo \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or Verification\";\nlibname vintage \"P:\\CH-Ranking\\Data\\&preYr\\2 Cleaned Data ready for Calculation or Verification\";\nlibname calcs \"P:\\CH-Ranking\\Data\\&curYr\\3 Data calculated needs checking\";\n\n\nNote: Use double quotes!!\n\n\n\n\n\n 73         %let curYr = 2023; /* current year */\n 74         %let preYr = 2022; /* previous year */\n 75         \n 76         libname datalib \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or\n 76       ! Verification\\SAS data sets\";\n NOTE: Libref DATALIB was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or \n       Verification\\SAS data sets \n 77         libname geo \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or\n 77       ! Verification\";\n NOTE: Libref GEO was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification \n 78         libname vintage \"P:\\CH-Ranking\\Data\\&preYr\\2 Cleaned Data ready for Calculation or\n 78       ! Verification\";\n NOTE: Libref VINTAGE was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2022\\2 Cleaned Data ready for Calculation or Verification \n 79         libname calcs \"P:\\CH-Ranking\\Data\\&curYr\\3 Data calculated needs checking\";\n NOTE: Libref CALCS was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\3 Data calculated needs checking \n\n\n\n\n\n\n%eval()\n\n\n%let year = 2023;\n\nlibname datalib \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or Verification\\SAS data sets\";\nlibname geo \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or Verification\";\nlibname vintage \"P:\\CH-Ranking\\Data\\%eval(&year-1)\\2 Cleaned Data ready for Calculation or Verification\"; \nlibname calcs \"P:\\CH-Ranking\\Data\\&year\\3 Data calculated needs checking\";\n\n\n\n\n\n\n 73         %let year = 2023;\n 74         \n 75         libname datalib \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or\n 75       ! Verification\\SAS data sets\";\n NOTE: Libref DATALIB was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or \n       Verification\\SAS data sets\n 76         libname geo \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or\n 76       ! Verification\";\n NOTE: Libref GEO was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification\n 77         libname vintage \"P:\\CH-Ranking\\Data\\%eval(&year-1)\\2 Cleaned Data ready for Calculation\n 77       !  or Verification\";\n NOTE: Libref VINTAGE was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2022\\2 Cleaned Data ready for Calculation or Verification \n 78         libname calcs \"P:\\CH-Ranking\\Data\\&year\\3 Data calculated needs checking\";\n NOTE: Libref CALCS was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\3 Data calculated needs checking\n\n\n\nSimply using &year-1 won’t work\n\n\nlibname vintage \"P:\\CH-Ranking\\Data\\&year-1\\2 Cleaned Data ready for Calculation or Verification\";"
  },
  {
    "objectID": "posts/2022-07-21-sas-macro-2/index.html#example-create-macro-variable",
    "href": "posts/2022-07-21-sas-macro-2/index.html#example-create-macro-variable",
    "title": "SAS Macro (2)",
    "section": "",
    "text": "v140 Social Associations\nAssign Libref’s\n\n\nlibname datalib 'P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification\\SAS data sets';\nlibname geo 'P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification';\nlibname vintage 'P:\\CH-Ranking\\Data\\2022\\2 Cleaned Data ready for Calculation or Verification';\nlibname calcs 'P:\\CH-Ranking\\Data\\2023\\3 Data calculated needs checking';\n\n\nUse two macro variables to replace 2023 and 2022, respectively\n\n\n%let curYr = 2023; /* current year */\n%let preYr = 2022; /* previous year */\n\nlibname datalib \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or Verification\\SAS data sets\";\nlibname geo \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or Verification\";\nlibname vintage \"P:\\CH-Ranking\\Data\\&preYr\\2 Cleaned Data ready for Calculation or Verification\";\nlibname calcs \"P:\\CH-Ranking\\Data\\&curYr\\3 Data calculated needs checking\";\n\n\nNote: Use double quotes!!\n\n\n\n\n\n 73         %let curYr = 2023; /* current year */\n 74         %let preYr = 2022; /* previous year */\n 75         \n 76         libname datalib \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or\n 76       ! Verification\\SAS data sets\";\n NOTE: Libref DATALIB was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or \n       Verification\\SAS data sets \n 77         libname geo \"P:\\CH-Ranking\\Data\\&curYr\\2 Cleaned Data ready for Calculation or\n 77       ! Verification\";\n NOTE: Libref GEO was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification \n 78         libname vintage \"P:\\CH-Ranking\\Data\\&preYr\\2 Cleaned Data ready for Calculation or\n 78       ! Verification\";\n NOTE: Libref VINTAGE was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2022\\2 Cleaned Data ready for Calculation or Verification \n 79         libname calcs \"P:\\CH-Ranking\\Data\\&curYr\\3 Data calculated needs checking\";\n NOTE: Libref CALCS was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\3 Data calculated needs checking \n\n\n\n\n\n\n%eval()\n\n\n%let year = 2023;\n\nlibname datalib \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or Verification\\SAS data sets\";\nlibname geo \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or Verification\";\nlibname vintage \"P:\\CH-Ranking\\Data\\%eval(&year-1)\\2 Cleaned Data ready for Calculation or Verification\"; \nlibname calcs \"P:\\CH-Ranking\\Data\\&year\\3 Data calculated needs checking\";\n\n\n\n\n\n\n 73         %let year = 2023;\n 74         \n 75         libname datalib \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or\n 75       ! Verification\\SAS data sets\";\n NOTE: Libref DATALIB was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or \n       Verification\\SAS data sets\n 76         libname geo \"P:\\CH-Ranking\\Data\\&year\\2 Cleaned Data ready for Calculation or\n 76       ! Verification\";\n NOTE: Libref GEO was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\2 Cleaned Data ready for Calculation or Verification\n 77         libname vintage \"P:\\CH-Ranking\\Data\\%eval(&year-1)\\2 Cleaned Data ready for Calculation\n 77       !  or Verification\";\n NOTE: Libref VINTAGE was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2022\\2 Cleaned Data ready for Calculation or Verification \n 78         libname calcs \"P:\\CH-Ranking\\Data\\&year\\3 Data calculated needs checking\";\n NOTE: Libref CALCS was successfully assigned as follows: \n       Engine:        V9 \n       Physical Name: P:\\CH-Ranking\\Data\\2023\\3 Data calculated needs checking\n\n\n\nSimply using &year-1 won’t work\n\n\nlibname vintage \"P:\\CH-Ranking\\Data\\&year-1\\2 Cleaned Data ready for Calculation or Verification\";"
  },
  {
    "objectID": "posts/2022-07-21-sas-macro-2/index.html#macros",
    "href": "posts/2022-07-21-sas-macro-2/index.html#macros",
    "title": "SAS Macro (2)",
    "section": "Macros",
    "text": "Macros\n\nDefining a Macro\n\n\n%MACRO macro-name;\n  macro-text\n%MEND &lt;macro-name&gt;;\n\n\nCalling a Macro\n\n\n%macro-name\n\n\nExample\n\n\n%macro today;\n  %put Today is &sysday &sysdate9;\n%mend;\n\n%today\n\n\n\nMacro Parameters\n\nThe ability to pass parameters to macros make them much more useful\n\n\n%MACRO macro-name (parameter-list);\n macro-text\n%MEND macro-name;\n\n\nPositional parameters\n\n\n%macro prnt(var, sum); \n   proc print data = srhigh;\n      var &var;\n      sum &sum;\n   run;\n%mend prnt;\n\n\nKeyword parameters\n\n\n%macro finance(yvar = expenses, xvar = division); \n   proc plot data = yearend;\n      plot &yvar * &xvar;\n   run;\n%mend finance;\n\n\nPositional parameters must come before keyword parameters\n\n\n\n\nProcedure to write a macro\n\nStart with a working SAS program\nGeneralize with macro variables\nCreate a macro\nTest"
  },
  {
    "objectID": "posts/2022-07-21-sas-macro-2/index.html#example-create-a-macro",
    "href": "posts/2022-07-21-sas-macro-2/index.html#example-create-a-macro",
    "title": "SAS Macro (2)",
    "section": "Example: Create a macro",
    "text": "Example: Create a macro\n\nv140 Social Associations; data source: Census, County Business Patterns (CBP)\nData link: https://www.census.gov/data/datasets/2020/econ/cbp/2020-cbp.html\nDownload “Complete County File”, “Complete State File”, “Complete U.S. File”\nTask: import three text files for county, state, and us, respectively\n\n\n\nStart with a working SAS program\n\n/* county data */\nPROC IMPORT\n  OUT = cbpcounties20 \n  DATAFILE = \"C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20co.txt\"\n  DBMS=dlm REPLACE; delimiter=\",\"; getnames=yes; guessingrows=500;\nrun;\n/* state data */\nPROC IMPORT\n  OUT= cbpstates20 \n  DATAFILE= \"C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20st.txt\"\n  DBMS=dlm REPLACE;delimiter=\",\"; getnames=yes; guessingrows=500;\nrun;\n/* us data */\nPROC IMPORT\n  OUT= cbpnational20 \n  DATAFILE= \"C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20us.txt\"\n  DBMS=dlm REPLACE; delimiter=\",\"; getnames=yes; guessingrows=500;\nrun;\n\n\nThe three Import procedures are different from two things: output data set and input file path\n\n\n\n\nGeneralize with macro variables\n\n%let dataOut = cbpcounties20;\n%let filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20co.txt;\n\nPROC IMPORT\n  OUT= &dataOut \n  DATAFILE= \"&filePath\" \n  DBMS=dlm REPLACE; delimiter=\",\"; getnames=yes; guessingrows=500;\nrun;\n\n\n\n\nCreate a macro\n\n%MACRO read_txt(dataOut=, filePath=);\n    PROC IMPORT\n    OUT = &dataOut\n    DATAFILE = \"&filePath\"\n    DBMS=dlm REPLACE; delimiter=\",\"; getnames=yes; guessingrows=500;\n    run;\n%MEND read_txt;\n\n\nUse macro\n\n\n/* county data */\n%read_txt(dataOut = cbpcounties20, \n          filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20co.txt);\n/* state data */\n%read_txt(dataOut = cbpstates20, \n          filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20st.txt);\n/* us data */\n%read_txt(dataOut = cbpnational20, \n          filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20us.txt);\n\n\n\n\nTest\n\nMPRNT\n\nMPRINT system option: print the resolved statements from macros in the SAS log.\nCan be very useful for debugging\n\n\n\n/* Turn on: */\nOPTIONS MPRINT; \n\n/* Turn off: */\nOPTIONS NOMPRINT;   \n\n\noptions mprint;\n%read_txt(dataOut = cbpcounties20, \n          filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20co.txt);\n\n\ncheck SAS log:\n\n\n 73         %read_txt(dataOut = cbpcounties20,\n 74         filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20co.txt);\n MPRINT(READ_TXT):   PROC IMPORT OUT = cbpcounties20 DATAFILE = \n \"C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20co.txt\" DBMS=dlm REPLACE;\n MPRINT(READ_TXT):   ADLM;\n MPRINT(READ_TXT):   delimiter=\",\";\n MPRINT(READ_TXT):   getnames=yes;\n MPRINT(READ_TXT):   guessingrows=500;\n MPRINT(READ_TXT):   run;\n\n\nSYMBOLGEN: writes the results of resolving macro variable references to the SAS log\nMLOGIC: helpful when we have %DO loops and or %IF-%THEN-%ELSE statements\nCompare data sets obtained with and without using macro"
  },
  {
    "objectID": "posts/2022-07-21-sas-macro-2/index.html#example---another-look",
    "href": "posts/2022-07-21-sas-macro-2/index.html#example---another-look",
    "title": "SAS Macro (2)",
    "section": "Example - Another look",
    "text": "Example - Another look\n\n/* county data */\n%read_txt( dataOut = cbpcounties20, \n           filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20co.txt); \n/* state data */\n%read_txt(dataOut = cbpstates20, \n          filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20st.txt);\n/* us data */\n%read_txt(dataOut = cbpnational20, \n          filePath = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\cbp20us.txt);\n\n\n“filePath”: too long :(\n\n\nModified\n\n%let filDir = C:\\01_CHRR\\SAS\\CHR_Ranking_2023\\v140_ini\\raw\\; \n\n%MACRO read_txt2(dataOut=, fileName=);\n    PROC IMPORT\n    OUT = &dataOut\n    DATAFILE = \"&filDir.&fileName..txt\" \n    DBMS=dlm REPLACE; delimiter=\",\"; getnames=yes;guessingrows=500;\n    run;\n%MEND read_txt2;\n\n–\n\n/* county data */\n%read_txt2(dataOut = cbpcounties20, fileName = cbp20co);\n/* state data */\n%read_txt2(dataOut = cbpstates20, fileName = cbp20st);\n/* us data */\n%read_txt(dataOut = cbpnational20, fileName = cbp20us);"
  },
  {
    "objectID": "posts/2022-07-21-sas-macro-2/index.html#more-to-explore",
    "href": "posts/2022-07-21-sas-macro-2/index.html#more-to-explore",
    "title": "SAS Macro (2)",
    "section": "More to explore …",
    "text": "More to explore …\n\nWays to make a macro available to current programs\n\nto compile a macro and use it for current sessions\nto store it as snippet in SAS Studio\nto save it as a permanent macro and then use a %INCLUDE statement\nto call it through the autocall facility\nto store it as a compiled macro\n\nConditional logic\n\n\n        %IF condition %THEN %DO;\n         action;\n        %END;\n\n\nLoops\n\n%do loop\n%do … %while\n%do … %until\n\n\n\n              %do i = 1% to 10; \n                %put %eval(&i**2);\n              %end;\n\n\n……"
  },
  {
    "objectID": "posts/2022-07-21-sas-macro-2/index.html#references",
    "href": "posts/2022-07-21-sas-macro-2/index.html#references",
    "title": "SAS Macro (2)",
    "section": "References",
    "text": "References\n\nhttps://myweb.uiowa.edu/pbreheny/misc/macros.pdf\nhttps://support.sas.com/resources/papers/proceedings/proceedings/sugi29/243-29.pdf\nhttps://www.listendata.com/2015/12/sas-macros-made-easy.html"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html",
    "href": "posts/2022-08-03-proc-sql/index.html",
    "title": "PROC SQL",
    "section": "",
    "text": "Structured Query Language (SQL): a standardized language used to retrieve and update data stored in relational tables (or databases).(A relational database is a collection of tables)\nAll the Relational Database Management Systems (RDMS) (e.g., Oracle, Microsoft SQL Server, MySQL, Postgres, SQL Server, etc.) use SQL as their standard database language\nA table in SQL is simply another term for a SAS data set\n\n\n\n\n\n\nSAS Term\nSQL Equivalent\n\n\n\n\nData set\nTable\n\n\nObservation\nRow\n\n\nVariable\nColumn\n\n\n\n\n\n\n\n\nPROC SQL offers an alternative to the DATA step for querying and combining SAS data sets"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#sql",
    "href": "posts/2022-08-03-proc-sql/index.html#sql",
    "title": "PROC SQL",
    "section": "",
    "text": "Structured Query Language (SQL): a standardized language used to retrieve and update data stored in relational tables (or databases).(A relational database is a collection of tables)\nAll the Relational Database Management Systems (RDMS) (e.g., Oracle, Microsoft SQL Server, MySQL, Postgres, SQL Server, etc.) use SQL as their standard database language\nA table in SQL is simply another term for a SAS data set\n\n\n\n\n\n\nSAS Term\nSQL Equivalent\n\n\n\n\nData set\nTable\n\n\nObservation\nRow\n\n\nVariable\nColumn\n\n\n\n\n\n\n\n\nPROC SQL offers an alternative to the DATA step for querying and combining SAS data sets"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#sql-query-basics",
    "href": "posts/2022-08-03-proc-sql/index.html#sql-query-basics",
    "title": "PROC SQL",
    "section": "SQL query basics",
    "text": "SQL query basics\n\nQuery: a request for information from a table or tables\nQuery result: a report, another table, a view\nExample:\nemployee table\n\n\n\n\n\n\nssn\nlast_name\nfirst_name\ndepartment\nsalary\n\n\n\n\n666666666\nSmith\nJosh\nAdmin\n50,000\n\n\n222222222\nFord\nTom\nMarketing\n40,000\n\n\n333333333\nGrace\nLily\nDatabase\n55,000\n\n\n\n\n\n\n\nI would like to \\(\\color{blue}{\\text{SELECT}}\\) last name, department, and salary \\(\\color{blue}{\\text{FROM}}\\) the employee table \\(\\color{blue}{\\text{WHERE}}\\) the employee’s salary is greater than 45,000.\n\nSELECT last_name, department, salary\nFROM employee\nWHERE salary&gt;45000;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#proc-sql",
    "href": "posts/2022-08-03-proc-sql/index.html#proc-sql",
    "title": "PROC SQL",
    "section": "PROC SQL",
    "text": "PROC SQL\n\nPROC SQL;\n  ...\n\nQUIT;  \n\n\nPROC SQL; \n  SELECT last_name, department, salary\n  FROM employee\n  WHERE salary&gt;45000;\nQUIT;  \n\n\nSELECT statement syntax\n\n\nPROC SQL options;\n SELECT column(s)\n   FROM table-name\n   WHERE expression\n   GROUP BY column(s)\n   HAVING expression\n   ORDER BY column(s);\nQUIT;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#example-subset-variables-columns-select-obs.-rows",
    "href": "posts/2022-08-03-proc-sql/index.html#example-subset-variables-columns-select-obs.-rows",
    "title": "PROC SQL",
    "section": "Example: Subset variables (columns) + Select obs. (rows)",
    "text": "Example: Subset variables (columns) + Select obs. (rows)\n\nOverview of sashelp.cars data set\n\n\n/* cars data set */\nproc print data = sashelp.cars;\nrun;\n\n\nGet a list of sports cars made by Audi; keep columns Make, Model, Type, Origin, MSRP\n\n\n/* Data Step */\ndata audi_sports_cars; \nset sashelp.cars;\n  where type = \"Sports\" and make = \"Audi\";\n  keep Make Model Type Origin MSRP;\nrun;\n\n\n/* PROC SQL */\nproc sql;\n    select Make, Model, Type, Origin, MSRP\n    from sashelp.cars\n    where type = \"Sports\" and make = \"Audi\";\nquit;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#summary-functions",
    "href": "posts/2022-08-03-proc-sql/index.html#summary-functions",
    "title": "PROC SQL",
    "section": "Summary functions",
    "text": "Summary functions\n\nAVG/MEAN\nCOUNT/FREQ/N\nSUM\nMAX\nMIN\nNMISS\nSTD\nVAR\n…"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#sum-by-group",
    "href": "posts/2022-08-03-proc-sql/index.html#sum-by-group",
    "title": "PROC SQL",
    "section": "Sum by Group",
    "text": "Sum by Group\n\ngroup by one column\n\n\nproc sql;\n    select var1, sum(var2) as sum_var2\n    from my_data\n    group by var1;\nquit;\n\n\ngroup by multiple columns\n\n\nproc sql;\n    select var1, var2, sum(var3) as sum_var3\n    from my_data\n    group by var1, var2;\nquit;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#example-soccer-teams",
    "href": "posts/2022-08-03-proc-sql/index.html#example-soccer-teams",
    "title": "PROC SQL",
    "section": "Example: soccer teams",
    "text": "Example: soccer teams\n\nA table with goals scored by 12 players from two soccer teams\n\n\ndata have;\n    input id team $ position $ goals;\n    datalines;\n1 A Forward 15\n2 A Forward 12\n3 A Forward 29\n4 A Center 11\n5 A Center 9\n6 A Center 16\n7 B Forward 25\n8 B Forward 20\n9 B Forward 34\n10 B Center 19\n11 B Center 3\n12 B Center 8\n;\nrun;\n\n\nsome queries\n\n/* total goals by both teams */\nproc sql;\n    select sum(goals) as sum_goals\n    from have;\nquit;\n\n\n/* goals by each team */\nproc sql;\n    select team, sum(goals) as sum_goals\n    from have\n    group by team; \nquit;\n\n\n/* goals by team and position*/\nproc sql;\n    select team, position, sum(goals) as sum_goals\n    from have\n    group by team, position; \nquit;\n\n\n/*calculate sum and avg of goals by team, grouped by team and position*/\nproc sql;\n    select team, position, sum(goals) as sum_goals, avg(goals) as avg_goals\n    from have\n    group by team, position;\nquit;\n    \n/* round to 1 decimal point */\nproc sql;\n    select team, position, sum(goals) as sum_goals, \n            round(avg(goals), 0.1) as avg_goals\n    from have\n    group by team, position;\nquit;    \n\n\n/* find unique/distinct values within a column */\nproc sql;\n    select distinct team\n    from have;\nquit;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#create-table-from-a-query-result",
    "href": "posts/2022-08-03-proc-sql/index.html#create-table-from-a-query-result",
    "title": "PROC SQL",
    "section": "Create table from a query result",
    "text": "Create table from a query result\n\nPROC SQL;\nCREATE TABLE table-name AS \n  SELECT column(s)\n    FROM table-name\n    &lt;WHERE expression&gt;\n    ...;\nQUIT;\n\n\nExample\n\n\n/* create table: goals by team and position*/\nproc sql;\ncreate table want as \n    select team, position, sum(goals) as sum_goals\n    from have\n    group by team, position;\nquit;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#dictionary-tables",
    "href": "posts/2022-08-03-proc-sql/index.html#dictionary-tables",
    "title": "PROC SQL",
    "section": "Dictionary Tables",
    "text": "Dictionary Tables\n\nDICTIONARY tables (views) have information related to currently defined libnames, table names, column names and attributes, formats, and so on.\nColumn names: \\(\\color{blue}{{SASHELP \\rightarrow VCOLUMN}}\\)\n\n\n/* get column names within a table*/\nproc sql ;\n create table dict_col as\n     select name\n     from dictionary.columns\n     where  libname = 'WORK' \n                and memname = 'ADD_DATA_SET_NAME_HERE' \n     order by name;\nquit;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#example-nchs-bridged-race-population-estimates",
    "href": "posts/2022-08-03-proc-sql/index.html#example-nchs-bridged-race-population-estimates",
    "title": "PROC SQL",
    "section": "Example: NCHS Bridged-Race Population Estimates",
    "text": "Example: NCHS Bridged-Race Population Estimates\n\nEstimates of resident population by year, county, single-year of age (0, 1, 2, …, 85 years and over), bridged-race category, Hispanic origin, and sex\nhttps://www.cdc.gov/nchs/nvss/bridged_race/data_documentation.htm\nVintage 2020: pcen_v2020_y20.sas7bdat.zip (~22 MB)\nGoal: get county and state population under 75 by age groups of 10\n\n\nlibname pop \"C:\\temp\";\ndata pop20; set pop.pcen_v2020_y20; run; \n\n\nAdd statecode, countycode columns; assign age groups\n\ndata pop20_1 (drop=age st_fips co_fips); \nset pop20 (keep=co_fips st_fips age pop);\nstatecode = put(st_fips, z2.); \ncountycode = put(co_fips, z3.);\nif age&lt;75; /*Subset: keep rows with age &lt; 75 */\nif age=0 then age_cat=0;\nelse if age&lt;=14 then age_cat=1;\nelse if age&lt;=24 then age_cat=2;\nelse if age&lt;=34 then age_cat=3;\nelse if age&lt;=44 then age_cat=4;\nelse if age&lt;=54 then age_cat=5;\nelse if age&lt;=64 then age_cat=6;\nelse if age&lt;=74 then age_cat=7;\nelse age_cat=99;\nrun;\n\n\n\nUse DATA step\n\n/* pop by county and age_cat; data step */\nproc sort data=pop20_1; by statecode countycode age_cat; run;\n\ndata pop_cnt; set pop20_1;\nby statecode countycode age_cat;\nif first.age_cat then pop_cnt = 0;  * (1) Initialization;\npop_cnt + pop;                                * (2) Accumulation;\nif last.age_cat then output;            * (3) Output;\ndrop pop;\nrun;\n\n\n/* pop by state and age_cat */\nproc sort data=pop20_1; by statecode age_cat; run;\n\ndata pop_st; set pop20_1;\nby statecode age_cat;\nif first.age_cat then pop_st = 0;   * (1) Initialization;\npop_st + pop;                                   * (2) Accumulation;\nif last.age_cat then output;            * (3) Output;\ndrop countycode pop;\nrun;\n\n\n\nUse PROC SQL\n\n/* pop by county and age_cat; proc sql */\nproc sql;   \ncreate table pop20_cnt_sql as\n    select distinct statecode, countycode, age_cat, sum(pop) as pop\n    from pop20_1\n    group by statecode, countycode, age_cat;\nquit;\n\n\n/* pop by state and age_cat; proc sql */\nproc sql;   \ncreate table pop20_st_sql as\n    select distinct statecode, age_cat, sum(pop) as pop\n    from pop20_1\n    group by statecode, age_cat;    \nquit;"
  },
  {
    "objectID": "posts/2022-08-03-proc-sql/index.html#more-to-explore",
    "href": "posts/2022-08-03-proc-sql/index.html#more-to-explore",
    "title": "PROC SQL",
    "section": "More to explore",
    "text": "More to explore\n\nCRUD (create, read, update, delete)\nSQL Joins: Inner Join, Left Join, Right Join, Full Join\nPROC SQL + SAS MACRO\n…\nReferences:\nSAS® 9.4 SQL Procedure User’s Guide\nPROC SQL for DATA Step Die-Hards\nAN INTRODUCTION TO THE SQL PROCEDURE\nIntroduction to Proc SQL\nLESSON 1 : PROC SQL TUTORIAL FOR BEGINNERS (20 EXAMPLES)"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html",
    "href": "posts/2022-08-18-introduction-to-r/index.html",
    "title": "Introduction to R",
    "section": "",
    "text": "Script\nR markdown\nApp"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#demo",
    "href": "posts/2022-08-18-introduction-to-r/index.html#demo",
    "title": "Introduction to R",
    "section": "",
    "text": "Script\nR markdown\nApp"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#plan",
    "href": "posts/2022-08-18-introduction-to-r/index.html#plan",
    "title": "Introduction to R",
    "section": "Plan",
    "text": "Plan\n\nGeneral introduction\nStyles\nVisualization\nR markdown\nGit"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#why-r",
    "href": "posts/2022-08-18-introduction-to-r/index.html#why-r",
    "title": "Introduction to R",
    "section": "Why R?",
    "text": "Why R?\n\nProgramming language developed by statisticians for data analysis\nFree and open-source\nEasy to get help: A large, growing, and active community of R users\nNumerous resources for learning and asking questions\nExcellent graphics and visualization tools, even dynamic Shiny\nBroad range of data analysis tools/packages\nEasy to generate reproducible reports\nEasy to integrate with other tools"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#the-r-console",
    "href": "posts/2022-08-18-introduction-to-r/index.html#the-r-console",
    "title": "Introduction to R",
    "section": "The R console",
    "text": "The R console\n\nBasic interaction with R is through typing in the console\nThis is the terminal or command-line interface\n\n\n\n\n\n\nRStudio is highly recommended\n\nDownload R: https://cran.r-project.org/\nThen download RStudio: http://www.rstudio.com/"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#rstudio-is-an-ide-for-r",
    "href": "posts/2022-08-18-introduction-to-r/index.html#rstudio-is-an-ide-for-r",
    "title": "Introduction to R",
    "section": "RStudio is an IDE for R",
    "text": "RStudio is an IDE for R\nRStudio has 4 main windows (i.e., ‘panes’):\n\nSource\nConsole/Terminal/Render/Jobs\nEnvironment/History\nFiles/Plots/Packages/Help/Viewer"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#stylesdialectsfashionsflavorssyntaxes",
    "href": "posts/2022-08-18-introduction-to-r/index.html#stylesdialectsfashionsflavorssyntaxes",
    "title": "Introduction to R",
    "section": "Styles/Dialects/Fashions/Flavors/Syntaxes",
    "text": "Styles/Dialects/Fashions/Flavors/Syntaxes\n\nBase R: what you get when you open up R for the first time; stable\nTidyverse: readability and flexibility\ndata.table: fast and concise"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#base-r-vs-tidyverse",
    "href": "posts/2022-08-18-introduction-to-r/index.html#base-r-vs-tidyverse",
    "title": "Introduction to R",
    "section": "Base R vs Tidyverse",
    "text": "Base R vs Tidyverse\n\ngapminder dataset\n\n\nlibrary(gapminder)\ndim(gapminder)\n\n[1] 1704    6\n\nhead(gapminder)\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\n\n\nsubset rows for United States - base R:\n\n\ngapminder[gapminder$country == \"United States\", ]\n\n# A tibble: 12 × 6\n   country       continent  year lifeExp       pop gdpPercap\n   &lt;fct&gt;         &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 United States Americas   1952    68.4 157553000    13990.\n 2 United States Americas   1957    69.5 171984000    14847.\n 3 United States Americas   1962    70.2 186538000    16173.\n 4 United States Americas   1967    70.8 198712000    19530.\n 5 United States Americas   1972    71.3 209896000    21806.\n 6 United States Americas   1977    73.4 220239000    24073.\n 7 United States Americas   1982    74.6 232187835    25010.\n 8 United States Americas   1987    75.0 242803533    29884.\n 9 United States Americas   1992    76.1 256894189    32004.\n10 United States Americas   1997    76.8 272911760    35767.\n11 United States Americas   2002    77.3 287675526    39097.\n12 United States Americas   2007    78.2 301139947    42952.\n\nsubset(gapminder, country == \"United States\")\n\n# A tibble: 12 × 6\n   country       continent  year lifeExp       pop gdpPercap\n   &lt;fct&gt;         &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 United States Americas   1952    68.4 157553000    13990.\n 2 United States Americas   1957    69.5 171984000    14847.\n 3 United States Americas   1962    70.2 186538000    16173.\n 4 United States Americas   1967    70.8 198712000    19530.\n 5 United States Americas   1972    71.3 209896000    21806.\n 6 United States Americas   1977    73.4 220239000    24073.\n 7 United States Americas   1982    74.6 232187835    25010.\n 8 United States Americas   1987    75.0 242803533    29884.\n 9 United States Americas   1992    76.1 256894189    32004.\n10 United States Americas   1997    76.8 272911760    35767.\n11 United States Americas   2002    77.3 287675526    39097.\n12 United States Americas   2007    78.2 301139947    42952.\n\n\n\ntidyverse\n\n\ngapminder %&gt;% filter(country == \"United States\")\n\n# A tibble: 12 × 6\n   country       continent  year lifeExp       pop gdpPercap\n   &lt;fct&gt;         &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n 1 United States Americas   1952    68.4 157553000    13990.\n 2 United States Americas   1957    69.5 171984000    14847.\n 3 United States Americas   1962    70.2 186538000    16173.\n 4 United States Americas   1967    70.8 198712000    19530.\n 5 United States Americas   1972    71.3 209896000    21806.\n 6 United States Americas   1977    73.4 220239000    24073.\n 7 United States Americas   1982    74.6 232187835    25010.\n 8 United States Americas   1987    75.0 242803533    29884.\n 9 United States Americas   1992    76.1 256894189    32004.\n10 United States Americas   1997    76.8 272911760    35767.\n11 United States Americas   2002    77.3 287675526    39097.\n12 United States Americas   2007    78.2 301139947    42952."
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#piping",
    "href": "posts/2022-08-18-introduction-to-r/index.html#piping",
    "title": "Introduction to R",
    "section": "Piping",
    "text": "Piping\n\ntidyverse pipe: %&gt;%\nnative pipe (since R4.1.0): |&gt;\nEnables chaining: the output of a previous function becomes the first argument of the next function\nExample: sort continents by their total population in 2007\n\n\ngapminder %&gt;%\n  filter(year == 2007) %&gt;%\n  group_by(continent) %&gt;%\n  summarise(totalPop = sum(pop)) %&gt;%\n  arrange(desc(totalPop))\n\n# A tibble: 5 × 2\n  continent   totalPop\n  &lt;fct&gt;          &lt;dbl&gt;\n1 Asia      3811953827\n2 Africa     929539692\n3 Americas   898871184\n4 Europe     586098529\n5 Oceania     24549947\n\n\n\ngapminder dataset\n\\(\\to\\) then filter rows for year 2007\n\\(\\to\\) then group rows by continent\n\\(\\to\\) then summarize population for each continent\n\\(\\to\\) then arrange rows with descending total population"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#assignment-operator",
    "href": "posts/2022-08-18-introduction-to-r/index.html#assignment-operator",
    "title": "Introduction to R",
    "section": "Assignment operator",
    "text": "Assignment operator\nMost variables are created with the assignment operator, &lt;- or =\n\nx &lt;- 3\nprint(x)\n\n[1] 3\n\nx = 3\nprint(x)\n\n[1] 3\n\n\n\nthere are differences\nshort-cut for &lt;-:\nAlt + - in RStudio"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#ggplot2",
    "href": "posts/2022-08-18-introduction-to-r/index.html#ggplot2",
    "title": "Introduction to R",
    "section": "ggplot2",
    "text": "ggplot2\n\nboxplot: distributions of life expectancy by continent\n\n\ngapminder %&gt;% \n  mutate(continent = reorder(continent, lifeExp, FUN=median)) %&gt;%\n  ggplot(aes(x=continent, y=lifeExp, fill=continent)) +\n  geom_boxplot(outlier.size=2)\n\n\n\n\n\nline/point plot\n\n\ngapminder %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(lifeExp=median(lifeExp)) %&gt;%\n  ggplot(aes(x=year, y=lifeExp, color=continent)) +\n  geom_line(size=1) + \n  geom_point(size=1.5) +\n  geom_smooth(aes(fill=continent), method=\"lm\")\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nsource"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#r-markdown",
    "href": "posts/2022-08-18-introduction-to-r/index.html#r-markdown",
    "title": "Introduction to R",
    "section": "R Markdown",
    "text": "R Markdown\n\nAllows the user to integrate R code into a report\nWhen data changes or code changes, so does the report\nNo need to copy-and-paste graphics, tables, or numbers\nCreates reproducible reports\n\nAnyone who has your R Markdown (.Rmd) file and input data can re-run your analysis and get the exact same results (tables, figures, summaries)\n\nCan output report in HTML (default), Microsoft Word, or PDF"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#git",
    "href": "posts/2022-08-18-introduction-to-r/index.html#git",
    "title": "Introduction to R",
    "section": "Git",
    "text": "Git\n\nHappy Git and GitHub for the useR\nExcuse me, do you have a moment to talk about version control?"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#install-r-and-rstudio",
    "href": "posts/2022-08-18-introduction-to-r/index.html#install-r-and-rstudio",
    "title": "Introduction to R",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nHands-On Programming with R"
  },
  {
    "objectID": "posts/2022-08-18-introduction-to-r/index.html#resources",
    "href": "posts/2022-08-18-introduction-to-r/index.html#resources",
    "title": "Introduction to R",
    "section": "Resources",
    "text": "Resources\n\nR for Data Science\nR Markdown: The Definitive Guide\nggplot2: Elegant Graphics for Data Analysis\nMastering Shiny\nAnalyzing US Census Data: Methods, Maps, and Models in R\nR for Epidemiology\nBig Book of R\nReproducible Research with R and RStudio\nProject-oriented workflow\n……"
  },
  {
    "objectID": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html",
    "href": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html",
    "title": "Pull Data Using EPA’s Data Service API",
    "section": "",
    "text": "EPA’s Envirofacts Data Service API provides access to its databases, including Safe Drinking Water Information System (SDWIS). We use data from SDWIS to calculate a measure (i.e., Drinking Water Violations) for CHRR rankings. We usually download data from SDWIS website for the previous year when data available in SDWIS is up to Quarter 3 of current year. For example, when SDWIS data is available up to 2022 quarter 3, we extract data for 01-JAN-2021 to 31-DEC-2021. In this post, I will show how we can use API to pull data from SDWIS."
  },
  {
    "objectID": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#construct-a-url",
    "href": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#construct-a-url",
    "title": "Pull Data Using EPA’s Data Service API",
    "section": "Construct a URL",
    "text": "Construct a URL\nThe key to pulling data via data API is to construct a URL (Uniform Resource Locators), or a link, with a specific set of parameters (i.e., set up conditions for columns). For example, the following URL can help us pull first 20 rows of violation data in CSV format that are health based, for CWS water systems, and on date 01-JAN-20:\n“https://data.epa.gov/efservice/VIOLATION/PWS_TYPE_CODE/CWS/IS_HEALTH_BASED_IND/Y/COMPL_PER_BEGIN_DATE/01-JAN-20/CSV/rows/0:19”\nWe can break up the URL into a few pieces and see what they mean.\n\n\n\n\n\n\n1: Address to EPA data API\n2: Table name: VIOLATION\n3-7: specify parameters\n\n3: set PWS_TYPE_CODE = CWS\n4: set IS_HEALTH_BASED_IND = Y\n5: set COMPL_PER_BEGIN_DATE = 01-JAN-20\n6: set output data file type as CSV. This is optional. If not specified, the defualt is XML. We can also choose the output format as JSON or Excel.\n7: select first 20 rows rows = 0:19 (row number starts at 0). This is also optional. However, the output is limited to 10000 rows at a time. So, we are fine without specifying rows if the data has less than 10000 rows. But it is a good idea to know how many rows in the data we want to extract and decide if we need to specify rows."
  },
  {
    "objectID": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#ways-to-extract-data",
    "href": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#ways-to-extract-data",
    "title": "Pull Data Using EPA’s Data Service API",
    "section": "Ways to extract data",
    "text": "Ways to extract data\n\nThe Simplest way to pull data is just to copy the link and paste it in a web browser (e.g., Google Chrome) and hit “enter” key. And be patient if you are pulling a large data set, because the web browser may show nothing after you clicked “enter”. This way is simple, but it may not be very reproducible. We may make and edit the URL in Word, since we need to specify a few parameters. Then copy the URL and paste it to Chrome. When we get the csv file, we will process it in SAS, Excel, or other software. So, we need to work at various places, i.e. using different software during the process.\nAnother way is to write a piece code in R, Python, or SAS. For example, we can use the following R code to extract the data:\n\n\nurl &lt;- \"https://data.epa.gov/efservice/VIOLATION/PWS_TYPE_CODE/CWS/IS_HEALTH_BASED_IND/Y/COMPL_PER_BEGIN_DATE/01-JAN-20/CSV/rows/0:19\"\n\ndata &lt;- readr::read_csv(url, show_col_types = FALSE)\n\ndata %&gt;% DT::datatable()\n\nThe extracted data is a dataframe. We can check how many rows and columns are in the file, save it for later processing, or start working on it right away. In other words, we can stay in one place (e.g., RStudio, a Python notebook), get almost everything done, and keep detailed notes for others and ourselves when we need to re-run the code later."
  },
  {
    "objectID": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#extract-violation-data-for-one-year",
    "href": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#extract-violation-data-for-one-year",
    "title": "Pull Data Using EPA’s Data Service API",
    "section": "Extract violation data for one year",
    "text": "Extract violation data for one year\nWe need data for one year, i.e., 365 or 366 days. One way to do this is to construct a list of URL’s for all the dates, extract data, and assemble them together. Assume we need data from 01-JAN-20 to 31-DEC-20. Let’s first construct the list of URL’s.\n\nCreaete a helper with a list of links we need\n\n# define base url\nurl_base &lt;- \"https://data.epa.gov/efservice/VIOLATION/PWS_TYPE_CODE/CWS/IS_HEALTH_BASED_IND/Y/COMPL_PER_BEGIN_DATE/\"\n\n# define start date\nstart_date &lt;- as.Date(\"2020/01/01\")\n  \n# define end date\nend_date &lt;- as.Date(\"2020/12/31\")\n  \n# generate range of dates\ndate_range &lt;- seq(start_date, end_date, \"days\")\n\ndf_helper &lt;- date_range %&gt;%\n  as_tibble() %&gt;% \n  mutate(date = format(value, \"%d-%b-%y\")) %&gt;% \n  mutate(date = toupper(date)) %&gt;% \n  select(date) %&gt;% \n  mutate(url = paste0(url_base, date, \"/CSV\"))\n\ndf_helper %&gt;% head()\n\n\n\nLoop through days to get data\n\n# a empty list to save dataframes\ndf_list_vio  &lt;-  list()\n\n# loop\nfor( i in 1:nrow(df_helper)) {\n  print(paste0(\"date: \", df_helper$date[i]))\n  print(df_helper$url[i])\n\n  data &lt;- read_csv(df_helper$url[i], col_types = cols(.default = \"c\")) \n\n  df_list_vio[[i]] &lt;- data\n}\n\n# assemble data\ndf_vio &lt;- df_list_vio %&gt;%\n  map_dfr(bind_rows)\n\n\nIt took about 15 minutes to loop through all dates\n\n\n\nTidyverse style code\nWe can replace the for loop using map function and make the code more tidyverse style.\n\ndf_tidy_vio &lt;- df_helper %&gt;% \n  select(url) %&gt;% \n  pull() %&gt;% \n  map_dfr(~ read_csv(., col_types = cols(.default = \"c\")))"
  },
  {
    "objectID": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#extract-violation-data-by-specifying-rows",
    "href": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#extract-violation-data-by-specifying-rows",
    "title": "Pull Data Using EPA’s Data Service API",
    "section": "Extract violation data by specifying rows",
    "text": "Extract violation data by specifying rows\nIt takes time to make 365 or 365 extractions. A faster way is to extract all available data and then subset data for the time range we need. First, we need to find out how many rows we will pull if we do not specify a date. We will need to add the option “COUNT” in the query to get the information, like this:\n“https://data.epa.gov/efservice/VIOLATION/PWS_TYPE_CODE/CWS/IS_HEALTH_BASED_IND/Y/COUNT”\nYou can put the URL in Chrome and see something like this:\n\n&lt;Envirofacts&gt;\n&lt;RequestRecordCount&gt;138068&lt;/RequestRecordCount&gt;\n&lt;/Envirofacts&gt;\n\nIf we want to do it in R, this is something we may do:\n\nurl_count &lt;- \"https://data.epa.gov/efservice/VIOLATION/PWS_TYPE_CODE/CWS/IS_HEALTH_BASED_IND/Y/COUNT\"\n\nhttr::GET(url_count)%&gt;% \n  #convert to text object using httr\n  httr::content(as = \"text\") %&gt;% \n  str_extract_all(pattern = \"(?&lt;=RequestRecordCount\\\\&gt;).+?(?=\\\\&lt;)\")\n\nSo, there are way more than 10000 rows. We can following the instruction,\n\n\n“the output is limited to 10000 rows of data at a time, but a user can pick which 10000 rows of data and then return to retrieve the next 10000.”\n\n\nand we can make multiple extractions by specifying rows. Or can we set rows as “0:138067” (Note: the number of rows changes when new records are added into EPA database) and get all data in one extraction? It actually worked. Maybe Envirofacts made improvement and lifted the limit on the number of rows to exact?"
  },
  {
    "objectID": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#other-software",
    "href": "posts/2022-10-09-pull-data-from-epa-databases-through-its-data-service-api/index.html#other-software",
    "title": "Pull Data Using EPA’s Data Service API",
    "section": "Other software",
    "text": "Other software\nWe can use other software, such as Python and SAS, to extract data via API as well. Here is an example of using python to extract first 20 rows of violation data in CSV format that are health based, from CWS, and on 01-JAN-20:\n\nimport pandas as pd\n\nurl = \"https://data.epa.gov/efservice/VIOLATION/PWS_TYPE_CODE/CWS/IS_HEALTH_BASED_IND/Y/COMPL_PER_BEGIN_DATE/01-JAN-20/CSV/rows/0:19\"\n\ndata = pd.read_csv(url)\ndata.head()\n\nHere is an example of using SAS proc http:\n\nfilename data_api \"C:\\test.csv\";\n\n%let url = https://data.epa.gov/efservice/VIOLATION/PWS_TYPE_CODE/CWS/IS_HEALTH_BASED_IND/Y/COMPL_PER_BEGIN_DATE/01-JAN-20/CSV/rows/0:19;\n\nproc http\n  url=\"&url\"\n  method= \"GET\"\n  out=data_api;\nrun;\n\nproc import\n  file=data_api\n  out=data\n  dbms=csv\n  replace;\nrun;"
  },
  {
    "objectID": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html",
    "href": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html",
    "title": "Calculate county-level rural population rate from Census blocks data using Python",
    "section": "",
    "text": "The goal of this post is to calculate Percentage of population living in a rural area using shapefiles of 2020 Tabulation (Census) Block. Each of the block shapefiles has a column UR20, which indicates if a block is Urban or Rural, and a column POP20, which has the population estimates based on 2020 decennial census. We can find total, rural, and urban populations in each county by aggregating the block-level data and then calculate percentage of rural population by dividing rural population by total population. Python is used in this post.\nWe will first calculate percentage of rural population in WI using blocks data, and then extend the procedure to find % rural for all counties in 50 states and DC"
  },
  {
    "objectID": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html#import-wi-blocks-shapefile",
    "href": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html#import-wi-blocks-shapefile",
    "title": "Calculate county-level rural population rate from Census blocks data using Python",
    "section": "Import WI blocks shapefile",
    "text": "Import WI blocks shapefile\nWe will directly download the WI blocks shapefile (277 MB) using GeoPandas from Census website. It will take a few minutes, depending on the computer and network. (~3 min on my machine)\nurl = \"https://www2.census.gov/geo/tiger/TIGER2020/TABBLOCK20/tl_2020_55_tabblock20.zip\"\nwi_blocks = gpd.read_file(url)\nThe shapefile has more information than we actually need. So let’s just keep the 4 columns we need and also rename the FIPS code columns.\n\nwi_blocks = wi_blocks[['STATEFP20', 'COUNTYFP20', 'UR20', 'POP20']].\\\n    rename(columns={'STATEFP20':'statecode', 'COUNTYFP20':'countycode'})\nNow we can aggregate the data from block-level to county-level and find total, rural, and urban populations.\n# total pop for each WI county\nwi_ct_pop = wi_blocks.groupby(['statecode', 'countycode'])[['POP20']].sum().reset_index().rename(columns={'POP20':'pop'})\n\n# rural pop at county-level\nwi_ct_pop_r = wi_blocks[wi_blocks.UR20 == \"R\"].groupby(['statecode', 'countycode'])[['POP20']].\\\n    sum().reset_index().rename(columns={'POP20':'pop_r'})\n    \n# urban pop at county-level\nwi_ct_pop_u = wi_blocks[wi_blocks.UR20 == \"U\"].groupby(['statecode', 'countycode'])[['POP20']].\\\n    sum().reset_index().rename(columns={'POP20':'pop_u'})\nNext, we will join the three population dataframes together and calculate the rural rate.\n# left join\nwi_ct_rural_rate = wi_ct_pop.merge(us_ct_pop_r, on=['statecode', 'countycode'], how='left').\\\n    merge(us_ct_pop_u, on=['statecode', 'countycode'], how='left')\n    \n# rural rate\nwi_ct_rural_rate['rate'] = wi_ct_rural_rate['pop_r']/wi_ct_rural_rate['pop']\nLet’s take a look at the first a few rows in wi_ct_rural_rate\nwi_ct_rural_rate.head(5)\n\n\n# A tibble: 5 × 6\n  statecode countycode    pop pop_r pop_u     rate\n      &lt;int&gt; &lt;chr&gt;       &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;\n1        55 001         20654 20654 NaN      1    \n2        55 003         16027  8802 7225.0   0.549\n3        55 005         46711 36555 10156.0  0.783\n4        55 007         16220 16220 NaN      1    \n5        55 009        268740 44584 224156.0 0.166"
  },
  {
    "objectID": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html#import-data---2020-tabulation-census-block",
    "href": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html#import-data---2020-tabulation-census-block",
    "title": "Calculate county-level rural population rate from Census blocks data using Python",
    "section": "import data - ‘2020 Tabulation (Census) Block’",
    "text": "import data - ‘2020 Tabulation (Census) Block’\nIf you download Census block shapefiles for 50 states and DC, those zipped files are about 9GB all together. I assembled all those files together and saved the results in a parquet file (~8.4GB). Apache Arrow is good at processing large data files (including parquet files) with high performance.\nAfter reading in the parquet file, the data was converted to a Pandas dataframe.\nus_blocks = pq.read_table('data/TABBLOCK20_shp_all.parquet', columns=['GEOID20', 'UR20', 'POP20']).to_pandas(split_blocks=True, self_destruct=True)\nThe dataframe looks like this:\n\n\n# A tibble: 3 × 3\n  GEOID20         UR20  POP20\n  &lt;chr&gt;           &lt;chr&gt; &lt;int&gt;\n1 011339657003025 R        15\n2 010150016001026 U        20\n3 010150011012065 R        17\n\n\nLet’s create statecode and countycode columns by substring GEOID20:\nus_blocks['statecode'] = us_blocks.GEOID20.str[:2]\nus_blocks['countycode'] = us_blocks.GEOID20.str[2:5]\n# drop 'GEOID20'\nus_blocks = us_blocks.drop('GEOID20', axis=1)\nNow the dataframe looks like this:\n\n\n# A tibble: 3 × 4\n  UR20  POP20 statecode countycode\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     \n1 R        15 01        133       \n2 U        20 01        015       \n3 R        17 01        015       \n\n\nAgain, we can aggregate the data from block-level to county-level and find total, rural, and urban populations for all counties.\n# total pop\nus_ct_pop = us_blocks.groupby(['statecode', 'countycode'])[['POP20']].sum().reset_index().rename(columns={'POP20':'pop'})\n# rural pop\nus_ct_pop_r = us_blocks[us_blocks.UR20 == \"R\"].groupby(['statecode', 'countycode'])[['POP20']].\\\n    sum().reset_index().rename(columns={'POP20':'pop_r'})\n# urban pop    \nus_ct_pop_u = us_blocks[us_blocks.UR20 == \"U\"].groupby(['statecode', 'countycode'])[['POP20']].\\\n    sum().reset_index().rename(columns={'POP20':'pop_u'})\nWe can join the three dataframes and calculate rural population rates.\n# left join\nus_ct_rural_rate = us_ct_pop.merge(us_ct_pop_r, on=['statecode', 'countycode'], how='left').\\\n    merge(us_ct_pop_u, on=['statecode', 'countycode'], how='left')\n# rural rate \nus_ct_rural_rate['rate'] = us_ct_rural_rate['pop_r']/us_ct_rural_rate['pop']\nLet’s check the first few rows of us_ct_rural_rate\nus_ct_rural_rate.head(5)\n\n\n# A tibble: 5 × 6\n  statecode countycode    pop pop_r pop_u   rate\n      &lt;int&gt;      &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n1         1          1  58805 23920 34885  0.407\n2         1          3 231767 87113 144654 0.376\n3         1          5  25223 16627 8596   0.659\n4         1          7  22293 22293 NaN    1    \n5         1          9  59134 53510 5624   0.905"
  },
  {
    "objectID": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html#compare-the-calcuated-results-with-data-released-by-census",
    "href": "posts/2023-05-15-calculate-county-level-rural-population-rate-from-census-blocks-data-using-python/index.html#compare-the-calcuated-results-with-data-released-by-census",
    "title": "Calculate county-level rural population rate from Census blocks data using Python",
    "section": "Compare the calcuated results with data released by Census",
    "text": "Compare the calcuated results with data released by Census\nWe can compare the calculated results with the data released by Census: County-level Urban and Rural information for the 2020 Census. The data is in an excel file, and we can download it using Pandas, which takes ~ 1 minute.\ncensus_ct_ur_2020 = pd.read_excel('https://www2.census.gov/geo/docs/reference/ua/2020_UA_COUNTY.xlsx')\nLet’s select only columns we need and rename them.’_PUB’ is used to indicate public (or published) data from Census.\n# select needed columns\ncensus_ct_ur_2020 = census_ct_ur_2020[['STATE', 'COUNTY', 'POP_COU','POP_URB', 'POP_RUR', 'POPPCT_RUR']].\\\n    rename(columns={'STATE':'statecode', 'COUNTY':'countycode', 'POP_COU':'pop_PUB', 'POP_URB':'pop_u_PUB', 'POP_RUR':'pop_r_PUB', 'POPPCT_RUR':'rate_PUB'})\n\ncensus_ct_ur_2020.head(3)  \n\n\n# A tibble: 3 × 6\n  statecode countycode pop_PUB pop_u_PUB pop_r_PUB rate_PUB\n      &lt;int&gt;      &lt;int&gt;   &lt;int&gt;     &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n1         1          1   58805     34885     23920    0.407\n2         1          3  231767    144654     87113    0.376\n3         1          5   25223      8596     16627    0.659\n\n\nWe only need rows for 50 states and DC, and we need to convert statecode and countycode to strings.\n# keep only 50 states + DC\ncensus_ct_ur_2020 = census_ct_ur_2020[census_ct_ur_2020.statecode &lt; 57]\n# convert `statecode` and `countycode` to strings padded with '0'\ncensus_ct_ur_2020['statecode'] = census_ct_ur_2020['statecode'].astype(str).str.zfill(2)\ncensus_ct_ur_2020['countycode'] = census_ct_ur_2020['countycode'].astype(str).str.zfill(3)\n\ncensus_ct_ur_2020.head(3)\n\n\n# A tibble: 3 × 6\n  statecode countycode pop_PUB pop_u_PUB pop_r_PUB rate_PUB\n      &lt;int&gt;      &lt;int&gt;   &lt;int&gt;     &lt;int&gt;     &lt;int&gt;    &lt;dbl&gt;\n1         1          1   58805     34885     23920    0.407\n2         1          3  231767    144654     87113    0.376\n3         1          5   25223      8596     16627    0.659\n\n\nNow we can join Census data with the data from our calculations using statecode and countycode as key. In the joined data, we add a column rate_diff to see the difference between the two rates.\n# left join\ncompare_rural_rate = census_ct_ur_2020.merge(us_ct_rural_rate, on=['statecode', 'countycode'], how='left')\n# difference between calculated rates and Census published rates\ncompare_rural_rate['rate_dif'] = compare_rural_rate['rate'] - compare_rural_rate['rate_PUB']\n\nIf we filter compare_rural_rate by setting rate_dif &gt; 1e-9, we will get an empty dataframe, which means no row has rate_dif &gt; 1e-9.\n# find if there are cases where difference &gt; 1e-9\ncompare_rural_rate[compare_rural_rate.rate_dif &gt; 1e-9]"
  },
  {
    "objectID": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html",
    "href": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html",
    "title": "Subgroup Population by Age Category from Census Population Estimates",
    "section": "",
    "text": "Years of Potential Life Lost (YPLL) is widely used for measuring the rate and distribution of premature mortality. CHRR uses YPLL as one of health outcomes to measure county health status.YPLL is a rate and age-adjusted, and it can be calculated as\n\\[\n\\boxed{ {YPLL \\: Rate }_{{age-adj}}=\\sum_{i=0}^{75} \\frac {(75-i)\\cdot d_i\\cdot  {w_i}}{p_i} \\cdot 100,000}\n\\] where\n\n\\(75\\) is the upper age limit established,\n\\(i\\) is the midpoint of the grouped year of age at death (e.g., 59.5 for age group 55-64),\n\\(d_i\\) is the number of deaths at age \\(i\\).\n\\(p_i\\) is the population for age group i.\n\\(\\sum p_i\\) is the population between the lower and upper age limits (e.g., 75 as the upper age limit and 0 as the lower age limit)\n\\({w_i}\\) is the weight of age adjustment for group i on the basis of the US standard year 2000 population. \\(\\sum w_i = 1\\)\n\nThe population data is needed in the denominator of the above formula, and \\(p_i\\) has been based on the single-year-of-age, bridged-race, county-level population estimates released by the National Center for Health Statistics (NCHS). The series of bridged-race population estimates is a collaborative work between NCHS and U. S. Census Bureau. However, NCHS discontinued the production of single-year-of-age, bridged-race population estimates after releasing 2020 data. In their documentation, it is mentioned that “The U.S. Census Bureau annually releases unbridged population estimates for five-year age groups and race at the county level.” This post uses R to explore the feasibility of getting population data needed for subgroup YPLL calculations from Census population estimates."
  },
  {
    "objectID": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#age-categories-age_cat",
    "href": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#age-categories-age_cat",
    "title": "Subgroup Population by Age Category from Census Population Estimates",
    "section": "Age categories (age_cat)",
    "text": "Age categories (age_cat)\nFrom the lower age limit 0 to the upper age limit 75, the population is usually grouped into 8 age categories (coded from 0 to 7):\n\n\n\n\nAge categories\n\n\nAge\nage_cat\n\n\n\n\n0\n0\n\n\n1-14\n1\n\n\n15-24\n2\n\n\n25-34\n3\n\n\n35-44\n4\n\n\n45-54\n5\n\n\n55-64\n6\n\n\n65-74\n7"
  },
  {
    "objectID": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#raceethnicity-subgroups",
    "href": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#raceethnicity-subgroups",
    "title": "Subgroup Population by Age Category from Census Population Estimates",
    "section": "Race/ethnicity subgroups",
    "text": "Race/ethnicity subgroups\nFive race/ethnicity subgroups have been used in YPLL calculations at CHRR.\n\n\n\nSubgroups\n\n\nSubgroup\nDescription\n\n\n\n\n1\nnon-Hispanic, White alone\n\n\n2\nnon-Hispanic, Black alone\n\n\n3\nnon-Hispanic, AIAN (American Indian or Alaska Native) alone\n\n\n4\nnon-Hispanic, Asian alone\n\n\n8\nHispanic\n\n\n\n\n\n\n\nNote: Bridged and un-bridged race populations are not identical. By bridging, those in the group “two or more races” are assigned using probabilistic factors and included in White, Black, AIAN, or Asian."
  },
  {
    "objectID": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#get-data-directly-from-census-pft-website",
    "href": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#get-data-directly-from-census-pft-website",
    "title": "Subgroup Population by Age Category from Census Population Estimates",
    "section": "Get data directly from Census pft website",
    "text": "Get data directly from Census pft website\n\ncc_est2020_all &lt;- read_csv(\"https://www2.census.gov/programs-surveys/popest/datasets/2010-2020/counties/asrh/CC-EST2020-ALLDATA.csv\") |&gt;  \n  janitor::clean_names() |&gt;  \n  glimpse()\n\nRows: 776,321\nColumns: 80\n$ sumlev       &lt;chr&gt; \"050\", \"050\", \"050\", \"050\", \"050\", \"050\", \"050\", \"050\", \"…\n$ state        &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01…\n$ county       &lt;chr&gt; \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"…\n$ stname       &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"A…\n$ ctyname      &lt;chr&gt; \"Autauga County\", \"Autauga County\", \"Autauga County\", \"Au…\n$ year         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ agegrp       &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ tot_pop      &lt;dbl&gt; 54571, 3579, 3991, 4290, 4290, 3080, 3157, 3330, 4157, 40…\n$ tot_male     &lt;dbl&gt; 26569, 1866, 2001, 2171, 2213, 1539, 1543, 1594, 2004, 19…\n$ tot_female   &lt;dbl&gt; 28002, 1713, 1990, 2119, 2077, 1541, 1614, 1736, 2153, 21…\n$ wa_male      &lt;dbl&gt; 21295, 1411, 1521, 1658, 1628, 1201, 1234, 1289, 1596, 16…\n$ wa_female    &lt;dbl&gt; 22002, 1316, 1526, 1620, 1585, 1184, 1223, 1298, 1645, 16…\n$ ba_male      &lt;dbl&gt; 4559, 362, 399, 431, 502, 293, 277, 276, 338, 303, 315, 3…\n$ ba_female    &lt;dbl&gt; 5130, 317, 374, 406, 424, 312, 350, 378, 432, 384, 353, 3…\n$ ia_male      &lt;dbl&gt; 119, 5, 14, 15, 12, 6, 1, 1, 11, 9, 14, 11, 7, 6, 3, 4, 0…\n$ ia_female    &lt;dbl&gt; 139, 3, 8, 12, 7, 9, 3, 10, 9, 14, 11, 12, 9, 13, 8, 5, 6…\n$ aa_male      &lt;dbl&gt; 200, 13, 17, 23, 25, 8, 7, 12, 30, 16, 19, 10, 6, 7, 4, 2…\n$ aa_female    &lt;dbl&gt; 284, 15, 21, 18, 14, 7, 23, 25, 30, 36, 27, 17, 9, 13, 9,…\n$ na_male      &lt;dbl&gt; 29, 1, 1, 4, 4, 3, 6, 3, 2, 1, 3, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ na_female    &lt;dbl&gt; 18, 0, 3, 1, 2, 2, 3, 1, 3, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n$ tom_male     &lt;dbl&gt; 367, 74, 49, 40, 42, 28, 18, 13, 27, 20, 13, 14, 8, 9, 6,…\n$ tom_female   &lt;dbl&gt; 429, 62, 58, 62, 45, 27, 12, 24, 34, 18, 23, 20, 7, 14, 1…\n$ wac_male     &lt;dbl&gt; 21633, 1479, 1570, 1694, 1664, 1228, 1250, 1301, 1621, 16…\n$ wac_female   &lt;dbl&gt; 22391, 1368, 1583, 1681, 1624, 1206, 1235, 1319, 1675, 16…\n$ bac_male     &lt;dbl&gt; 4704, 405, 425, 453, 525, 301, 280, 279, 342, 307, 317, 3…\n$ bac_female   &lt;dbl&gt; 5306, 362, 403, 436, 444, 324, 351, 386, 446, 388, 359, 3…\n$ iac_male     &lt;dbl&gt; 277, 23, 27, 29, 23, 17, 14, 6, 29, 22, 22, 24, 11, 14, 7…\n$ iac_female   &lt;dbl&gt; 314, 18, 19, 27, 20, 24, 12, 15, 22, 27, 25, 28, 14, 26, …\n$ aac_male     &lt;dbl&gt; 300, 34, 32, 32, 39, 16, 12, 18, 38, 22, 22, 11, 8, 8, 5,…\n$ aac_female   &lt;dbl&gt; 409, 28, 42, 37, 31, 12, 25, 40, 44, 41, 32, 20, 11, 14, …\n$ nac_male     &lt;dbl&gt; 42, 3, 3, 4, 6, 5, 6, 3, 2, 3, 3, 0, 3, 0, 1, 0, 0, 0, 0,…\n$ nac_female   &lt;dbl&gt; 37, 1, 4, 5, 5, 3, 3, 2, 3, 4, 5, 0, 0, 2, 0, 0, 0, 0, 0,…\n$ nh_male      &lt;dbl&gt; 25875, 1778, 1933, 2105, 2153, 1474, 1477, 1533, 1948, 19…\n$ nh_female    &lt;dbl&gt; 27386, 1651, 1916, 2055, 2026, 1499, 1570, 1674, 2108, 20…\n$ nhwa_male    &lt;dbl&gt; 20709, 1337, 1460, 1613, 1580, 1141, 1180, 1234, 1547, 15…\n$ nhwa_female  &lt;dbl&gt; 21485, 1260, 1465, 1570, 1543, 1151, 1184, 1245, 1610, 16…\n$ nhba_male    &lt;dbl&gt; 4512, 356, 398, 421, 495, 291, 272, 273, 336, 301, 312, 3…\n$ nhba_female  &lt;dbl&gt; 5091, 313, 372, 403, 420, 308, 347, 373, 429, 383, 348, 3…\n$ nhia_male    &lt;dbl&gt; 103, 2, 12, 12, 12, 6, 1, 1, 8, 8, 11, 10, 7, 6, 3, 4, 0,…\n$ nhia_female  &lt;dbl&gt; 115, 2, 2, 9, 5, 7, 3, 8, 7, 11, 11, 11, 9, 12, 8, 4, 6, …\n$ nhaa_male    &lt;dbl&gt; 194, 13, 17, 22, 23, 8, 5, 12, 30, 16, 18, 10, 6, 7, 4, 2…\n$ nhaa_female  &lt;dbl&gt; 280, 15, 21, 18, 14, 6, 23, 25, 29, 36, 26, 17, 9, 13, 8,…\n$ nhna_male    &lt;dbl&gt; 13, 0, 0, 3, 1, 2, 1, 1, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ nhna_female  &lt;dbl&gt; 9, 0, 3, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ nhtom_male   &lt;dbl&gt; 344, 70, 46, 34, 42, 26, 18, 12, 26, 18, 11, 14, 7, 9, 5,…\n$ nhtom_female &lt;dbl&gt; 406, 61, 53, 55, 43, 26, 12, 23, 32, 18, 22, 18, 6, 14, 1…\n$ nhwac_male   &lt;dbl&gt; 21026, 1402, 1506, 1643, 1616, 1166, 1196, 1245, 1571, 15…\n$ nhwac_female &lt;dbl&gt; 21853, 1312, 1517, 1624, 1580, 1172, 1196, 1266, 1638, 16…\n$ nhbac_male   &lt;dbl&gt; 4647, 396, 423, 440, 518, 299, 275, 275, 340, 304, 314, 3…\n$ nhbac_female &lt;dbl&gt; 5258, 357, 400, 429, 439, 320, 348, 380, 442, 387, 354, 3…\n$ nhiac_male   &lt;dbl&gt; 251, 19, 25, 24, 23, 15, 14, 6, 25, 20, 17, 23, 11, 14, 6…\n$ nhiac_female &lt;dbl&gt; 282, 17, 12, 22, 18, 21, 12, 13, 20, 24, 24, 25, 13, 25, …\n$ nhaac_male   &lt;dbl&gt; 291, 34, 30, 30, 37, 16, 10, 18, 38, 22, 21, 11, 8, 8, 5,…\n$ nhaac_female &lt;dbl&gt; 398, 28, 39, 36, 30, 11, 25, 39, 42, 41, 31, 20, 11, 14, …\n$ nhnac_male   &lt;dbl&gt; 23, 1, 1, 3, 3, 4, 1, 1, 1, 2, 3, 0, 2, 0, 1, 0, 0, 0, 0,…\n$ nhnac_female &lt;dbl&gt; 27, 0, 4, 4, 4, 2, 1, 1, 1, 3, 5, 0, 0, 2, 0, 0, 0, 0, 0,…\n$ h_male       &lt;dbl&gt; 694, 88, 68, 66, 60, 65, 66, 61, 56, 51, 34, 24, 20, 9, 1…\n$ h_female     &lt;dbl&gt; 616, 62, 74, 64, 51, 42, 44, 62, 45, 39, 47, 21, 13, 15, …\n$ hwa_male     &lt;dbl&gt; 586, 74, 61, 45, 48, 60, 54, 55, 49, 45, 25, 21, 17, 9, 9…\n$ hwa_female   &lt;dbl&gt; 517, 56, 61, 50, 42, 33, 39, 53, 35, 34, 40, 16, 12, 12, …\n$ hba_male     &lt;dbl&gt; 47, 6, 1, 10, 7, 2, 5, 3, 2, 2, 3, 2, 2, 0, 2, 0, 0, 0, 0…\n$ hba_female   &lt;dbl&gt; 39, 4, 2, 3, 4, 4, 3, 5, 3, 1, 5, 2, 0, 2, 0, 1, 0, 0, 0,…\n$ hia_male     &lt;dbl&gt; 16, 3, 2, 3, 0, 0, 0, 0, 3, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ hia_female   &lt;dbl&gt; 24, 1, 6, 3, 2, 2, 0, 2, 2, 3, 0, 1, 0, 1, 0, 1, 0, 0, 0,…\n$ haa_male     &lt;dbl&gt; 6, 0, 0, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ haa_female   &lt;dbl&gt; 4, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ hna_male     &lt;dbl&gt; 16, 1, 1, 1, 3, 1, 5, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ hna_female   &lt;dbl&gt; 9, 0, 0, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ htom_male    &lt;dbl&gt; 23, 4, 3, 6, 0, 2, 0, 1, 1, 2, 2, 0, 1, 0, 1, 0, 0, 0, 0,…\n$ htom_female  &lt;dbl&gt; 23, 1, 5, 7, 2, 1, 0, 1, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0,…\n$ hwac_male    &lt;dbl&gt; 607, 77, 64, 51, 48, 62, 54, 56, 50, 47, 27, 21, 17, 9, 1…\n$ hwac_female  &lt;dbl&gt; 538, 56, 66, 57, 44, 34, 39, 53, 37, 34, 41, 18, 13, 12, …\n$ hbac_male    &lt;dbl&gt; 57, 9, 2, 13, 7, 2, 5, 4, 2, 3, 3, 2, 3, 0, 2, 0, 0, 0, 0…\n$ hbac_female  &lt;dbl&gt; 48, 5, 3, 7, 5, 4, 3, 6, 4, 1, 5, 2, 0, 2, 0, 1, 0, 0, 0,…\n$ hiac_male    &lt;dbl&gt; 26, 4, 2, 5, 0, 2, 0, 0, 4, 2, 5, 1, 0, 0, 1, 0, 0, 0, 0,…\n$ hiac_female  &lt;dbl&gt; 32, 1, 7, 5, 2, 3, 0, 2, 2, 3, 1, 3, 1, 1, 0, 1, 0, 0, 0,…\n$ haac_male    &lt;dbl&gt; 9, 0, 2, 2, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ haac_female  &lt;dbl&gt; 11, 0, 3, 1, 1, 1, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,…\n$ hnac_male    &lt;dbl&gt; 19, 2, 2, 1, 3, 1, 5, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ hnac_female  &lt;dbl&gt; 10, 1, 0, 1, 1, 1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…"
  },
  {
    "objectID": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#overview-of-the-raw-data",
    "href": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#overview-of-the-raw-data",
    "title": "Subgroup Population by Age Category from Census Population Estimates",
    "section": "Overview of the raw data",
    "text": "Overview of the raw data\nThere are 776321 rows and 80 columns in the downloaded dataframe. The file layout is available here.\nHere are the columns that we will need to have a dataset with necessary age-categories and subgroups for YPLL calculation.\n\nstate: state FIPS code\ncounty: county FIPS code\nyear: Year of data\nThe key for YEAR is as follows:\n\n1 = 4/1/2010 Census population\n2 = 4/1/2010 population estimates base\n3 = 7/1/2010 population estimate\n4 = 7/1/2011 population estimate\n5 = 7/1/2012 population estimate\n6 = 7/1/2013 population estimate\n7 = 7/1/2014 population estimate\n8 = 7/1/2015 population estimate\n9 = 7/1/2016 population estimate\n10 = 7/1/2017 population estimate\n11 = 7/1/2018 population estimate\n12 = 7/1/2019 population estimate\n13 = 7/1/2020 population estimate\n\n\nWe will subset the original data with year == 13 to get 2020 population estimate.\n\nagegrp: Age group\n0 = Total\n1 = Age 0 to 4 years\n2 = Age 5 to 9 years\n3 = Age 10 to 14 years\n4 = Age 15 to 19 years\n5 = Age 20 to 24 years\n6 = Age 25 to 29 years\n7 = Age 30 to 34 years\n8 = Age 35 to 39 years\n9 = Age 40 to 44 years\n10 = Age 45 to 49 years\n11 = Age 50 to 54 years\n12 = Age 55 to 59 years\n13 = Age 60 to 64 years\n14 = Age 65 to 69 years\n15 = Age 70 to 74 years\n16 = Age 75 to 79 years\n17 = Age 80 to 84 years\n18 = Age 85 years or older\n\nWe need agegrp from 1 to 15 to create age categories shown in Table 1. One challenge is how we can get infant population (age_cat = 0) from a dataset with 5-year age groups. One way is to approximate the infant population by taking 20% of population in agegrp = 1 (age 0 to 4 years), assuming the data has a uniform distribution within this age group. This approach is not ideal, but it may be a realistic option for most counties. A possible better way to have infant population is to use births data for each county, which is out of the scope for this post.\n\n\n\n\n\nConstruct age categories for YPLL calculation\n\n\n\n\n\ncolumns for race/ethnicity groups\n\nTable 3 shows the columns in the raw data that we need to create the subgroups in Table 2.\n\n\n\n\nColumns needed to create subgroups\n\n\ncolumn\ndescription\nsubgroup\n\n\n\n\nNHWA_MALE\nNot Hispanic, White alone male population\n1\n\n\nNHWA_FEMALE\nNot Hispanic, White alone female population\n1\n\n\nNHBA_MALE\nNot Hispanic, Black or African American alone male population\n2\n\n\nNHBA_FEMALE\nNot Hispanic, Black or African American alone female population\n2\n\n\nNHIA_MALE\nNot Hispanic, American Indian and Alaska Native alone male population\n3\n\n\nNHIA_FEMALE\nNot Hispanic, American Indian and Alaska Native alone female population\n2\n\n\nNHAA_MALE\nNot Hispanic, Asian alone male population\n4\n\n\nNHAA_FEMALE\nNot Hispanic, Asian alone female population\n4\n\n\nNHNA_MALE\nNot Hispanic, Native Hawaiian and Other Pacific Islander alone male population\n4\n\n\nNHNA_FEMALE\nNot Hispanic, Native Hawaiian and Other Pacific Islander alone female population\n4\n\n\nH_MALE\nHispanic male population\n8\n\n\nH_FEMALE\nHispanic female population\n8"
  },
  {
    "objectID": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#subset-the-raw-data-keep-only-needed-columns",
    "href": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#subset-the-raw-data-keep-only-needed-columns",
    "title": "Subgroup Population by Age Category from Census Population Estimates",
    "section": "subset the raw data, keep only needed columns",
    "text": "subset the raw data, keep only needed columns\n\ncc_2020_agegrp_sig_r &lt;- cc_est2020_all |&gt; \n  # get 2020 data with agegrp from 1 to 15\n  filter(year == 13, agegrp %in% c(1:15)) |&gt; \n  select(statecode = state, countycode = county, agegrp,\n         starts_with(\"nhwa_\"), # Not Hispanic, White alone\n         starts_with(\"nhba_\"), # Not Hispanic, Black or African American alone\n         starts_with(\"nhia_\"), # Not Hispanic, American Indian and Alaska Native alone\n         starts_with(\"nhaa_\"), # Not Hispanic, Asian alone\n         starts_with(\"nhna_\"), # Not Hispanic, Native Hawaiian and Other Pacific Islander alone\n         starts_with(\"h_\") # Hispanic\n         ) |&gt; \n  mutate(white = nhwa_male + nhwa_female,\n         black = nhba_male + nhba_female,\n         aian = nhia_male + nhia_female,\n         asian = nhaa_male + nhaa_female + nhna_male + nhna_female,\n         hispanic = h_male + h_female\n         ) |&gt; \n  select(-c(4:15))\n\ncc_2020_agegrp_sig_r |&gt; head(5)\n\n# A tibble: 5 × 8\n  statecode countycode agegrp white black  aian asian hispanic\n  &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 01        001             1  2252   743     6    48      161\n2 01        001             2  2358   752    14    49      161\n3 01        001             3  2580   878    18    34      169\n4 01        001             4  2494   817     8    51      145\n5 01        001             5  2179   715     8    42      112"
  },
  {
    "objectID": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#county-population-by-age-categories-and-raceethnicity-subgroups",
    "href": "posts/2023-06-07-subgroup-population-by-age-category-from-census-population-estimates/index.html#county-population-by-age-categories-and-raceethnicity-subgroups",
    "title": "Subgroup Population by Age Category from Census Population Estimates",
    "section": "County population by age categories and race/ethnicity subgroups",
    "text": "County population by age categories and race/ethnicity subgroups\n\nage_cat = 2-7\n\n# 15-74: age cat 2-7\ncc_2020_sg_r_agecat_2_7 &lt;- cc_2020_agegrp_sig_r |&gt; \n  filter(agegrp %in% c(4:15)) |&gt; \n  mutate(age_cat = case_when(\n    agegrp %in% c(4, 5) ~ 2, # 15-19, 19-24\n    agegrp %in% c(6, 7) ~ 3, # 25-29, 30-34\n    agegrp %in% c(8, 9) ~ 4, # 35-39, 40-44\n    agegrp %in% c(10, 11) ~ 5, # 45-49, 50-54\n    agegrp %in% c(12, 13) ~ 6, # 55-59, 60-64\n    agegrp %in% c(14, 15) ~ 7, # 65-69, 70-74\n  )) |&gt; \n  select(-agegrp) |&gt; \n  group_by(statecode, countycode, age_cat) |&gt; \n  summarise(across(c(white, black, aian, asian, hispanic), ~sum(., na.rm = TRUE)), .groups = \"drop\")\n\ncc_2020_sg_r_agecat_2_7 |&gt; head(5)\n\n# A tibble: 5 × 8\n  statecode countycode age_cat white black  aian asian hispanic\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 01        001              2  4673  1532    16    93      257\n2 01        001              3  5294  1684    27    80      274\n3 01        001              4  5168  1654    23   134      304\n4 01        001              5  5547  1497    34   109      187\n5 01        001              6  5756  1335    44    61      119\n\n\n\n\nage_cat = 0: infant population\nTake 1/5 of population in agegrp = 1\n\n# age_cat = 0, infants\ncc_2020_sg_r_agecat_0 &lt;- cc_2020_agegrp_sig_r |&gt; \n  filter(agegrp %in% c(1)) |&gt; \n  mutate(age_cat = 0, .after = 3) |&gt; \n  select(-agegrp) |&gt; \n  mutate(across(c(4:8), ~ .x/5))\n\ncc_2020_sg_r_agecat_0 |&gt; head(5)\n\n# A tibble: 5 × 8\n  statecode countycode age_cat  white black  aian asian hispanic\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 01        001              0  450.  149.    1.2   9.6     32.2\n2 01        003              0 1787.  251    11    30.4    209. \n3 01        005              0   85.2 140     1     0.4     27  \n4 01        007              0  175.   44.8   0.6   0       13  \n5 01        009              0  543.   10.2   2     6.4    105. \n\n\n\n\nage_cat = 1: 1-14 year\n\n# age_cat = 1: 1-14\ncc_2020_sg_r_agecat_1 &lt;- cc_2020_agegrp_sig_r |&gt; \n  filter(agegrp %in% c(1:3)) |&gt; \n  mutate(across(c(4:8), ~if_else(agegrp==1, . * 4/5, .))) |&gt;   \n  group_by(statecode, countycode) |&gt; \n  summarise(across(c(2:6), ~ sum(., na.rm = TRUE)), .groups = \"drop\") |&gt; \n  ungroup() |&gt; \n  mutate(age_cat = 1, .after = 3)\n\ncc_2020_sg_r_agecat_1 |&gt; head(5)\n\n# A tibble: 5 × 8\n  statecode countycode  white age_cat black  aian asian hispanic\n  &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 01        001         6740.       1 2224.  36.8 121.      459.\n2 01        003        28270.       1 3746  195   491.     2965.\n3 01        005         1310.       1 2033    7    24.6     405 \n4 01        007         2476.       1  626.  12.4   4       157 \n5 01        009         8031.       1  166.  24    56.6    1615.\n\n\n\n\nall age categories\n\ncc_2020_sg_r_agecat_0_7 &lt;- bind_rows(\n  cc_2020_sg_r_agecat_0,\n  cc_2020_sg_r_agecat_1,\n  cc_2020_sg_r_agecat_2_7) |&gt;\n  arrange(statecode, countycode, age_cat) |&gt;  \n  # round numbers\n  mutate(across(c(4:8), ~round(.)))\n\ncc_2020_sg_r_agecat_0_7 |&gt; \n  head(5)\n\n# A tibble: 5 × 8\n  statecode countycode age_cat white black  aian asian hispanic\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 01        001              0   450   149     1    10       32\n2 01        001              1  6740  2224    37   121      459\n3 01        001              2  4673  1532    16    93      257\n4 01        001              3  5294  1684    27    80      274\n5 01        001              4  5168  1654    23   134      304\n\n\n\n\nPivot tabel longer so that we have one column of subgroup codes and one column for population\n\ncc_2020_sg_r_agecat_0_7_long &lt;- cc_2020_sg_r_agecat_0_7 |&gt; \n  pivot_longer(cols = c(4:8), names_to = \"race\", values_to = \"pop\") |&gt; \n  mutate(race = case_when(\n    race == \"white\" ~ 1,\n    race == \"black\" ~ 2,\n    race == \"aian\" ~ 3,\n    race == \"asian\" ~ 4,\n    race == \"hispanic\" ~ 8,\n    TRUE ~ NA_real_))\n\nLet’s take a look at the top 20 rows.\n\n# top 20 rows\ncc_2020_sg_r_agecat_0_7_long |&gt; \n  head(50) |&gt;\n  print_kbl()\n\n\n\n\nstatecode\ncountycode\nage_cat\nrace\npop\n\n\n\n\n01\n001\n0\n1\n450\n\n\n01\n001\n0\n2\n149\n\n\n01\n001\n0\n3\n1\n\n\n01\n001\n0\n4\n10\n\n\n01\n001\n0\n8\n32\n\n\n01\n001\n1\n1\n6740\n\n\n01\n001\n1\n2\n2224\n\n\n01\n001\n1\n3\n37\n\n\n01\n001\n1\n4\n121\n\n\n01\n001\n1\n8\n459\n\n\n01\n001\n2\n1\n4673\n\n\n01\n001\n2\n2\n1532\n\n\n01\n001\n2\n3\n16\n\n\n01\n001\n2\n4\n93\n\n\n01\n001\n2\n8\n257\n\n\n01\n001\n3\n1\n5294\n\n\n01\n001\n3\n2\n1684\n\n\n01\n001\n3\n3\n27\n\n\n01\n001\n3\n4\n80\n\n\n01\n001\n3\n8\n274\n\n\n01\n001\n4\n1\n5168\n\n\n01\n001\n4\n2\n1654\n\n\n01\n001\n4\n3\n23\n\n\n01\n001\n4\n4\n134\n\n\n01\n001\n4\n8\n304\n\n\n01\n001\n5\n1\n5547\n\n\n01\n001\n5\n2\n1497\n\n\n01\n001\n5\n3\n34\n\n\n01\n001\n5\n4\n109\n\n\n01\n001\n5\n8\n187\n\n\n01\n001\n6\n1\n5756\n\n\n01\n001\n6\n2\n1335\n\n\n01\n001\n6\n3\n44\n\n\n01\n001\n6\n4\n61\n\n\n01\n001\n6\n8\n119\n\n\n01\n001\n7\n1\n4181\n\n\n01\n001\n7\n2\n865\n\n\n01\n001\n7\n3\n30\n\n\n01\n001\n7\n4\n38\n\n\n01\n001\n7\n8\n59\n\n\n01\n003\n0\n1\n1787\n\n\n01\n003\n0\n2\n251\n\n\n01\n003\n0\n3\n11\n\n\n01\n003\n0\n4\n30\n\n\n01\n003\n0\n8\n209\n\n\n01\n003\n1\n1\n28270\n\n\n01\n003\n1\n2\n3746\n\n\n01\n003\n1\n3\n195\n\n\n01\n003\n1\n4\n491\n\n\n01\n003\n1\n8\n2965"
  },
  {
    "objectID": "posts/2023-06-09-run-r-in-sas/index.html",
    "href": "posts/2023-06-09-run-r-in-sas/index.html",
    "title": "Run R in SAS",
    "section": "",
    "text": "Usually, if a problem can be solved in one programming language, this problem can also be solved in another programming language. However, each programming language has strengths and weaknesses, and each is better at some tasks than others. It is intriguing to combine the strengths of different languages, such as SAS and R. For example, there are many advantages to calling R from SAS according to this blog. This post summarizes the procedure to set up a PC for running R in SAS."
  },
  {
    "objectID": "posts/2023-06-09-run-r-in-sas/index.html#cfg-file",
    "href": "posts/2023-06-09-run-r-in-sas/index.html#cfg-file",
    "title": "Run R in SAS",
    "section": "‘cfg’ file",
    "text": "‘cfg’ file\nFirst, the RLANG option must be set when SAS is started. This can be done by adding the following two lines in the SAS configuration file sasv9.cfg, which is at C:\\Program Files\\SASHome\\SASFoundation\\9.4\\sasv9.cfg on my PC.\n-config \"C:\\Program Files\\SASHome\\SASFoundation\\9.4\\nls\\en\\sasv9.cfg\"\n-RLANG\nYou can use any text editor to open and edit sasv9.cfg, but you need administrative rights to save the edited file.\nTo test if this setup is successful, we can run the following command in SAS:\nproc options option=rlang;\nrun;\nIf you get the following statement:\nNORLANG            Do not support access to R language interfaces\nthen, you still do not have permission to call R from SAS yet, and you need to check the setup of the cfg file.\nIf you see this statement:\nRLANG             Enables SAS to execute R language statements.\nthen, great! You have the permission to call R from SAS now! But before you can actually call R in SAS, you need to tell SAS where R is installed on your PC."
  },
  {
    "objectID": "posts/2023-06-09-run-r-in-sas/index.html#r_home-environment-variable",
    "href": "posts/2023-06-09-run-r-in-sas/index.html#r_home-environment-variable",
    "title": "Run R in SAS",
    "section": "R_HOME environment variable",
    "text": "R_HOME environment variable\nSAS needs an R_HOME environment variable so that it knows where to find the correct, available version of R. In other words, the R_HOME environment variable tells SAS where it should go to find R. There are a few options to define an environment. One way is to modify the environment variable in Windows directly, by going to My Computer -&gt; right click-properties -&gt; Advanced -&gt; Environment Variables, and setting it there. This also requires administrative rights.\nHere is a good post about how to define the R_HOME environment variable. I borrowed the image from there:\n\n\n\n\n\nOne important thing is that you must specify the correct path to the R directory. In my case, since R 3.6.3 is installed in the folder/directory C:\\Users\\MyUserName\\Documents\\R\\R-3.6.3, I set the ‘Variable value’ of R_HOME as C:\\Users\\MyUserName\\Documents\\R\\R-3.6.3.\nAfter the setting up cfg file and R_HOME, you may need to restart your computer."
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "",
    "text": "The State of Connecticut (CT) has adopted nine planning regions as county-equivalent geographic units. These regions replaced the previous eight counties (i.e., legacy counties). The Census Bureau has implemented this change since 2022. As a result, public data and geospatial products from Census now reflect the new boundaries. By the year 2024, all Census Bureau operations and publications have used the nine new county-equivalent boundaries, names, and codes. On the other hand, the transition to county-equivalents in Connecticut does not affect block group and census tract boundaries, which makes it possible to link tracts to legacy counties by spatial analysis.\nThere are situations where we may need to re-construct data for the legacy CT counties by aggregating data from Census tract-level data. For example, one may want to compare ACS S2801 (Types of Computers and Internet Subscriptions) data between 2021 and 2022 at CT county-level. A crosswalk to link 2022 Census tracts and legacy counties in Connecticut is useful - one can aggregate new data at tract-level to obtain data at legacy county-level.\nIn this blog post, we’ll explore how to link the 2022 Census tracts with the 2020 counties in Connecticut using spatial analysis. Specifically, we’ll achieve this through spatial join using the sf package in R. We can create the same crosswalk in Python, which we’ll explore in a future post."
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#ct-counties-2020",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#ct-counties-2020",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "CT counties 2020",
    "text": "CT counties 2020\n\n8 counties\nCounty FIPS codes: 001, 003, 005, 007, 009, 011, 013, 015\n\n\nggplot(data = ct_2020, aes(fill = NAME, text = NAME,  label = NAME)) + \n  geom_sf(color=\"black\",\n          size=0.25, show.legend = FALSE, alpha = 0.1) + \n  geom_sf_label(fill = NA,  # override the fill from aes()\n                fun.geometry = sf::st_centroid) +\n  theme_void()"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#ct-counties-2022",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#ct-counties-2022",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "CT counties 2022",
    "text": "CT counties 2022\n\n9 counties\nCounty FIPS codes: 110, 120, 130, 140, 150, 160, 170, 180, 190\n\n\nggplot(data = ct_2022, aes(fill = NAME, text = NAME,  label = NAME)) + \n  geom_sf(color=\"black\",\n          size=0.25, show.legend = FALSE, alpha=0.2) + \n  geom_sf_label(fill = \"white\",  # override the fill from aes()\n                fun.geometry = sf::st_centroid) +\n  theme_void()"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#county-2020-tract",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#county-2020-tract",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "2020 county + 2020 tract",
    "text": "2020 county + 2020 tract\n\nggplot() + \n  geom_sf(data = ct_tracts_2020, fill = NA) +\n  geom_sf(data = ct_2020, fill = NA, color=\"red\", linewidth = 1)+\n  theme_void()"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#county-2022-tract",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#county-2022-tract",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "2020 county + 2022 tract",
    "text": "2020 county + 2022 tract\n\nggplot() + \n  geom_sf(data = ct_tracts_2022, fill = NA) +\n  geom_sf(data = ct_2020, fill = NA, color=\"red\", linewidth = 1)+\n  theme_void()"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#county-2022-tract-1",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#county-2022-tract-1",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "2022 county + 2022 tract",
    "text": "2022 county + 2022 tract\n\nggplot() + \n  geom_sf(data = ct_tracts_2022, fill = NA) +\n  geom_sf(data = ct_2022, fill = NA, color=\"blue\", linewidth = 1)+\n  theme_void()"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#st_intersects-not-working",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#st_intersects-not-working",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "st_intersects: not working",
    "text": "st_intersects: not working\n\nct_sel &lt;- \"003\"\n\ntracts_sel &lt;- ct_tracts_2022 %&gt;% \n  st_join(ct_2020 %&gt;%  filter(COUNTYFP == ct_sel),\n          join = st_intersects,\n          left = FALSE,\n          suffix = c(\"_2022\", \"_2020\"))\n  \np &lt;- ggplot() + \n  geom_sf_interactive(data = tracts_sel, fill = NA, aes(tooltip = NAMELSAD_2022, data_id = TRACTCE) ) + \n  geom_sf_interactive(data = ct_2020 %&gt;%  filter(COUNTYFP == ct_sel), fill = NA, color=\"red\", linewidth = 1)+\n  theme_void()\n\nggiraph::girafe(ggobj = p\n                , width_svg = 8, height_svg = 5\n                ) %&gt;% \n  girafe_options(opts_hover(css = \"fill:cyan;\"),\n                 opts_zoom(min = .8, max = 5))\n\n\n\n\n\nThe boundary of county ‘003’ is the read curve. As we can see, st_intersects gives us not only tracts inside county ‘003’, but also those tracts that touches county boundary outside. Maybe we should try st_within."
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#st_within",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#st_within",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "st_within",
    "text": "st_within\n\nct_sel &lt;- \"003\"\n\ntracts_sel &lt;- ct_tracts_2022 %&gt;% \n  st_join(ct_2020 %&gt;%  filter(COUNTYFP == ct_sel),\n          join = st_within,\n          left = FALSE,\n          suffix = c(\"_2022\", \"_2020\"))\n  \nggplot() + \n  geom_sf(data = tracts_sel, fill = NA ) + \n  geom_sf(data = ct_2020 %&gt;%  filter(COUNTYFP == ct_sel), fill = NA, color=\"red\", linewidth = 1)+\n  theme_void()\n\n\n\n\nThis time, we do get the tracts totally inside the county, but tracts around the county boundary are not included. st_within returns only if a geometry is entirely inside a polygon. Those border tracts may not exactly match the county boundary, since we are dealing with 2022 tracts polygons and 2020 county polygons."
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#alternative-modified-st_within",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#alternative-modified-st_within",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "Alternative: modified st_within",
    "text": "Alternative: modified st_within\nThis stackoverflow post gave helpful information of how we can proceed. The key ideal is: use st_within to check if a point is within a polygon, instead of checking if a smaller polygon (i.e., a tract) is within a larger polygon (i.e., a county):\n\n“One alternative is to turn each tract into a point and intersect on those, which removes the coincident borders that gave me trouble.”\n“st_point_on_surface() is preferable to st_centroid() since it guarantees the point will lie on the surface of the polygon, whereas some polygons will have centroids lying outside its surface.”\n\nSo, we will use a point to represent a tract and then apply st_within.\n\nct_sel &lt;- \"003\"\n\n# get centroids of tracts; find tract points within a county by 'st_within'\npoints &lt;- ct_tracts_2022 %&gt;% \n  mutate(geo_tract = geometry) %&gt;% \n  mutate(geometry = st_point_on_surface(geometry)) %&gt;% \n  st_join(ct_2020 %&gt;%  filter(COUNTYFP == ct_sel),\n          join = st_within,\n          left = FALSE,\n          suffix = c(\"_2022\", \"_2020\")) \n\n# \ntracts_within &lt;- ct_tracts_2022 %&gt;% \n  mutate(geo_tract = geometry) %&gt;% \n  mutate(geometry = st_point_on_surface(geometry)) %&gt;% \n  st_join(ct_2020 %&gt;%  filter(COUNTYFP == ct_sel),\n          join = st_within,\n          left = FALSE,\n          suffix = c(\"_2022\", \"_2020\")) %&gt;% \n  mutate(geometry = geo_tract ) \n\ntracts_within %&gt;% \n  ggplot() +\n  geom_sf(fill = NA) + # tracts\n  geom_sf(data = points, size = 1, color = 'blue') + # points\n  geom_sf(data = ct_2020 %&gt;%  filter(COUNTYFP == ct_sel), fill = NA, color=\"red\", linewidth = 1)+ # county\n  theme_void()\n\n\n\n\nGreat! It worked. Now we can work on linking 2022 tracts to 2020 counties."
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#function",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#function",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "Function",
    "text": "Function\nLet’s write a function to find 2022 tracts in a 2020 county. We’ll call it get_tracts_in_county.\n\nget_tracts_in_county &lt;- function(tracts, county, ct_fips){\n  tracts_in_cnty &lt;- tracts %&gt;% \n    mutate(geometry = st_point_on_surface(geometry)) %&gt;% \n    st_join(county %&gt;%  filter(COUNTYFP == ct_fips),\n            join = st_within,\n            left = FALSE,\n            suffix = c(\"_2022\", \"_2020\"))  %&gt;% \n    select(statecode = STATEFP_2022, countycode_2022 = COUNTYFP_2022, tract_fips_2022 = GEOID_2022, countycode_2020 = COUNTYFP_2020) %&gt;% \n    st_drop_geometry()\n  \n  return(tracts_in_cnty)\n}"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#crosswalk",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#crosswalk",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "Crosswalk",
    "text": "Crosswalk\nWe can now use the function to create the crosswalk between 2022 tracts and 2020 counties.\n\nlinked_tracts22_counties20 &lt;- ct_2020 %&gt;% \n  pull(COUNTYFP) %&gt;% \n  str_sort() %&gt;% \n  map(~get_tracts_in_county(tracts = ct_tracts_2022, county = ct_2020, ct_fips = .x)) %&gt;% \n  list_rbind()\n\nlinked_tracts22_counties20 %&gt;% head()\n\n  statecode countycode_2022 tract_fips_2022 countycode_2020\n1        09             190     09190022201             001\n2        09             190     09190022202             001\n3        09             190     09190022101             001\n4        09             190     09190022102             001\n5        09             190     09190020302             001\n6        09             190     09190020301             001"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#get-data",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#get-data",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "Get data",
    "text": "Get data\n\nCT_tractcrosswalk2022 &lt;- read_csv(\"https://raw.githubusercontent.com/CT-Data-Collaborative/2022-tract-crosswalk/main/2022tractcrosswalk.csv\") %&gt;% \n  glimpse()\n\nRows: 879 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): tract_fips_2020, Tract_fips_2022, town_name, town_fips_2020, town_...\ndbl  (3): tract_name, PUMA2020code, school_district_code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nRows: 879\nColumns: 15\n$ tract_fips_2020      &lt;chr&gt; \"09009350400\", \"09009350500\", \"09009352701\", \"090…\n$ Tract_fips_2022      &lt;chr&gt; \"09140350400\", \"09140350500\", \"09140352701\", \"091…\n$ tract_name           &lt;dbl&gt; 3504.00, 3505.00, 3527.01, 3527.02, 3528.00, 1660…\n$ town_name            &lt;chr&gt; \"Waterbury\", \"Waterbury\", \"Waterbury\", \"Waterbury…\n$ town_fips_2020       &lt;chr&gt; \"0900980070\", \"0900980070\", \"0900980070\", \"090098…\n$ town_fips_2022       &lt;chr&gt; \"0914080070\", \"0914080070\", \"0914080070\", \"091408…\n$ county_name          &lt;chr&gt; \"New Haven\", \"New Haven\", \"New Haven\", \"New Haven…\n$ county_fips_2020     &lt;chr&gt; \"09009\", \"09009\", \"09009\", \"09009\", \"09009\", \"090…\n$ ce_name_2022         &lt;chr&gt; \"Naugatuck Valley Planning Region\", \"Naugatuck Va…\n$ ce_fips_2022         &lt;chr&gt; \"09140\", \"09140\", \"09140\", \"09140\", \"09140\", \"091…\n$ PUMA2020code         &lt;dbl&gt; 20601, 20601, 20601, 20601, 20601, 20503, 20504, …\n$ PUMA2020name         &lt;chr&gt; \"Waterbury Town\", \"Waterbury Town\", \"Waterbury To…\n$ school_district_code &lt;dbl&gt; 151, 151, 151, 151, 151, 62, 76, 108, 151, 166, 6…\n$ school_district_name &lt;chr&gt; \"Waterbury\", \"Waterbury\", \"Waterbury\", \"Waterbury…\n$ zip5_zcta2020        &lt;chr&gt; \"06704\", \"06706\", \"06705\", \"06705\", \"06706\", \"065…\n\nCT_tract22_county20_crosswalk &lt;- CT_tractcrosswalk2022 %&gt;% \n  janitor::clean_names() %&gt;% \n  select(tract_fips_2020, tract_fips_2022, county_fips_2020) %&gt;% \n  filter(tract_fips_2020 != tract_fips_2022) %&gt;% \n  mutate(tract_fips_2022 = str_pad(tract_fips_2022, 11, \"left\", \"0\"),\n         tract_fips_2020 = str_pad(tract_fips_2020, 11, \"left\", \"0\"),\n         county_fips_2020 = str_pad(county_fips_2020, 5, \"left\", \"0\"))\n\nCT_tract22_county20_crosswalk %&gt;% head()\n\n# A tibble: 6 × 3\n  tract_fips_2020 tract_fips_2022 county_fips_2020\n  &lt;chr&gt;           &lt;chr&gt;           &lt;chr&gt;           \n1 09009350400     09140350400     09009           \n2 09009350500     09140350500     09009           \n3 09009352701     09140352701     09009           \n4 09009352702     09140352702     09009           \n5 09009352800     09140352800     09009           \n6 09009166002     09170166002     09009"
  },
  {
    "objectID": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#compare-with-home-made-dict",
    "href": "posts/2024-01-14-create-a-crosswalk-between-2022-census-tracts-and-2020-counties-in-connecticut-by-spatial-join/index.html#compare-with-home-made-dict",
    "title": "Create a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join",
    "section": "Compare with home-made dict",
    "text": "Compare with home-made dict\nWe can use anti_join to see the differences.\n\nlinked_tracts22_counties20 %&gt;% \n  anti_join(CT_tract22_county20_crosswalk,\n            by = 'tract_fips_2022')\n\n  statecode countycode_2022 tract_fips_2022 countycode_2020\n1        09             120     09120990000             001\n2        09             190     09190990000             001\n3        09             130     09130990100             007\n4        09             170     09170990000             009\n5        09             180     09180990100             011\n\nCT_tract22_county20_crosswalk %&gt;% \n  anti_join(linked_tracts22_counties20,\n            by = 'tract_fips_2022')\n\n# A tibble: 0 × 3\n# ℹ 3 variables: tract_fips_2020 &lt;chr&gt;, tract_fips_2022 &lt;chr&gt;,\n#   county_fips_2020 &lt;chr&gt;\n\n\nOverall, our data matched CT Data Collaborative data for 879 rows, in terms of tract FIPS codes, 2020 and 2022 county FIPS codes. Our data has 5 more tracts compared with CT Data Collaborative data. They excluded those 5 tracts according to following notes:\n\n“Note that tracts that contained only water and were not within municipal boundaries are excluded from this crosswalk. These are 2020 tracts 09001990000, 09007990100, 09009990000, and 09011990100.”"
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html",
    "title": "Parallel Computing in R",
    "section": "",
    "text": "As datasets grow in size and analyses become more complex, the need for efficient computing becomes increasingly crucial. Parallel computing offers a solution by leveraging multiple processors or cores to execute tasks simultaneously, thereby reducing computation time. In this post, we will explore how to harness the power of parallel computing in R using packages, including foreach and furrr."
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#get-data",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#get-data",
    "title": "Parallel Computing in R",
    "section": "Get data",
    "text": "Get data\nThere are over 660,000 rows and 15 columns in the trend data. We will use these 5 columns: ‘measureid’, ‘yearspan’,‘statecode’, ‘countycode’, and ‘rawvalue’. For simplicity, we will remove rows with ‘rawvalue’ not missing for county level data (i.e., ‘countycode’ not ‘000’).\n\ntrend_data &lt;- readr::read_csv(\"https://www.countyhealthrankings.org/sites/default/files/media/document/chr_trends_csv_2023.csv\",\n                              show_col_types = FALSE) %&gt;% \n  select(measureid, yearspan, statecode, countycode, rawvalue) %&gt;% \n  filter(!is.na(rawvalue), countycode != '000') %&gt;% \n  glimpse()\n\nRows: 639,621\nColumns: 5\n$ measureid  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ yearspan   &lt;chr&gt; \"1997-1999\", \"1998-2000\", \"1999-2001\", \"2000-2002\", \"2001-2…\n$ statecode  &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\",…\n$ countycode &lt;chr&gt; \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"001\", \"00…\n$ rawvalue   &lt;dbl&gt; 8723.90, 8269.20, 7816.80, 8936.80, 9427.10, 9562.90, 9326.…\n\n\nWe still have almost 640,000 rows. Let us explore the data a little bit more.\n\nNumber of counties: 3143\n\n\ntrend_data %&gt;% \n  distinct(statecode, countycode) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  3143\n\n\n\nNumber of measures: 15\n\n\ntrend_data %&gt;% \n  distinct(measureid) %&gt;% \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1    15\n\n\nWe will find the trend for each county by each measure using linear regression \\(y = kx + b\\) (‘year’ will be x, and ‘rawvalue’ will be y). Not all counties have data for all 15 measures. But still, we will need to do ~40,000 linear regressions.\n\ntrend_data %&gt;% head()\n\n# A tibble: 6 × 5\n  measureid yearspan  statecode countycode rawvalue\n      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;         &lt;dbl&gt;\n1         1 1997-1999 01        001           8724.\n2         1 1998-2000 01        001           8269.\n3         1 1999-2001 01        001           7817.\n4         1 2000-2002 01        001           8937.\n5         1 2001-2003 01        001           9427.\n6         1 2002-2004 01        001           9563."
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#prepare-data",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#prepare-data",
    "title": "Parallel Computing in R",
    "section": "Prepare data",
    "text": "Prepare data\n\nConstruct a column ‘fipscode’ as an ID for each county, and remove ‘statecode’, ‘countycode’\nExtract ‘year’ from ‘yearspan’\nCentralize the years so that the central year is 0; ‘year’ will be used as the ‘x’ in the linear regression.\n\n\ntrend_data_clean &lt;- trend_data %&gt;% \n  mutate(fipscode = paste0(statecode, countycode), .before = 1) %&gt;% \n  select(-c(statecode, countycode)) %&gt;% \n  separate(yearspan, into = c('year1', 'year2')) %&gt;% \n  mutate(year = case_when(\n    is.na(year2) ~ as.numeric(year1),\n    TRUE ~ (as.numeric(year1) + as.numeric(year2))/2\n  )) %&gt;% \n  mutate(year = round(scale(year, scale = FALSE), digits = 1) ) %&gt;% \n  select(-c(year1, year2))\n\nWarning: Expected 2 pieces. Missing pieces filled with `NA` in 573431 rows [66191,\n66192, 66193, 66194, 66195, 66196, 66197, 66198, 66199, 66200, 66201, 66202,\n66203, 66204, 66205, 66206, 66207, 66208, 66209, 66210, ...].\n\ntrend_data_clean \n\n# A tibble: 639,621 × 4\n   fipscode measureid rawvalue year[,1]\n   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 01001            1    8724.      -15\n 2 01001            1    8269.      -14\n 3 01001            1    7817.      -13\n 4 01001            1    8937.      -12\n 5 01001            1    9427.      -11\n 6 01001            1    9563.      -10\n 7 01001            1    9327.       -9\n 8 01001            1    9577.       -8\n 9 01001            1    9712.       -7\n10 01001            1    9647.       -6\n# ℹ 639,611 more rows\n\n\nLet us create a list of county FIPS codes, which will be used later.\n\ncnty_list &lt;- trend_data_clean %&gt;% \n  distinct(fipscode) %&gt;% \n  pull(fipscode)"
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#lm-for-one-county-one-measure",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#lm-for-one-county-one-measure",
    "title": "Parallel Computing in R",
    "section": "lm() for one county, one measure",
    "text": "lm() for one county, one measure\nWe will use county ‘01001’ and measure 1 as an example. Our goal is to get estimates and p-values for intercept and x (i.e., ‘year’).\n\ncnyt_test &lt;- trend_data_clean %&gt;% \n  filter(fipscode == \"01001\", measureid == 1)\n\ncnyt_test\n\n# A tibble: 22 × 4\n   fipscode measureid rawvalue year[,1]\n   &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 01001            1    8724.      -15\n 2 01001            1    8269.      -14\n 3 01001            1    7817.      -13\n 4 01001            1    8937.      -12\n 5 01001            1    9427.      -11\n 6 01001            1    9563.      -10\n 7 01001            1    9327.       -9\n 8 01001            1    9577.       -8\n 9 01001            1    9712.       -7\n10 01001            1    9647.       -6\n# ℹ 12 more rows\n\n\n\nlm_test &lt;-  lm(rawvalue ~ year, data = cnyt_test)\n\nlm_test %&gt;% \n  broom::tidy() \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   8726.      159.      55.0  2.61e-23\n2 year           -24.3      20.4     -1.19 2.47e- 1\n\n\nWe want to save estimate and p.value from the regression. Let us wrap the above code into a function.\n\nget_county_reg &lt;- function(df_cnty){\n  \n  cnty_reg &lt;-  lm(rawvalue ~ year, data = df_cnty)\n  \n  return(\n    cnty_reg %&gt;% \n      broom::tidy() %&gt;% \n      select(term, estimate, p_value=p.value)\n  )\n} \n\nLet us now test this function.\n\nget_county_reg(df_cnty = cnyt_test)\n\n# A tibble: 2 × 3\n  term        estimate  p_value\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   8726.  2.61e-23\n2 year           -24.3 2.47e- 1\n\n\nIt looks good."
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#lm-for-one-county-for-all-15-measures",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#lm-for-one-county-for-all-15-measures",
    "title": "Parallel Computing in R",
    "section": "lm() for one county for all 15 measures",
    "text": "lm() for one county for all 15 measures\nThe function get_county_reg returns a dataframe. We can use the idea of list-column data structure in Chapter 25 in the 1st ediction of the book R for Data Science and save the model results as dataframes in a dataframe.\nAnd we want to have regression results of 15 measures for a county.\n\ncnty_test_15m &lt;- trend_data_clean %&gt;% \n  filter(fipscode == \"01001\") %&gt;% \n  group_by(fipscode, measureid) %&gt;% \n  nest() %&gt;%  \n  mutate(model_res = map(data, get_county_reg)) %&gt;% \n  ungroup()\n\ncnty_test_15m\n\n# A tibble: 15 × 4\n   fipscode measureid data              model_res       \n   &lt;chr&gt;        &lt;dbl&gt; &lt;list&gt;            &lt;list&gt;          \n 1 01001            1 &lt;tibble [22 × 2]&gt; &lt;tibble [2 × 3]&gt;\n 2 01001            3 &lt;tibble [13 × 2]&gt; &lt;tibble [2 × 3]&gt;\n 3 01001            4 &lt;tibble [11 × 2]&gt; &lt;tibble [2 × 3]&gt;\n 4 01001            5 &lt;tibble [9 × 2]&gt;  &lt;tibble [2 × 3]&gt;\n 5 01001           23 &lt;tibble [20 × 2]&gt; &lt;tibble [2 × 3]&gt;\n 6 01001           24 &lt;tibble [20 × 2]&gt; &lt;tibble [2 × 3]&gt;\n 7 01001           45 &lt;tibble [14 × 2]&gt; &lt;tibble [2 × 3]&gt;\n 8 01001           50 &lt;tibble [9 × 2]&gt;  &lt;tibble [2 × 3]&gt;\n 9 01001           85 &lt;tibble [13 × 2]&gt; &lt;tibble [2 × 3]&gt;\n10 01001           88 &lt;tibble [12 × 2]&gt; &lt;tibble [2 × 3]&gt;\n11 01001          122 &lt;tibble [13 × 2]&gt; &lt;tibble [2 × 3]&gt;\n12 01001          125 &lt;tibble [18 × 2]&gt; &lt;tibble [2 × 3]&gt;\n13 01001          134 &lt;tibble [13 × 2]&gt; &lt;tibble [2 × 3]&gt;\n14 01001          155 &lt;tibble [9 × 2]&gt;  &lt;tibble [2 × 3]&gt;\n15 01001          169 &lt;tibble [12 × 2]&gt; &lt;tibble [2 × 3]&gt;\n\n\nLet us check the results in column ‘model_res’.\n\ncnty_test_15m$model_res[[1]]\n\n# A tibble: 2 × 3\n  term        estimate  p_value\n  &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   8726.  2.61e-23\n2 year           -24.3 2.47e- 1\n\n\nYes, the results are there. Let us wrap this part as a function.To be more efficient, we can remove the column data after regression is done. We will do that in the function below.\n\nget_cnty_reg_15_m &lt;- function(df_trend, cnty_fipscode){\n  \n  cnty_reg_15m &lt;- df_trend %&gt;% \n    filter(fipscode == cnty_fipscode) %&gt;% \n    group_by(fipscode, measureid) %&gt;% \n    nest() %&gt;% \n    mutate(model_res = map(data, get_county_reg)) %&gt;% \n    ungroup() %&gt;% \n    select(-data)\n  \n  return(cnty_reg_15m)\n  \n}\n\nTest function:\n\nget_cnty_reg_15_m(df_trend = trend_data_clean, cnty_fipscode = \"01001\")\n\n# A tibble: 15 × 3\n   fipscode measureid model_res       \n   &lt;chr&gt;        &lt;dbl&gt; &lt;list&gt;          \n 1 01001            1 &lt;tibble [2 × 3]&gt;\n 2 01001            3 &lt;tibble [2 × 3]&gt;\n 3 01001            4 &lt;tibble [2 × 3]&gt;\n 4 01001            5 &lt;tibble [2 × 3]&gt;\n 5 01001           23 &lt;tibble [2 × 3]&gt;\n 6 01001           24 &lt;tibble [2 × 3]&gt;\n 7 01001           45 &lt;tibble [2 × 3]&gt;\n 8 01001           50 &lt;tibble [2 × 3]&gt;\n 9 01001           85 &lt;tibble [2 × 3]&gt;\n10 01001           88 &lt;tibble [2 × 3]&gt;\n11 01001          122 &lt;tibble [2 × 3]&gt;\n12 01001          125 &lt;tibble [2 × 3]&gt;\n13 01001          134 &lt;tibble [2 × 3]&gt;\n14 01001          155 &lt;tibble [2 × 3]&gt;\n15 01001          169 &lt;tibble [2 × 3]&gt;\n\n\nIt worked. Let us now do lm() for all counties and all measures, exploring both sequential and parallel ways."
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#for-loop",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#for-loop",
    "title": "Parallel Computing in R",
    "section": "for loop",
    "text": "for loop\nWe will first use a for loop and time the process. It took about 4 minutes.\n\ntictoc::tic()\n\nn &lt;- length(cnty_list)\n\n# create an empty list to save results\nlst_trend_res &lt;- vector(\"list\", length = n)\n\nfor (i in 1:n) {\n  # print(paste0(\"i=\", i, \"  \", cnty_list[i]) )\n  \n   # call function to do regression\n  dat &lt;- get_cnty_reg_15_m(df_trend = trend_data_clean, cnty_fipscode = cnty_list[i])\n  \n  # save data to list \n  lst_trend_res[[i]] &lt;- dat\n}\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `model_res = map(data, get_county_reg)`.\nℹ In group 11: `fipscode = \"25021\"` and `measureid = 122`.\nCaused by warning in `summary.lm()`:\n! essentially perfect fit: summary may be unreliable\n\n# bind rows for each dataset\nall_trend_res_1 &lt;- bind_rows(lst_trend_res ) %&gt;% \n  unnest(model_res)\n\ntictoc::toc()\n\n226.09 sec elapsed\n\n\n\nall_trend_res_1 %&gt;% head()\n\n# A tibble: 6 × 5\n  fipscode measureid term          estimate  p_value\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n1 01001            1 (Intercept) 8726.      2.61e-23\n2 01001            1 year         -24.3     2.47e- 1\n3 01001            3 (Intercept)    0.151   3.52e-12\n4 01001            3 year          -0.00560 7.99e- 4\n5 01001            4 (Intercept) 2565.      2.17e-11\n6 01001            4 year         -66.5     4.07e- 3"
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#map",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#map",
    "title": "Parallel Computing in R",
    "section": "map",
    "text": "map\nNow, we will try purrr::map(). A little faster than for loop. And the code is cleaner.\n\ntictoc::tic()\n\nall_trend_res_2 &lt;- cnty_list %&gt;% \n  map( ~get_cnty_reg_15_m(df_trend = trend_data_clean, cnty_fipscode = .)) %&gt;% \n  list_rbind() %&gt;% \n  unnest(model_res)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `model_res = map(data, get_county_reg)`.\nℹ In group 11: `fipscode = \"25021\"` and `measureid = 122`.\nCaused by warning in `summary.lm()`:\n! essentially perfect fit: summary may be unreliable\n\ntictoc::toc() \n\n235.02 sec elapsed"
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#foreach",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#foreach",
    "title": "Parallel Computing in R",
    "section": "foreach",
    "text": "foreach\nNow let see if we can save time by parallazing the process with foreach. Yes, we can: the time dropped to ~ 1 minute.\n\ntictoc::tic()\n\nn &lt;- length(cnty_list)\n\nall_trend_res_3 &lt;-  \n  foreach (i= 1:n, \n         .combine=rbind,\n         .packages='tidyverse') %dopar% {\n   # call function to do regression\n  get_cnty_reg_15_m(df_trend = trend_data_clean, cnty_fipscode = cnty_list[i])\n\n}\n\nall_trend_res_3 &lt;- all_trend_res_3 %&gt;%\n  unnest(model_res)\n\ntictoc::toc()\n\n94.82 sec elapsed\n\n\n\nall_trend_res_3 %&gt;% \n  head()\n\n# A tibble: 6 × 5\n  fipscode measureid term          estimate  p_value\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n1 01001            1 (Intercept) 8726.      2.61e-23\n2 01001            1 year         -24.3     2.47e- 1\n3 01001            3 (Intercept)    0.151   3.52e-12\n4 01001            3 year          -0.00560 7.99e- 4\n5 01001            4 (Intercept) 2565.      2.17e-11\n6 01001            4 year         -66.5     4.07e- 3"
  },
  {
    "objectID": "posts/2024-03-15-parallel-computing-in-r/index.html#future_map",
    "href": "posts/2024-03-15-parallel-computing-in-r/index.html#future_map",
    "title": "Parallel Computing in R",
    "section": "future_map",
    "text": "future_map\nFinally, let us try future_map. Less than 1 minute, even faster than as foreach. Again, the code is cleaner.\n\ntictoc::tic()\n\nall_trend_res_4 &lt;- cnty_list %&gt;% \n  future_map( ~get_cnty_reg_15_m(df_trend = trend_data_clean, cnty_fipscode = .)) %&gt;% \n  list_rbind() %&gt;% \n  unnest(model_res)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `model_res = map(data, get_county_reg)`.\nℹ In group 11: `fipscode = \"25021\"` and `measureid = 122`.\nCaused by warning in `summary.lm()`:\n! essentially perfect fit: summary may be unreliable\n\ntictoc::toc()\n\n96.59 sec elapsed\n\n\n\nall_trend_res_4 %&gt;% head()\n\n# A tibble: 6 × 5\n  fipscode measureid term          estimate  p_value\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;\n1 01001            1 (Intercept) 8726.      2.61e-23\n2 01001            1 year         -24.3     2.47e- 1\n3 01001            3 (Intercept)    0.151   3.52e-12\n4 01001            3 year          -0.00560 7.99e- 4\n5 01001            4 (Intercept) 2565.      2.17e-11\n6 01001            4 year         -66.5     4.07e- 3"
  },
  {
    "objectID": "posts/2024-03-18-reading-fixed-width-files-in-sas-r-and-python/index.html",
    "href": "posts/2024-03-18-reading-fixed-width-files-in-sas-r-and-python/index.html",
    "title": "Reading Fixed-Width Files in SAS, R, and Python",
    "section": "",
    "text": "Introduction\nA fixed-width data file is a type of structured data format where each column within the file has a specific, predetermined width. In contrast to delimited files like CSV (comma-separated values), where columns are separated by a delimiter character such as a comma or tab, fixed-width files organize data by allocating a fixed number of characters for each column. In this post, we’ll explore how to read fixed-width files using SAS, R, and Python.\n\n\nAbout Fixed-Width Files\nFixed-width files, characterized by the allocation of specific widths for each column, offer several advantages and limitations:\n\nStore Specific Information in Fixed Columns:\n\nIn fixed-width files, data is organized with precision, with each column allocated a predetermined width. This structured approach ensures that specific types of information are consistently stored in their designated columns. For example, in a dataset containing personal information, the first 10 characters might be reserved for the first name, the next 15 for the last name, and so forth.\n\nSpeed and Low Resource Consumption:\n\nDue to their simple structure, fixed-width files are typically processed more quickly and with lower resource consumption compared to more complex file formats. This efficiency is especially advantageous when working with large datasets or in environments with limited computational resources.\n\nRaw Data Without Additional Formatting:\n\nFixed-width files contain nothing but raw data, devoid of any additional formatting or metadata commonly found in other file formats. This raw nature simplifies the storage and exchange of data but requires careful handling during processing to extract meaningful insights.\n\nLack of Variable/Column/Field Names, Labels, or Tags:\n\nUnlike other data formats such as CSV or Excel, fixed-width files do not inherently include variable names, column labels, or any form of tagging to identify the data within each column. As a result, navigating and interpreting the data can be challenging without prior knowledge of the file’s structure. It is thus crucial to understand the variables and columns from the file documentation when working on a fixed-width data file.\n\n\nThe Dataset: Vital Statistics Online Data Portal\nTo illustrate the process of reading fixed-width files, let’s use a dataset from the ‘Vital Statistics Online Data Portal’. This dataset contains vital statistics information, such as birth and death records, organized in a fixed-width format. Our task is to extract meaningful insights from this data using SAS, R, and Python. We will use 2022 U.S. Territories Birth Data, which is a text file about 30 MB after unzipping. If we open the txt file in an text editor, we will see things as following:\n\n\n\n\n\nThere are columns of numbers and letters, but no column headers. We need the documentation before we can properly read and use the data. If we go to page 9 of the documentation (a screenshot shown below), we see ‘202201’ actually are for two columns: Birth Year and Birth Month.\n\n\n\n\n\nThe documentation is the ‘map’ that we use to identify each column in the data file. Assume we want to extract the following information from the data: Birth Year, Birth Month, Mother’s Single Years of Age, Mother’s Race Recode 31, Mother’s Hispanic Origin Recode. Then we need to find their positions in the documentation. Let us list them in a table:\n\n\n\n\n\n\n  \n    \n      variable\n      position\n      width\n    \n  \n  \n    Birth Year\n9-12\n4\n    Birth Month\n13-14\n2\n    Mother’s Single Years of Age\n75-76\n2\n    Mother’s Race Recode 31\n105-106\n2\n    Mother’s Hispanic Origin Recode\n115\n1\n  \n  \n  \n\n\n\n\n\n\nSAS\nWe can use a data step to read in a fixed-width file. One important thing is to specify the positions of columns to read. We can the following code to read the sample data, assuming we put the txt file at ‘C:’.\ndata Nat2022; infile 'C:\\Nat2022PublicPS.c20230516.r20231002.txt';\ninput \n   @9  birth_year $4.\n   @13  birth_month $2.\n   @75  m_age 2.\n   @105 m_race_recode_31 $2.\n   @115 m_hispanic_origin_recode $1.\n   ; \nrun;\nHere is a screenshot of the first 10 rows in the dataset ‘Nat2022’:\n\n\n\n\n\n\n\nR\nNow let us read in the fixed-width file using read_fwf() function from the readr package. We will use the start and end positions for each column.\n\nNat2022_r &lt;- read_fwf(file = \"Nat2022PublicPS.c20230516.r20231002.txt\", \n                    col_positions =  fwf_positions(\n                      start = c(9, 13, 75, 105, 115),\n                      end = c(12, 14, 76, 106, 115),\n                      col_names = c(\"birth_year\", \"birth_month\", \"m_age\", \"m_race_recode_31\", \"m_hispanic_origin_recode\")),\n                    show_col_types = FALSE)\n\nNat2022_r %&gt;% head(10)\n\n# A tibble: 10 × 5\n   birth_year birth_month m_age m_race_recode_31 m_hispanic_origin_recode\n        &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;                               &lt;dbl&gt;\n 1       2022 01             33 10                                      5\n 2       2022 01             28 04                                      0\n 3       2022 01             24 01                                      0\n 4       2022 01             30 05                                      0\n 5       2022 01             28 04                                      0\n 6       2022 01             32 05                                      0\n 7       2022 01             28 05                                      0\n 8       2022 01             33 04                                      0\n 9       2022 01             38 04                                      0\n10       2022 01             29 05                                      0\n\n\n\n\nPython\nWe can read fixed-width files in Python using the read_fwf function in Pandas. Again, we need to specify the widths of each column in the file, just like we did in SAS or R.One thing to note that the index in Python starts from 0, not 1, and we need to adjust the positions by 1. Also, we can directly read the data using the URL, without downloading the file.\nimport pandas as pd\n\ncolspecs = [(9-1, 12), (13-1, 14), (75-1, 76), (105-1, 106), (115-1, 115)]  # define column widths\ncols_names = [\"birth_year\", \"birth_month\", \"m_age\", \"m_race_recode_31\", \"m_hispanic_origin_recode\"]\n\nnat2022_py = pd.read_fwf('https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/natality/Nat2022ps.zip', \n            encoding = 'utf_8', \n            index_col = False,\n            compression='zip',\n            colspecs=colspecs, \n            names= cols_names)\n\nnat2022_py.head(10)\n\n\n\n\n\n\n\nConclusion\nIn this post, we’ve explored the reading fixed-width files in SAS, R, and Python using a dataset from the ‘Vital Statistics Online Data Portal’. The techniques outlined here can be helpful for processing fixed-width data efficiently. Whether you prefer SAS, R, or Python, one critical thing to handle fixed-width files is to know the file structure and identify the positions of columns in the data."
  },
  {
    "objectID": "posts/2024-03-18-reading-fixed-width-files-in-sas-r-and-python/read_fixed_width_file.html",
    "href": "posts/2024-03-18-reading-fixed-width-files-in-sas-r-and-python/read_fixed_width_file.html",
    "title": "Ganhua Lu",
    "section": "",
    "text": "import pandas as pd\n\ncolspecs = [(9-1, 12), (13-1, 14), (75-1, 76), (105-1, 106), (115-1, 115)]  # define column widths\ncols_names = [\"birth_year\", \"birth_month\", \"m_age\", \"m_race_recode_31\", \"m_hispanic_origin_recode\"]\n\nnat2022_py = pd.read_fwf('https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/DVS/natality/Nat2022ps.zip', \n            encoding = 'utf_8', \n            index_col = False,\n            compression='zip',\n            colspecs=colspecs, \n            names= cols_names)\n\nnat2022_py.head(10)\n\n\n\n\n\n\n\n\nbirth_year\nbirth_month\nm_age\nm_race_recode_31\nm_hispanic_origin_recode\n\n\n\n\n0\n2022\n1\n33\n10\n5\n\n\n1\n2022\n1\n28\n4\n0\n\n\n2\n2022\n1\n24\n1\n0\n\n\n3\n2022\n1\n30\n5\n0\n\n\n4\n2022\n1\n28\n4\n0\n\n\n5\n2022\n1\n32\n5\n0\n\n\n6\n2022\n1\n28\n5\n0\n\n\n7\n2022\n1\n33\n4\n0\n\n\n8\n2022\n1\n38\n4\n0\n\n\n9\n2022\n1\n29\n5\n0"
  },
  {
    "objectID": "posts/2024-08-27-using-the-substr-function-to-create-new-variables-in-sas-a-lengthy-matter/index.html",
    "href": "posts/2024-08-27-using-the-substr-function-to-create-new-variables-in-sas-a-lengthy-matter/index.html",
    "title": "Using the SUBSTR Function to Create New Variables in SAS: A Lengthy Matter",
    "section": "",
    "text": "Introduction\nWhen working with data in SAS, the SUBSTR function is a powerful tool for extracting substrings from character variables. However, when using SUBSTR to create new variables, a critical detail can easily be overlooked — the length of the newly created variable.\n\n\nUnderstanding SUBSTR and Variable Length\nThe SUBSTR function allows you to extract a portion of a string based on a specified starting position and length. Let’s explore this with a dataset containing FIPS codes and names for the state of Wisconsin and some of its counties:\ndata have;\n    input fipscode $ name $20.;\n    datalines;\n55000 Wisconsin\n55001 Adams County\n55003 Ashland County\n55005 Barron County\n55007 Bayfield County\n55009 Brown County\n55011 Buffalo County\n;\nrun;\nSuppose you want to create two new variables: statecode (the first 2 characters in the FIPS code) and countycode (the last 3 characters in the FIPS code). Here’s how you might approach this:\ndata want;\n    set have;\n    statecode = substr(fipscode, 1, 2);\n    countycode = substr(fipscode, 3, 3);\nrun;\n\n\nThe Importance of Setting Variable Length\nIn the code above, statecode and countycode are created using the SUBSTR function. However, both of these new variables will inherit the length of the original fipscode variable, which is 5 characters. As we can see here:\n\n\n\n\n\nColumn_Name\nColumn_Type\nColumn_Length\n\n\n\n\nfipscode\nchar\n5\n\n\nname\nchar\n20\n\n\nstatecode\nchar\n5\n\n\ncountycode\nchar\n5\n\n\n\n\n\n\n\nWhy Does This Happen? It is because SAS retains the length of the original variable when creating a new one using SUBSTR, or as stated in SAS documentation:\n\nIn a DATA step, if the SUBSTR (right of =) function returns a value to a variable that has not previously been assigned a length, then that variable is given the length of the first argument. (SAS documentation)\n\nThis can lead to inefficiencies in your data processing:\n\nWasted Space: If the original variable was significantly longer, the new variable will occupy more storage space than necessary.\nUnintended Data: Any characters beyond the length of the extracted substring will remain in the new variable, often filled with blanks, which may cause issues in data analysis and reporting.\n\n\n\nSetting the Length Correctly\nTo avoid these pitfalls, we can explicitly set the length of the new variable before using SUBSTR. Here’s how you can do it:\ndata want_better;\n    set counties;\n    length statecode $2 countycode $3;\n    statecode = substr(fipscode, 1, 2);\n    countycode = substr(fipscode, 3, 3);\nrun;\nBy specifying length statecode $2 countycode $3;, you ensure that statecode is 2 characters long and countycode is 3 characters long, which are the correct sizes. This optimizes the dataset for both storage and processing. If we check the attributes of variables in want_better, we see countycode and statecode have the desired lengths:\n\n\n\n\n\nColumn_Name\nColumn_Type\nColumn_Length\n\n\n\n\nstatecode\nchar\n2\n\n\ncountycode\nchar\n3\n\n\nfipscode\nchar\n5\n\n\nname\nchar\n20\n\n\n\n\n\n\n\n\n\nSummary\nWhen using SUBSTR to create new variables in SAS, always remember to set the length of the new variable appropriately. This small step can prevent inefficiencies and unexpected behavior in our data processing. It’s a simple yet good practice for anyone looking to write clean, efficient SAS code."
  },
  {
    "objectID": "posts/2024-10-26-understanding-icd-10-codes-and-the-113-causes-of-death-in-cdc-wonder/index.html",
    "href": "posts/2024-10-26-understanding-icd-10-codes-and-the-113-causes-of-death-in-cdc-wonder/index.html",
    "title": "Building a Crosswalk: Linking ICD-10 Codes with the Rankable Causes of Death in CDC WONDER",
    "section": "",
    "text": "The International Classification of Diseases, 10th Revision (ICD-10) is a globally recognized system for classifying diseases and health conditions. Each ICD-10 code represents a specific disease, making this coding system essential for standardizing mortality statistics. By providing a universal framework for categorizing and tracking causes of death, ICD-10 codes enable health organizations to monitor mortality trends, identify leading causes of death, and analyze patterns across diverse populations.\nThe National Center for Health Statistics (NCHS) uses ICD-10 codes in its CDC WONDER databases (e.g., Underlying Cause of Death) to organize raw death data into accessible and usable mortality statistics. This process enables users to query and analyze data on causes of death based on geography, age, sex, race, and other demographics.\nIn this post, we begin with a brief introduction toICD-10 codes, the 113 Causes of Death (with a focus on rankable causes), and the relationship between ICD-10 codes and these causes. We then explore how to build a crosswalk between ICD-10 codes and rankable causes using both R and Python. In an upcoming post, we will use this crosswalk to analyze raw mortality data and to obtain the leading causes of death in the U.S."
  },
  {
    "objectID": "posts/2024-10-26-understanding-icd-10-codes-and-the-113-causes-of-death-in-cdc-wonder/index.html#r",
    "href": "posts/2024-10-26-understanding-icd-10-codes-and-the-113-causes-of-death-in-cdc-wonder/index.html#r",
    "title": "Building a Crosswalk: Linking ICD-10 Codes with the Rankable Causes of Death in CDC WONDER",
    "section": "R",
    "text": "R\nCDC WONDER’s Underlying Cause of Death database is a dynamic website, as it requires user consent and the content and structure change based on the user selection. But we are only interested in ICD-10 113 Causes List in the HTML file, rather than getting the mortality data. So we can simply download the html file and then do the scraping. We can save the html as ‘Underlying Cause of Death, 2018-2022, Single Race Request Form.html’ if we accept the default file name; of course, you are free to name the file however you prefer.\nLet’s load the raw HTML file using the read_html() from rvest package.\n\nwonder &lt;- read_html(\"data/Underlying Cause of Death, 2018-2022, Single Race Request Form.html\")\n\nWe can then locate the ICD-10 113 Causes List section in the HTML using its xpath and the tag &lt;option&gt;. Each &lt;option&gt; corresponds to a cause in the list: the attribute value in the &lt;option&gt; is the cause ID (e.g., GR113-001), and the text of an &lt;option&gt; is the cause name (e.g., #Salmonella infections (A01-A02)). We extract the cause ID’s and cause names and keep them into two dataframes, cause_id and cause, respectively.\n\ncause_id &lt;- wonder %&gt;% \n  html_nodes(xpath='//*[@id=\"D158.V4\"]/div[1]') %&gt;% \n  html_elements('option') %&gt;% \n  html_attr(\"value\") %&gt;% \n  as_tibble() %&gt;% \n  rename(cause_id = value)\n\ncause &lt;- wonder %&gt;% \n  html_nodes(xpath='//*[@id=\"D158.V4\"]/div[1]') %&gt;% \n  html_elements('option') %&gt;% \n  html_text2() %&gt;% \n  as_tibble() %&gt;% \n  rename(cause = value)\n\nWe then bind the columns of cause_id and cause, filter to keep only rankable causes, extract ICD-10 codes from cause names, and clean both ICD-10 codes and cause names.\n\nrankable_causes &lt;-  bind_cols(cause_id, cause) %&gt;% \n  # keep only rankable causes\n  filter(str_detect(cause, \"#\")) %&gt;% \n  # extract ICD-10 codes from cause name\n  mutate(icd10 = sub(\".*\\\\((.*)\\\\).*\", \"\\\\1\", cause)) %&gt;% \n  # clean ICD-10 codes\n  mutate(icd10 = str_replace_all(icd10, \"\\\\*\", \"\")) %&gt;% \n  # clean cause names\n  mutate(cause = str_extract(cause, \"(?&lt;=#).+?(?=\\\\()\")) %&gt;% \n  mutate(icd10 = case_when(\n    icd10==\"V01-X59,Y85-Y86\" ~ \"V01-V99,W00-W99,X00-X59,Y85-Y86\",\n    icd10==\"U01-U02,X85-Y09,Y87.1\" ~ \"U01-U02,X85-X99,Y00-Y09,Y87.1\",\n    TRUE ~ icd10\n  )) %&gt;% \n  separate_rows(icd10, sep = \",\")\n\nrankable_causes %&gt;% \n  DT::datatable()\n\n\n\n\n\n\nTo streamline processing, we address two special cases in ICD-10 code groups:\n\nreplace V01-X59 with V01-V99,W00-W99,X00-X59\nreplace X85-Y09 with X85-X99,Y00-Y09\n\nThese two adjustments makes it easier to expand a code range (e.g., converting A07-A09 to A07,A08,A09), a step that will be needed when creating the crosswalk. We also split rows when a cause includes multiple ICD-10 code groups separated by a comma. For example, a cause like #Shigellosis and amebiasis (A03, A06) is divided into two rows: one for the code A03 and another for A06.\nNext, we need to handle cases with code ranges, such as A01-A02 and A16-A19. We can expand these ranges by listing each individual code from start to end, separated by commas, and then split the rows so that each code has its own row. Let’s write a function expand_code_range_r to expand a code range.\n\nexpand_code_range_r &lt;- function(text){\n  letter &lt;- str_extract_all(text, \"[A-Z]+\")[[1]][1]\n  numbers &lt;- str_extract_all(text, \"\\\\d+\")[[1]]\n  \n  res &lt;- paste0(letter, \n                str_pad(seq(numbers[1], numbers[2]), 2, \"left\", \"0\") , \n                collapse = \",\" )\n  \n  return(res)\n}\n\nNow we can create the crosswalk between ICD-10 codes and rankable causes. We get a dataframe crosswalk_icd10_vs_causes of 1037 rows.\n\ncrosswalk_icd10_vs_causes &lt;- bind_rows(\n  rankable_causes %&gt;% \n    filter(!str_detect(icd10, \"-\")),\n  \n  rankable_causes %&gt;% \n    filter(str_detect(icd10, \"-\")) %&gt;% \n    mutate(icd10 = icd10 %&gt;% \n             map_chr(expand_code_range_r)) %&gt;% \n    separate_rows(icd10, sep = \",\")\n) %&gt;% \n  arrange(cause_id, icd10)\n\ncrosswalk_icd10_vs_causes %&gt;% \n  DT::datatable()"
  },
  {
    "objectID": "posts/2024-10-26-understanding-icd-10-codes-and-the-113-causes-of-death-in-cdc-wonder/index.html#python",
    "href": "posts/2024-10-26-understanding-icd-10-codes-and-the-113-causes-of-death-in-cdc-wonder/index.html#python",
    "title": "Building a Crosswalk: Linking ICD-10 Codes with the Rankable Causes of Death in CDC WONDER",
    "section": "Python",
    "text": "Python\nWe can follow a similar process to create the same crosswalk in Python. First, let’s load some packages.\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom bs4 import BeautifulSoup\n\nThen, we read in the HTML file and parse its content using BeautifulSoup.\n\n# Load the HTML file\nwith open('data/Underlying Cause of Death, 2018-2022, Single Race Request Form.html', 'r', encoding='latin-1') as file:\n    content = file.read()\n\n# Parse the HTML content\nsoup = BeautifulSoup(content, 'html.parser')\n\nBeautifulSoup does not support XPath expressions. But we can find the cause list by getting all option tags and filtering tags with attribute values prefixed with GR113. Then we can get the cause ID’s and cause names for rankable causes.\n\n# Find the section where the ICD-10 113 Cause List is located\n# The causes are in a list of options in a select dropdown, which have an attribute of value prefixed with 'GR113'\nicd_section = soup.find_all('option', value=lambda x: x and 'GR113-' in x, string=re.compile(r'\\(.*\\)'))  \n# icd_section\n\n# soup.find_all('option', value=lambda x: x and 'GR113-' in x, text=re.compile(r'\\(.*\\)'))[0].get('value') \ncause_id_lst = [item.get('value')  for item in icd_section if item.text.strip().startswith('#')]\ncause_id_lst\n\n['GR113-001', 'GR113-002', 'GR113-004', 'GR113-007', 'GR113-008', 'GR113-009', 'GR113-010', 'GR113-011', 'GR113-012', 'GR113-013', 'GR113-014', 'GR113-015', 'GR113-016', 'GR113-017', 'GR113-019', 'GR113-044', 'GR113-045', 'GR113-046', 'GR113-047', 'GR113-050', 'GR113-051', 'GR113-052', 'GR113-054', 'GR113-069', 'GR113-070', 'GR113-071', 'GR113-073', 'GR113-076', 'GR113-080', 'GR113-082', 'GR113-087', 'GR113-088', 'GR113-090', 'GR113-091', 'GR113-092', 'GR113-093', 'GR113-096', 'GR113-097', 'GR113-102', 'GR113-103', 'GR113-104', 'GR113-105', 'GR113-108', 'GR113-109', 'GR113-112', 'GR113-124', 'GR113-127', 'GR113-130', 'GR113-134', 'GR113-135', 'GR113-136', 'GR113-137']\n\nrankable_causes_lst = [item.text.strip() for item in icd_section if item.text.strip().startswith('#')]\nrankable_causes_lst\n\n['#Salmonella infections (A01-A02)', '#Shigellosis and amebiasis (A03,A06)', '#Tuberculosis (A16-A19)', '#Whooping cough (A37)', '#Scarlet fever and erysipelas (A38,A46)', '#Meningococcal infection (A39)', '#Septicemia (A40-A41)', '#Syphilis (A50-A53)', '#Acute poliomyelitis (A80)', '#Arthropod-borne viral encephalitis (A83-A84,A85.2)', '#Measles (B05)', '#Viral hepatitis (B15-B19)', '#Human immunodeficiency virus (HIV) disease (B20-B24)', '#Malaria (B50-B54)', '#Malignant neoplasms (C00-C97)', '#In situ neoplasms, benign neoplasms and neoplasms of uncertain or unknown behavior (D00-D48)', '#Anemias (D50-D64)', '#Diabetes mellitus (E10-E14)', '#Nutritional deficiencies (E40-E64)', '#Meningitis (G00,G03)', '#Parkinson disease (G20-G21)', '#Alzheimer disease (G30)', '#Diseases of heart (I00-I09,I11,I13,I20-I51)', '#Essential hypertension and hypertensive renal disease (I10,I12,I15)', '#Cerebrovascular diseases (I60-I69)', '#Atherosclerosis (I70)', '#Aortic aneurysm and dissection (I71)', '#Influenza and pneumonia (J09-J18)', '#Acute bronchitis and bronchiolitis (J20-J21)', '#Chronic lower respiratory diseases (J40-J47)', '#Pneumoconioses and chemical effects (J60-J66,J68,U07.0)', '#Pneumonitis due to solids and liquids (J69)', '#Peptic ulcer (K25-K28)', '#Diseases of appendix (K35-K38)', '#Hernia (K40-K46)', '#Chronic liver disease and cirrhosis (K70,K73-K74)', '#Cholelithiasis and other disorders of gallbladder (K80-K82)', '#Nephritis, nephrotic syndrome and nephrosis (N00-N07,N17-N19,N25-N27)', '#Infections of kidney (N10-N12,N13.6,N15.1)', '#Hyperplasia of prostate (N40)', '#Inflammatory diseases of female pelvic organs (N70-N76)', '#Pregnancy, childbirth and the puerperium (O00-O99)', '#Certain conditions originating in the perinatal period (P00-P96)', '#Congenital malformations, deformations and chromosomal abnormalities (Q00-Q99)', '#Accidents (unintentional injuries) (V01-X59,Y85-Y86)', '#Intentional self-harm (suicide) (*U03,X60-X84,Y87.0)', '#Assault (homicide) (*U01-*U02,X85-Y09,Y87.1)', '#Legal intervention (Y35,Y89.0)', '#Operations of war and their sequelae (Y36,Y89.1)', '#Complications of medical and surgical care (Y40-Y84,Y88)', '#Enterocolitis due to Clostridium difficile (A04.7)', '#COVID-19 (U07.1)']\n\n\nWe create a dataframe rankable_causes_py that contains the rankable cause ID’s and names.\n\nrankable_causes_py = pd.DataFrame({'cause_id': cause_id_lst, 'cause': rankable_causes_lst})\nrankable_causes_py.head()\n\n    cause_id                                    cause\n0  GR113-001         #Salmonella infections (A01-A02)\n1  GR113-002     #Shigellosis and amebiasis (A03,A06)\n2  GR113-004                  #Tuberculosis (A16-A19)\n3  GR113-007                    #Whooping cough (A37)\n4  GR113-008  #Scarlet fever and erysipelas (A38,A46)\n\n\nIn dataframe rankable_causes_py, we extract ICD-10 codes from the ‘cause’ column, isolating them from other text or symbols. After extraction, we clean both the ICD-10 codes and the ‘cause’ column. The code also includes logic for handling two special cases, where certain ICD-10 codes are replaced with more specific combinations to improve accuracy for later processing. For rows that contain multiple ICD-10 code groups separated by ‘,’, we split these into individual code groups to avoid clustering data within single cells. Next, we “explode” the DataFrame, creating a new row for each unique ICD-10 code or code group, so that each row represents a single code or code group independently. The final output is a structured DataFrame, where each row contains a clean, individual ICD-10 code or code group and its corresponding cause of death.\n\ncdc_rankable_causes_lst = (rankable_causes_py\n # Extract ICD-10 codes from the cause string\n .assign(icd10 = lambda df_: df_['cause'].str.extract(r'\\(([^)]+)\\)$'))\n # Remove '*' from ICD-10 codes\n .assign(icd10 = lambda df_: df_['icd10'].str.replace(r'\\*', '', regex=True))\n# two special cases with different letters in one string\n .assign(icd10 = lambda df_: np.where(df_['icd10'].str.contains(\"X85-Y09\") , \"U01-U02,X85-X99,Y00-Y09,Y87.1\", df_['icd10']) )\n .assign(icd10 = lambda df_: np.where(df_['icd10'].str.contains(\"V01-X59,Y85-Y86\") , \"V01-V99,W00-W99,X00-X59,Y85-Y86\", df_['icd10']) )\n .assign(cause = lambda df_: df_['cause'].str.replace(r'\\(([^)]+)\\)$|#', '', regex=True))\n # Split ICD-10 codes into a list\n .assign(icd10= lambda df_: df_.icd10.str.split(','))\n # Expand the list of ICD-10 codes\n .explode('icd10') \n )\n\ncdc_rankable_causes_lst\n\n     cause_id                                        cause    icd10\n0   GR113-001                       Salmonella infections   A01-A02\n1   GR113-002                   Shigellosis and amebiasis       A03\n1   GR113-002                   Shigellosis and amebiasis       A06\n2   GR113-004                                Tuberculosis   A16-A19\n3   GR113-007                              Whooping cough       A37\n..        ...                                          ...      ...\n48  GR113-134        Operations of war and their sequelae     Y89.1\n49  GR113-135  Complications of medical and surgical care   Y40-Y84\n49  GR113-135  Complications of medical and surgical care       Y88\n50  GR113-136  Enterocolitis due to Clostridium difficile     A04.7\n51  GR113-137                                    COVID-19     U07.1\n\n[79 rows x 3 columns]\n\n\nSimilarly, let’s write a function expand_code_range to expand a code range in Python.\n\ndef expand_code_range(code_str):\n    \"\"\"\n    Expands a code range string into a comma-separated list of code strings.\n\n    Args:\n        code_str: A string representing a code range, e.g., \"a1-a9\".\n\n    Returns:\n        A comma-separated list of code strings representing the expanded range, e.g., \"a01,a02,a03,a04,a05,a06,a07,a08,a09\".\n    \"\"\"\n    # Extract the letter from the code string.\n    letter = re.findall(r'[a-zA-Z]+', code_str)[0]\n    # Extract the numbers from the code string and convert them to integers.\n    numbers_num = [int(num) for num in re.findall(r'\\d+', code_str)]\n    # Generate a list of code strings from the range of numbers.\n    res = ','.join([letter + str(x).zfill(2) for x in range(numbers_num[0], numbers_num[1] + 1)])\n    return res\n\nNow, we can create a crosswalk that is the same as the one we did in R.\n\ncrosswalk_icd10_vs_causes_py = (pd.concat([\n    # rows that do not need to expand\n    cdc_rankable_causes_lst\n    .loc[lambda df_: ~df_['icd10'].str.contains('-')],\n\n    #  rows taht need to expand\n    cdc_rankable_causes_lst\n    .loc[lambda df_: df_['icd10'].str.contains('-')]\n    .assign(icd10 = lambda df_: np.vectorize(expand_code_range)(df_['icd10']) )\n    .assign(icd10= lambda df_: df_.icd10.str.split(','))\n    .explode('icd10')\n    ])\n    .sort_values(['cause_id', 'icd10'])\n    .reset_index(drop=True)\n)\n\ncrosswalk_icd10_vs_causes_py\n\n       cause_id                                        cause  icd10\n0     GR113-001                       Salmonella infections     A01\n1     GR113-001                       Salmonella infections     A02\n2     GR113-002                   Shigellosis and amebiasis     A03\n3     GR113-002                   Shigellosis and amebiasis     A06\n4     GR113-004                                Tuberculosis     A16\n...         ...                                          ...    ...\n1032  GR113-135  Complications of medical and surgical care     Y83\n1033  GR113-135  Complications of medical and surgical care     Y84\n1034  GR113-135  Complications of medical and surgical care     Y88\n1035  GR113-136  Enterocolitis due to Clostridium difficile   A04.7\n1036  GR113-137                                    COVID-19   U07.1\n\n[1037 rows x 3 columns]"
  },
  {
    "objectID": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html",
    "href": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html",
    "title": "Pandas Equivalent to R Tidyverse ‘mutate(across())’",
    "section": "",
    "text": "Data manipulation is central to data analytics, and R’s tidyverse and Python’s pandas are two of the most popular tools for this purpose. While both ecosystems are powerful, they differ in syntax and philosophy. In R, the tidyverse emphasizes simplicity and readability, and mutate(across()) is a prime example. This function allows users to efficiently apply transformations to multiple columns, whether it’s standardizing values, performing custom calculations, or applying multiple functions in one step.\nPython’s pandas library, although it lacks a direct equivalent to mutate(across()), provides a versatile set of tools like assign and method chaining that can achieve similar results. However, navigating these tools to replicate tidyverse functionality requires familiarity with pandas’ idiomatic approaches.\nThis post aims to guide users through translating mutate(across()) from R into pandas, offering practical examples and tips. Whether you’re transitioning between the two languages or working in a multilingual data environment, this post will help you leverage the strengths of both tools effectively."
  },
  {
    "objectID": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#applying-a-single-transformation",
    "href": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#applying-a-single-transformation",
    "title": "Pandas Equivalent to R Tidyverse ‘mutate(across())’",
    "section": "1. Applying a Single Transformation",
    "text": "1. Applying a Single Transformation\nOne of the simplest use cases for mutate(across()) is applying a single function to a subset of columns. For instance, suppose you have a dataset containing numeric columns, and you want to scale all of them by dividing by 100.\n\nlibrary(dplyr)\n\n# Example dataset\ndf &lt;- tibble(\n  id = 1:5,\n  sales = c(100, 200, 300, 400, 500),\n  profit = c(10, 20, 30, 40, 50)\n)\n\ndf\n\n# A tibble: 5 × 3\n     id sales profit\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1   100     10\n2     2   200     20\n3     3   300     30\n4     4   400     40\n5     5   500     50\n\n# Applying a single transformation\ndf %&gt;%\n  mutate(across(c(sales, profit), ~ . / 100))\n\n# A tibble: 5 × 3\n     id sales profit\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1     1    0.1\n2     2     2    0.2\n3     3     3    0.3\n4     4     4    0.4\n5     5     5    0.5\n\n\nHere, across() specifies the columns to transform (sales and profit), and the lambda-style syntax ~ .x / 100 defines the operation."
  },
  {
    "objectID": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#applying-multiple-functions",
    "href": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#applying-multiple-functions",
    "title": "Pandas Equivalent to R Tidyverse ‘mutate(across())’",
    "section": "2. Applying Multiple Functions",
    "text": "2. Applying Multiple Functions\nmutate(across()) can also apply multiple functions to the same set of columns, generating new columns for each transformation. This is useful for tasks like summarizing data with multiple statistics.\n\ndf %&gt;%\n  mutate(across(c(sales, profit), list(mean = mean, sd = sd), .names = \"{col}_{fn}\"))\n\n# A tibble: 5 × 7\n     id sales profit sales_mean sales_sd profit_mean profit_sd\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1     1   100     10        300     158.          30      15.8\n2     2   200     20        300     158.          30      15.8\n3     3   300     30        300     158.          30      15.8\n4     4   400     40        300     158.          30      15.8\n5     5   500     50        300     158.          30      15.8\n\n\nThe .names argument controls how the new column names are generated, incorporating placeholders like {col} (original column name) and {fn} (function name)."
  },
  {
    "objectID": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#combining-conditional-logic",
    "href": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#combining-conditional-logic",
    "title": "Pandas Equivalent to R Tidyverse ‘mutate(across())’",
    "section": "3. Combining Conditional Logic",
    "text": "3. Combining Conditional Logic\nAnother strength of mutate(across()) is its ability to integrate conditional logic. For instance, you might want to apply different transformations based on column values.\n\ndf %&gt;%\n  mutate(across(c(sales, profit), ~ if_else(. &gt; 300, . * 10, .)))\n\n# A tibble: 5 × 3\n     id sales profit\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1   100     10\n2     2   200     20\n3     3   300     30\n4     4  4000     40\n5     5  5000     50\n\n\nIn this example, values greater than 300 are multiplied by 10, while others remain unchanged."
  },
  {
    "objectID": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#using-assign-for-a-single-transformation",
    "href": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#using-assign-for-a-single-transformation",
    "title": "Pandas Equivalent to R Tidyverse ‘mutate(across())’",
    "section": "1. Using assign() for a Single Transformation",
    "text": "1. Using assign() for a Single Transformation\nThe assign() method can be combined with pandas’ vectorized operations to modify multiple columns.\n\nimport pandas as pd\nimport numpy as np\n\n# Example dataset\ndf = pd.DataFrame({\n    'id': [1, 2, 3, 4, 5],\n    'sales': [100, 200, 300, 400, 500],\n    'profit': [10, 20, 30, 40, 50]\n})\n\ndf\n\n   id  sales  profit\n0   1    100      10\n1   2    200      20\n2   3    300      30\n3   4    400      40\n4   5    500      50\n\n# Applying a single transformation with assign\ndf.assign(\n    sales=lambda x: x['sales'] / 100,\n    profit=lambda x: x['profit'] / 100\n)\n\n   id  sales  profit\n0   1    1.0     0.1\n1   2    2.0     0.2\n2   3    3.0     0.3\n3   4    4.0     0.4\n4   5    5.0     0.5\n\n\nIn this example, we use assign() to create new columns sales and profit by dividing the original columns by 100. The lambda function syntax allows us to reference the columns directly. Pandas assign() is similar to tidyverse mutate() in that it returns a new DataFrame with the modified columns.\nHow can we do across() in pandas? We can unpack a dictionary comprehension into the keyword arguments of assign() to apply the same transformation to multiple columns.\n\ndf.assign(\n  **{col: df[col] / 100 for col in ['sales', 'profit']}\n  )\n\n   id  sales  profit\n0   1    1.0     0.1\n1   2    2.0     0.2\n2   3    3.0     0.3\n3   4    4.0     0.4\n4   5    5.0     0.5\n\n\nThe dictionary comprehension creates a dictionary where the keys are the column names (i.e., ‘sales’, ‘profit’) and the values are the desired calculations. The double-asterisk operator (**) unpacks the dictionary, passing its key-value pairs as keyword arguments to the assign function.\nTo apply a single transformation across selected columns, we can also use the apply() method combined with lambda functions or NumPy operations. However, apply() can be inefficient in certain scenarios. Unlike many Pandas operations, apply() doesn’t leverage vectorization, which performs operations on entire arrays at once. Instead, it iterates over rows or columns individually, leading to slower performance especially for large datasets."
  },
  {
    "objectID": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#applying-multiple-functions-1",
    "href": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#applying-multiple-functions-1",
    "title": "Pandas Equivalent to R Tidyverse ‘mutate(across())’",
    "section": "2. Applying Multiple Functions",
    "text": "2. Applying Multiple Functions\nTo apply multiple functions to the same set of columns, we can use a dictionary comprehension to generate new columns with the desired transformations.\n\ndf.assign(\n  **{f\"{col}_{fn}\": df[col].agg(fn) for col in ['sales', 'profit'] for fn in ['mean', 'std']}\n)\n\n   id  sales  profit  sales_mean   sales_std  profit_mean  profit_std\n0   1    100      10       300.0  158.113883         30.0   15.811388\n1   2    200      20       300.0  158.113883         30.0   15.811388\n2   3    300      30       300.0  158.113883         30.0   15.811388\n3   4    400      40       300.0  158.113883         30.0   15.811388\n4   5    500      50       300.0  158.113883         30.0   15.811388"
  },
  {
    "objectID": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#conditional-logic-with-assign",
    "href": "posts/2025-01-03-pandas-equivalent-to-r-tidyverse-mutate-across/index.html#conditional-logic-with-assign",
    "title": "Pandas Equivalent to R Tidyverse ‘mutate(across())’",
    "section": "3. Conditional Logic with assign",
    "text": "3. Conditional Logic with assign\nWe can integrate conditional logic directly into assign() using vectorized where() method from numpy .\n\ndf.assign(\n  **{col: np.where(df[col] &gt; 300, df[col] * 10, df[col]) for col in ['sales', 'profit']}\n)\n\n   id  sales  profit\n0   1    100      10\n1   2    200      20\n2   3    300      30\n3   4   4000      40\n4   5   5000      50"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Pandas Equivalent to R Tidyverse ‘mutate(across())’\n\n\n\n\n\n\n\nPython\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\nBuilding a Crosswalk: Linking ICD-10 Codes with the Rankable Causes of Death in CDC WONDER\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\nUsing the SUBSTR Function to Create New Variables in SAS: A Lengthy Matter\n\n\n\n\n\n\n\nSAS\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2024\n\n\n\n\n\n\n\n\nCreate a crosswalk between 2022 Census tracts and 2020 counties in Connecticut by spatial join\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\n\n\n\n\n\n\nRun R in SAS\n\n\n\n\n\n\n\nSAS\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\nReading Fixed-Width Files in SAS, R, and Python\n\n\n\n\n\n\n\nR\n\n\nSAS\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\n\n\n\n\n\n\nParallel Computing in R\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\n\n\nSubgroup Population by Age Category from Census Population Estimates\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n\n\n\n\n\n\nCalculate county-level rural population rate from Census blocks data using Python\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\nPull Data Using EPA’s Data Service API\n\n\n\n\n\n\n\nAPI\n\n\nR\n\n\nSAS\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\n\n\n\n\n\n\nIntroduction to R\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2022\n\n\n\n\n\n\n\n\nPROC SQL\n\n\n\n\n\n\n\nSAS\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2022\n\n\n\n\n\n\n\n\nSAS Macro (2)\n\n\n\n\n\n\n\nSAS\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2022\n\n\n\n\n\n\n\n\nSAS Macor (1)\n\n\n\n\n\n\n\nSAS\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2022\n\n\n\n\n\n\n\n\nWays to Run SAS\n\n\n\n\n\n\n\nSAS\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Pubs",
    "section": "",
    "text": "Google Scholar Profile."
  },
  {
    "objectID": "publications.html#book-book-chapter",
    "href": "publications.html#book-book-chapter",
    "title": "Pubs",
    "section": "Book / Book Chapter",
    "text": "Book / Book Chapter\n\n\nX. Y. Sui, G. H. Zhou, H. H. Pu, G. H. Lu, S. Mao, X. Y. Chen, and J. H. Chen (2022), Field-Effect Transistor Sensors Based on 2D Nanomaterials for Detecting Contaminants in Water, The World Scientific Reference of Water Science, Volume 1: Molecular Engineering of Water Sensors DOI\n\n\nJ. H. Chen, Z. Bo, and G. H. Lu (2015), Vertically-oriented Graphene: Synthesis and Applications, ISBN 978-3-319-15301-8, Springer. (Available at: Springer; Amazon)\n\n\nJ. H. Chen and G. H. Lu (2010), Carbon Nanotube-Nanoparticle Hybrid Structures, Carbon Nanotubes, Jose Mauricio Marulanda (Ed.), ISBN: 978-953-307-054-4, INTECH. (Chapter 31, p. 611-634) Access"
  },
  {
    "objectID": "publications.html#patents",
    "href": "publications.html#patents",
    "title": "Pubs",
    "section": "Patents",
    "text": "Patents\n\nJ. H. Chen and G. H. Lu, Controlled Decoration of Carbon Nanotubes with Aerosol Nanoparticles, U.S. Patent 8268405. (Filed on Aug 23, 2006; Publication date Sep 18, 2012)\nJ. H. Chen and G. H. Lu, Ambient-Temperature Gas Sensor, U.S. Patent 8240190. (Filed on August 28, 2008; cleared and issued on August 14, 2012)\nJ. H. Chen, K. H. Yu, Z. Bo, and G. H. Lu, High Electric Field Fabrication of Oriented Nanostructures, U.S. 9187823 (Filed on September 7, 2012; Publication date November 17, 2015)\nJ. H. Chen, S. Mao, G. H. Lu, K. H. Yu, and Z. Bo, Protein Detection using Thermally Reduced Graphene Oxide Sheets, U.S. Patent #9,676,621 B2 (Filed on February 17, 2012; Publication date June 13, 2017)\n\n(Back to top)"
  },
  {
    "objectID": "publications.html#peer-reviewed-journal-papers",
    "href": "publications.html#peer-reviewed-journal-papers",
    "title": "Pubs",
    "section": "Peer-Reviewed Journal Papers",
    "text": "Peer-Reviewed Journal Papers\n(Last updated: 03/29/2025; Total citations: 6,779 times according to ISI Web of Knowledge, 10637 times according to Google Scholar)\n\n\nA. Maity, H. H. Pu, X. Y. Sui, J. B. Chang, K. J. Bottum, B. Jin, G. H. Zhou, Y. L. Wang, G. H. Lu, and J. H. Chen, “Scalable Graphene Sensor Array for Real-time Toxins Monitoring in Flowing Water,” Nature Communications, 14, 4184, 2023.(DOI:10.1038/s41467-02339701-0)\n\n\nY. L. Wang, H. H. Pu, G. H. Lu, X. Y. Sui, and J. H. Chen, “Quantitative analysis of the synergistic effect of Au nanoparticles on SnO2-rGO nanocomposites for room temperature hydrogen sensing,” Physical Chemistry Chemical Physics, 23(3), 2377-2383, 2021. (DOI: 10.1039/D0CP05701K)\n\n\nX. Y. Sui, H. H. Pu, A. Maity, J. B. Chang, B. Jin, G. H. Lu, Y. L. Wang, R. Ren, S. Mao, and J. H. Chen, “Field-Effect Transistor Based on Percolation Network of Reduced Graphene Oxide for Real-Time ppb-Level Detection of Lead Ions in Water,” ECS Journal of Solid State Science and Technology, 9(11), 115012, 2020. (DOI: 10.1149/2162-8777/abaaf4).\n\n\nA. Maity, X. Y. Sui, H. H. Pu, K. J. Bottum, B. Jin, J. B. Chang, G. H. Zhou, G. H. Lu, and J. H. Chen, ” Sensitive field-effect transistor sensors with atomically thin black phosphorus nanosheets,” Nanoscale, 12, 1500-1512, 2020. (DOI: 10.1039/C9NR09354K)\n\n\nA. Maity, X. Y. Sui, B. Jin, H. H. Pu, K. J. Bottum, X. K. Huang, J. B. Chang, G. H. Zhou, G. H. Lu, and J. H. Chen, “Resonance-Frequency Modulation for Rapid, Point-of-Care Ebola-Glycoprotein Diagnosis with a Graphene-Based Field-Effect Biotransistor,” Analytical Chemistry, 90(24), 14230–14238, 2018. (DOI: 10.1021/acs.analchem.8b03226)\n\n\nY. Liu, Y. Wang, M. Ikram, H. Lv, J. B. Chang, Z. K. Li, L. F. Ma, A. U. Rehman, G. H. Lu, J. H. Chen, and K. Y. Shi, “Facile Synthesis of Highly Dispersed Co3O4 Nanoparticles on Expanded, Thin Black Phosphorus for a ppb-Level NOx Gas Sensor,” ACS Sensors, Just Accepted Manuscript, 2018. (DOI: 10.1021/acssensors.8b00397)\n\n\nJ. B. Chang, A. Maity, H. H. Pu, X. Y Sui, G. H. Zhou, R. Ren, G. H. Lu, and J. H. Chen, “Impedimetric phosphorene field-effect transistors for rapid detection of lead ions,” Nanotechnology, 29(37), 375501, 2018. (DOI: 10.1088/1361-6528/aacb6a)\n\n\nS. Mao, J. B. Chang, H. H. Pu, G. H. Lu, Q. Y. He, H. Zhang, and J. H. Chen, ” Two-dimensional Nanomaterial-based Field-effect Transistors for Chemical and Biological Sensing,” Chemical Society Reviews, 46, 6872–6904, 2017. (DOI:10.1039/C6CS00827E)\n\n\nX. R. Gao, G. H. Lu, and J. H. Chen, “Graphene-based Materials for Photoanodes in Dye-sensitized Solar Cells,” Frontiers in Energy Research, 3, 50, 2015. (DOI: 10.3389/fenrg.2015.00050)\n\n\nZ. Bo, H. H. Yang, P. Lv, J. J. Qian, K. H. Yu, G. H. Lu, W. Wei, J.H. Yan, and K.F. Cen, “Covalently Interconnected Carbon Nanotubes for Enhanced Charge Transport in Pseudocapacitors,” Physica Status Solidi B: Basic Solid State Physics, 252(10), 2236–2244, 2015. (DOI: 10.1002/pssb.201451742)\n\n\nS. Mao†, G. H. Lu†, and J. H. Chen, “Three-Dimensional Graphene-Based Composites for Energy Applications,” Nanoscale, 7, 6924-6943, 2015. (†contributed equally) (Invited Review Article) (DOI: 10.1039/C4NR06609J)\n\n\nS. Mao, G. H. Lu, and J. H. Chen, “Nanocarbon-based Gas Sensors: Progress and Challenges,” Journal of Materials Chemistry A, 2, 5573-5579, 2014. (Invited Highlight) (DOI: 10.1039/C3TA13823B)\n\n\nZ. H. Wen, G. H. Lu, S. M. Cui, H. Kim, S. Q. Ci, J. W. Jiang, P. T. Hurley, and J. H. Chen, “Rational Design of Carbon Network Cross-Linked Si–SiC Hollow Nanosphere as Anode of Lithium-Ion Batteries,” Nanoscale, 6, 342-351, 2014. (DOI: 10.1039/C3NR04162J)\n\n\nS. M. Cui†, S. Mao†, G. H. Lu†, and J. H. Chen, “Graphene Coupled with Nanocrystals: Opportunities and Challenges for Energy and Sensing Applications,” Journal of Physical Chemistry Letters, 4, 2441–2454, 2013. (†contributed equally) (DOI:10.1021/jz400976a) ACS LiveSlides highlight | Video highlight | YouTube\n\n\nT. Z. Huang, S. Mao, J. M. Yu, Z. H. Wen, G. H. Lu, and J. H. Chen, “Effects of N and F Doping on Structure and Photocatalytic Properties of Anatase TiO2 Nanoparticles,” RSC Advances, Accepted, 2013. (DOI: 10.1039/C3RA42600A)\n\n\nG. H. Lu and J. H. Chen, “Enhancing Undergraduate Student Learning and Research Experience through Hands-on Experiments on Novel Nanohybrid Devices and Systems,” Journal of Nano Education, 5(1), 61-69, 2013.\n\n\nE. C. Mattson, K. Pande, M. Unger, S. M. Cui, G. H. Lu, M. Gajdardziska-Josifovska, M. Weinert, J. H. Chen, and C. J. Hirschmugl, “Exploring Adsorption and Reactivity of NH3 on Reduced Graphene Oxide,” Journal of Physical Chemistry C, 117(20), 10698–10707, 2013. (DOI:10.1021/jp3122853)\n\n\nZ. Bo, G. H. Lu, P. X. Wang, and J. H. Chen, “Dimensional Analysis of Detrimental Ozone Generation by Negative Wire-to-Plate Corona Discharge in Both Dry and Humid Air”, Ozone-Science & Engineering, 35(1), 31-37, 2013. (DOI: 10.1080/01919512.2013.721286)\n\n\nG. H. Lu, K. H. Yu, Z. H. Wen, and J. H. Chen, “Semiconducting Graphene: Converting Graphene from Semimetal to Semiconductor,” Nanoscale, 5, 1353-1368, 2013. (DOI: 10.1039/C2NR32453A) (Invited review)\n\n\nZ. H. Wen, G. H. Lu, S. Mao, H. Kim, S. M. Cui, K. H. Yu, X. K. Huang, P. T. Hurley, O. Mao, and J. H. Chen, “Silicon Nanotube Anode for Lithium-ion Batteries,” Electrochemistry Communications, 29, 67-70, 2013. (DOI: 10.1016/j.elecom.2013.01.015)\n\n)\n\nZ. H. Wen, S. Ci, S Mao, S. M. Cui, G. H. Lu, K. H. Yu, S. Luo, Z. He, and J. H. Chen, “TiO2 Nanoparticles-decorated Carbon Nanotubes for Significantly Improved Bioelectricity Generation in Microbial Fuel Cells,” Journal of Power Sources, 234, 100-106, 2013. (DOI: 10.1016/j.jpowsour.2013.01.146)\n\n\nX. Lin, K. H. Yu, G. H. Lu, J. H. Chen, and C. Yuan, “Atomic Layer Deposition of TiO2 Interfacial Layer for Enhancing Performance of Quantum Dot and Dye Co-Sensitized Solar Cells,” Journal of Physics D: Applied Physics, 46, 024004, 2013. (DOI:10.1088/0022-3727/46/2/024004)\n\n\nK. H. Yu, Z. H. Wen, H. H. Pu, G. H. Lu, Z. Bo, H. Kim, Y. Y. Qian, E. Andrew, S. Mao, and J. H. Chen, “Hierarchical vertically-oriented graphene as a catalytic counter electrode in dye-sensitized solar cells,” Journal of Materials Chemistry A, 1, 188–193, 2013. (DOI: 10.1039/C2TA00380E)\n\n\nS. M. Cui, H. H. Pu, G. H. Lu, Z. H. Wen, E. Mattson, C. Hirschmugl, M. Gajdardziska-Josifovska, M. Weinert, J. H. Chen, “Fast and Selective Room-temperature Ammonia Sensors Using Silver Nanocrystals-Functionalized Carbon Nanotubes,” ACS Applied Materials & Interfaces, 4(9), 4898–4904 2012. (DOI:10.1021/am301229w)\n\n\nZ. H. Wen, X. C. Wang, S. Mao, Z. Bo, H. Kim, S. M. Cui, G. H. Lu, X. L. Feng, and J. H. Chen, “Crumpled Nitrogen-Doped Graphene Nanosheets with Ultrahigh Pore Volume for High-performance Supercapacitor,” Advanced Materials, 24(41), 5610–5616, 2012. (DOI: 10.1002/adma.201201920)\n\n\nM. Mashock, K. H. Yu, S. M. Cui, S. Mao, G. H. Lu, and J. H. Chen, “Modulating Gas Sensing Properties of CuO Nanowires through Creation of Discrete Nanosized P-N Junctions on Their Surfaces,” ACS Applied Materials & Interfaces, 4(8), 4192–4199, 2012. (* corresponding authors) (DOI: 10.1021/am300911z)\n\n\nG. H. Lu, K. H. Yu, and J. H. Chen, “Ordered Assembly of Sorted Single-walled Carbon Nanotubes by Drying an Aqueous Droplet on a Meshed Substrate,” Journal of Nanoscience and Nanotechnology, 12(9), 6968-6976, 2012. (DOI:10.1166/jnn.2012.6505)\n\n\nS. Mao, Z. H. Wen, H. Kim, G. H. Lu, P. Hurley, and J. H. Chen, “A General Approach to One-Pot Fabrication of Crumpled Graphene-Based Nanohybrids for Energy Applications,” ACS Nano, 6(8), 7505–7513, 2012. (DOI:10.1021/nn302818j)\n\n\nS. M. Cui, H. H. Pu, E. C. Mattson, G. H. Lu, S. Mao, M. Weinert, C. Hirschmugl, M. Gajdardziska-Josifovska, and J. H. Chen “Ag Nanocrystal as a Promoter for Carbon Nanotube-based Room-temperature Gas Sensors,” Nanoscale, 2012, 4, 5887-5894. (DOI: 10.1039/C2NR31556D)\n\n\nS. Mao, S. M. Cui, K. H. Yu, Z. H. Wen, G. H. Lu, and J. H. Chen, “Ultrafast Hydrogen Sensing through Hybrids of Semiconducting Single-Walled Carbon Nanotubes and Tin Oxide Nanocrystals,” Nanoscale,4, 1275-1279, 2012. (DOI: 10.1039/C2NR11765G)\n\n\nK. H. Yu, X. Lin, G. H. Lu, Z. H. Wen, C. Yuan, and J. H. Chen, “Optimizing CdS Quantum Dot-Sensitized Solar Cell Performance through Atomic Layer Deposition of Ultrathin TiO2 Coating,” RSC Advances, 2, 7843–7848, 2012. (DOI: 10.1039/C2RA20979A)\n\n\nS. Mao, S. M. Cui, G. H. Lu, K. H. Yu, Z. H. Wen, and J. H. Chen, “Tuning gas-sensing properties of reduced graphene oxide using tin oxide nanocrystals,”Journal of Materials Chemistry, 22, 11009–11013, 2012. (DOI: 10.1039/C2JM30378G)\n\n\nZ. Bo, Z. H. Wen, H. Kim, G. H Lu, K. H. Yu, and J. H. Chen, “One-Step Fabrication and Capacitive Behavior of Electrochemical Double Layer Capacitor Electrodes Using Vertically-Oriented Graphene Directly Grown on Metal,” Carbon, 50(12), 4379–4387, 2012. (DOI: 10.1016/j.carbon.2012.05.014)\n\n\nK. H. Chen†, G. H. Lu†, J. B. Chang†, S. Mao, K. H. Yu, S. M. Cui, and J. H. Chen, “Hg(II) Ion Detection Using Thermally Reduced Graphene Oxide Decorated with Functionalized Gold Nanoparticles,” Analytical Chemistry, 84(9), 4057-4062, 2012. (DOI: 10.1021/ac3000336) (†contributed equally)\n\n\nS. M. Cui, E. C. Mattson, G. H. Lu, C. Hirschmugl, M. Gajdardziska-Josifovska, and J. H. Chen, “Tailoring nanomaterial products through electrode material and oxygen partial pressure in a mini-arc plasma reactor,” Journal of Nanoparticle Research, 14(3), 1-13, 2012. (DOI: 10.1007/s11051-012-0744-5)\n\n\nZ. H. Wen, S. M. Cui, H. Kim, S. Mao, K. H. Yu, G. H. Lu, H. H. Pu, O. Mao, and J. H. Chen, “Binding Sn-based nanoparticles on graphene as the anode of rechargeable lithium-ion batteries,” Journal of Materials Chemistry, 22, 3300-3306, 2012. (DOI: 10.1039/C2JM14999K)\n\n\nK. H. Yu, G. H. Lu, K. H. Chen, S. Mao, H. Kim, and J. H. Chen, “Controllable Photoelectron Transfer in CdSe Nanocrystal–Carbon Nanotube Hybrid Structures,” Nanoscale, 4, 742-746, 2012. (DOI: 10.1039/C2NR11577H)\n\n\nG. H. Lu, K. H. Yu, L. E. Ocola, and J. H. Chen, “Ultrafast Room-Temperature NH3 Sensing with Positively-Gated Reduced Graphene Oxide Field-Effect Transistors,” Chemical Communications, 47(27), 7761-7763 , 2011. (DOI: 10.1039/C1CC12658J)\n\n\nE. C. Mattson, H. H. Pu, S. M. Cui, M. A. Schofield, S. Rhim, G. H. Lu, M. J. Nasse, R. S. Ruoff, M. Weinert, M. Gajdardziska-Josifovska, J. H. Chen, and C. J. Hirschmugl, “Evidence of Nanocrystalline Semiconducting Graphene Monoxide during Thermal Reduction of Graphene Oxide in Vacuum,” ACS Nano, 5(12), 9710–9717, 2011. (DOI: 10.1021/nn203160n)\n\n\nZ. Bo, S. M. Cui, K. H. Yu, G. H. Lu, S. Mao, and J. H. Chen, “Note: Continuous Synthesis of Uniform Vertical Graphene on Cylindrical Surfaces,” Review of Scientific Instruments, 82(8), 086116, 2011. (DOI: 10.1063/1.3624842 )\n\n\nK. H. Yu, G. H. Lu, Z. Bo, and J. H. Chen, “Selective Deposition of CdSe Nanoparticles on Reduced Graphene Oxide to Understand Photoinduced Charge Transfer in Hybrid Nanostructures,” ACS Applied Materials & Interfaces, 3(7), 2703–2709, 2011. (DOI: 10.1021/am200494v)\n\n\nS. Mao, K. H. Yu, S. M. Cui, Z. Bo, G. H. Lu, and Junhong Chen, “A new reducing agent to prepare single-layer, high-quality reduced graphene oxide for device applications,” Nanoscale, 3, 2849-2853, 2011. (DOI: 10.1039/c1nr10270b)\n\n\nK. H. Yu†, G. H. Lu†, Z. Bo, and J. H. Chen, “Carbon Nanotube with Chemically Bonded Graphene Leaves for Electronic and Optoelectronic Applications,” Journal of Physical Chemistry Letters, 2, 1556–1562, 2011. († contributed equally) (DOI: 10.1021/jz200641c)\n\n\nG. H. Lu, S. Park, K. H. Yu, R. S. Ruoff, L. E. Ocola, D. Rosenmann, and J. H. Chen, “Toward Practical Gas Sensing with Highly Reduced Graphene Oxide: A New Signal Processing Method to Circumvent Run-to-Run and Device-to-Device Variations,” ACS Nano, 5(2), 1154–1164, 2011. (DOI:10.1021/nn102803q)\n\n\nZ. Bo, K. H. Yu, G. H. Lu, S. M. Cui, S. Mao, and J. H. Chen, “Vertically oriented graphene sheets grown on metallic wires for greener corona discharges: lower power consumption and minimized ozone emission,” Energy & Environmental Science, 4, 2525–2528, 2011. (DOI:10.1039/C1EE01140E)\n\n\nK. H. Yu, P. X. Wang, G. H. Lu, K. H. Chen, Z. Bo, and J. H. Chen, “Patterning Vertically-oriented Graphene Sheets for Nanodevice Applications,” Journal of Physical Chemistry Letters, 2(6), 537–542, 2011. (DOI: 10.1021/jz200087w)\n\n\nZ. Bo, K. H. Yu, G. H. Lu, S. Mao, and J. H. Chen, “Understanding Growth of Carbon Nanowalls at Atmospheric Pressure Using Normal Glow Discharge Plasma-enhanced Chemical Vapor Deposition,” Carbon, 49(6), 1849-1858, 2011.\n\n\nK. H. Yu, G. H. Lu, Z. Bo, and J. H. Chen, “Selective Deposition of CdSe Nanoparticles on Reduced Graphene Oxide to Understand Photoinduced Charge Transfer in Hybrid Nanostructures,” ACS Applied Materials & Interfaces, 3(7), 2703–2709, 2011. (DOI: 10.1021/am200494v)\n\n\nS. Mao, K. H. Yu, S. M. Cui, Z. Bo, G. H. Lu, and Junhong Chen, “A new reducing agent to prepare single-layer, high-quality reduced graphene oxide for device applications,” Nanoscale, 3, 2849-2853, 2011. (DOI: 10.1039/c1nr10270b)\n\n\nK. H. Yu†, G. H. Lu†, Z. Bo, and J. H. Chen, “Carbon Nanotube with Chemically Bonded Graphene Leaves for Electronic and Optoelectronic Applications,” Journal of Physical Chemistry Letters, 2, 1556–1562, 2011. († contributed equally) (DOI: 10.1021/jz200641c)\n\n\nG. H. Lu, S. Park, K. H. Yu, R. S. Ruoff, L. E. Ocola, D. Rosenmann, and J. H. Chen, “Toward Practical Gas Sensing with Highly Reduced Graphene Oxide: A New Signal Processing Method to Circumvent Run-to-Run and Device-to-Device Variations,” ACS Nano, 5(2), 1154–1164, 2011. (DOI:10.1021/nn102803q)\n\n\nZ. Bo, K. H. Yu, G. H. Lu, S. M. Cui, S. Mao, and J. H. Chen, “Vertically oriented graphene sheets grown on metallic wires for greener corona discharges: lower power consumption and minimized ozone emission,” Energy & Environmental Science, 4, 2525–2528, 2011. (DOI:10.1039/C1EE01140E)\n\n\nK. H. Yu, P. X. Wang, G. H. Lu, K. H. Chen, Z. Bo, and J. H. Chen, “Patterning Vertically-oriented Graphene Sheets for Nanodevice Applications,” Journal of Physical Chemistry Letters, 2(6), 537–542, 2011. (DOI: 10.1021/jz200087w)\n\n\nZ. Bo, K. H. Yu, G. H. Lu, S. Mao, and J. H. Chen, “Understanding Growth of Carbon Nanowalls at Atmospheric Pressure Using Normal Glow Discharge Plasma-enhanced Chemical Vapor Deposition,” Carbon, 49(6), 1849-1858, 2011. (DOI: 10.1016/ j.carbon.2011.01.007)\n\n)\n\nS. Mao, K. H. Yu, G. H. Lu, and J. H. Chen, “Highly Sensitive Protein Sensor Based on Thermally-Reduced Graphene Oxide Field-Effect Transistor,” Nano Research, 4(10), 921-930, 2011. (DOI: 10.1007/s12274-011-0148-3) (PDF)\n\nK. H. Yu, Z. Bo, G. H. Lu, S. Mao, S. M. Cui, Y. W. Zhu, X. Q. Chen, R. S. Ruoff, and J. H. Chen, “Growth of Carbon Nanowalls at Atmospheric Pressure for One-step Gas Sensor Fabrication,” Nanoscale Research Letters, 6, 202, 2011. (DOI: 10.1186/1556-276X-6-202)\n\n\nB. J. Hansen, H. Chan, J. Lu, G. H. Lu, and J. H. Chen, “Short-circuit Diffusion Growth of Long Bi-crystal CuO Nanowires,”Chemical Physics Letters, 504(1-3), 41-45, 2011. (DOI: 10.1016/j.cplett.2011.01.040)\n\n\nS. Mao, G. H. Lu, K. H. Yu, Z. Bo, and J. H. Chen, “Specific Protein Detection using Thermally Reduced Graphene Oxide Sheet Decorated with Gold Nanoparticle-antibody Conjugates,” Advanced Materials, 22(32), 3521–3526, 2010. (DOI: 10.1002/adma.201000520)\n\n\nZ. Bo, K. H. Yu, G. H. Lu, S. Mao, J. H. Chen, and F. G. Fan, “Nanoscale Discharge Electrode for Minimizing Ozone Emission from Indoor Corona Devices,” Environmental Science & Technology, 44(16), 6337-6342, 2010. (DOI: 10.1021/es 903917f)\n\n\nB. Hansen, N. Kouklin, G. H. Lu, I. K. Lin, J. H. Chen, and X. Zhang, “Transport, Analyte Detection and Opto-Electronic Response of p-Type CuO Nanowires,” Journal of Physical Chemistry C, 114(6), 2440-2447, 2010. (DOI: 10.1021/jp908850j)\n\n\nS. M. Cui, G. H. Lu, S. Mao, K. H. Yu, and J. H. Chen, “One-dimensional Tungsten Oxide Growth through a Grain-by-Grain Buildup Process,” Chemical Physics Letters, 485(1-3), 64-68, 2010. (DOI:10.1016/j.cplett.2009.11.064)\n\n\nW. Smith, S. Mao, G. H. Lu, A. Catlett, J. H. Chen, and Y. P. Zhao, “The Effect of Ag Nanoparticle Loading on the Photocatalytic Activity of TiO2 Nanorod Arrays,” Chemical Physics Letters, 485(1-3), 171-175, 2010. (DOI:10.1016/j.cplett.2009.12.041)\n\n\nS. Mao, G. H. Lu, K. H. Yu, and J. H. Chen, “Specific Biosensing Using Carbon Nanotubes Functionalized with Gold Nanoparticle–Antibody Conjugates,” Carbon, 48(2), 479-486, 2010. (DOI:10.1016/j.carbon.2009.09.065)\n\n\nS. Mao, G. H. Lu, K. H. Yu, and J. H. Chen, “Protein Viability on Au Nanoparticles during an Electrospray and Electrostatic-Force-Directed Assembly Process,” Journal of Nanomaterials, No. 196393, 2010. (DOI:10.1155/2010/196393)\n\n\nG. H. Lu, L. E. Ocola, and J. H. Chen, “Reduced Graphene Oxide for Room-Temperature Gas Sensors,” Nanotechnology, 20(44), 445502, 2009. (DOI:10.1088/0957-4484/20/44/445502)\n\n\nG. H. Lu, L. E. Ocola, and J. H. Chen, “Room-Temperature Gas Sensing Based on Electron Transfer between Discrete Tin Oxide Nanocrystals and Multiwalled Carbon Nanotubes,” Advanced Materials, 21(24), 2487-2491, 2009. (Frontispiece Featured) (DOI:10.1002 / adma.200803536)\n\n\nG. H. Lu, L. E. Ocola, and J. H. Chen, “Gas Detection Using Low-Temperature Reduced Graphene Oxide Sheets,” Applied Physics Letters, 94, 083111, 2009. (Selected for the March 16th, 2009 issue of Virtual Journal of Nanoscale Science & Technology) (DOI:10.1063/1.3086896)\n\n\nG. H. Lu, S. Mao, S. Park, R. S. Ruoff, and J. H. Chen, “Facile, Noncovalent Decoration of Graphene Oxide Sheets with Nanocrystals,” Nano Research, 2(3), 192-200, 2009. (DOI:10.1007/s12274-009-9017-8)\n\n\nS. Mao, G. H. Lu, and J. H. Chen, “Carbon-Nanotube-Assisted Transmission Electron Microscopy Characterization of Aerosol Nanoparticles,” Journal of Aerosol Science, 40(2), 180-184, 2009. (DOI:10.1016/j.jaerosci.2008.10.001)\n\n\nG. H. Lu, M. Liu, K. H. Yu, and J. H. Chen, “Synthesis and Absorption Properties of SnO2 Nanocrystal-Carbon Nanotube Hybrid Structures,” Journal of Electronic Materials, 37(11), 1686-1690, 2008. (DOI:10.1007/s11664-008-0500-x)\n\n\nS. Mao, G. H. Lu, and J. H. Chen, “Coating Carbon Nanotubes with Colloidal Nanocrystals by Combining an Electrospray Technique with Directed Assembly Using an Electrostatic Field,” Nanotechnology 19(45), No. 455610, 2008. (DOI:10.1088/0957-4484/19/45/455610)\n\n\nM. Liu, G. H. Lu, and J. H. Chen, “Synthesis, Assembly, and Characterization of Si Nanoparticles and Si Nanoparticle-Carbon Nanotube Hybrid Structures,” Nanotechnology, 19(26), No. 265705, 2008. (DOI:10.1088/0957-4484/19/26/265705)\n\n\nL. Y. Zhu, G. H. Lu, and J. H. Chen, “A Generic Approach to Coat Carbon Nanotubes with Nanoparticles for Potential Energy Applications,” Journal of Heat Transfer-Special Issue on Energy Nanotechnology, 130(4), No. 044502, 2008. (DOI:10.1115/1.2787026)\n\n\nM. Omari, N. Kouklin, G. H. Lu, J. H. Chen, and M. Gajdardziska-Josifovska, “Fabrication of Cd3As2 nanowires by direct vapor-solid growth and their infrared absorption properties,” Nanotechnology 19(10), 105301, 2008. (DOI:10.1088/0957-4484/19/10/105301)\n\n\nB. J. Hansen, G. H. Lu, and J. H. Chen, “Direct Oxidation Growth of CuO Nanowires from Copper-Containing Substrates,” Journal of Nanomaterials, No. 830474, 2008. (DOI:10.1155/2008/830474)\n\n\nG. H. Lu, L. Y. Zhu, P. X. Wang, J. H. Chen, D. A. Dikin, R. S. Ruoff, Y. Yu, and Z. F. Ren, “Electrostatic-Force-Directed Assembly of Ag Nanocrystals onto Vertically Aligned Carbon Nanotubes,” Journal of Physical Chemistry C 111(48), 17919-17922, 2007. (DOI:10.1021/jp071523x)\n\n\nL. Y. Zhu, G. H. Lu, S. Mao, J. H. Chen, D. A. Dikin, X. Q. Chen, and R. S. Ruoff, “Ripening of Silver Nanoparticles on Carbon Nanotubes,” NANO 2(3), 149-156, 2007. (Cover featured) (DOI:10.1142/S1793292007000507)\n\n\nJ. H. Chen, G. H. Lu, L. Y. Zhu and R. C. Flagan, “A Simple and Versatile Mini-Arc Plasma Source for Nanocrystal Synthesis,” Journal of Nanoparticle Research 9(2), 203-213, 2007. (DOI:10.1007/s11051-006-9168-4)\n\n\nG. H. Lu, L. Kyle, L. E. Ocola, M. Gajdardziska-Josifovska, and J. H. Chen, “Gas Sensors Based on Tin Oxide Nanoparticles Synthesized from a Mini-Arc Plasma Source,” Journal of Nanomaterials, No. 60828, 2006. (DOI:10.1155/JNM/2006/60828)\n\n\nJ. H. Chen and G. H. Lu, “Controlled Decoration of Carbon Nanotubes with Nanoparticles,” Nanotechnology 17(12), 2891-2894, 2006. (Highlighted by Nanowerk Spotlight) (DOI:10.1088/0957-4484/17/12/011)\n\n\nZ. G. Yang, Q. X. Mao, G. B. Ma and G. H. Lu, “Characteristic Analysis of Substitute Refrigerant R134a Working under High-temperature Environment,” Journal of Building Energy and Environment 22(4), 14-16, 2003. (in Chinese with English Abstract)\n\n\nG. H. Lu, Q. X. Mao, L. R. Wu, and G. B. Ma, “Research on Traveling Crane Air Conditioning Systems’ Current Situations in the Steel-making Workshops of an Iron and Steel Works,” Journal of Building Energy and Environment 21(6), 12-14, 2002. (in Chinese with English Abstract)\n\n\nH. Liu, Q. X. Mao, J. H. Huang, G. B. Ma and G. H. Lu, “In-situ Test and Evaluation of AC Renovation of Control Center in a Steel Works,” Journal of Building Energy and Environment21(2), 59-62, 2002. (in Chinese with English Abstract)\n\n\n(Back to top)"
  },
  {
    "objectID": "publications.html#conference-presentationsposters",
    "href": "publications.html#conference-presentationsposters",
    "title": "Pubs",
    "section": "Conference Presentations/Posters",
    "text": "Conference Presentations/Posters\n(* presenter)\n\n\nG. H. Lu*, S. Park, K. H. Yu, R. S. Ruoff, L. E. Ocola, D. Rosenmann, and J. H. Chen, “Toward Practical Gas Sensors Using Highly Reduced Graphene Oxide,” Presented at the 2010 MRS Fall Meeting, Boston, MA, November 29-December 3, 2010. (Poster)\n\n\nS. Mao*, G. H. Lu, K. H. Yu, Z. Bo, and J. H. Chen, “Protein Detection using Thermally-Reduced Graphene Oxide and Gold Nanoparticle-antibody Composites,” Presented at the 2010 MRS Fall Meeting, Boston, MA, November 29-December 3, 2010. (Poster)\n\n\nG. H. Lu* J. H. Chen, and L. E. Ocola, ” Gas Sensors Based on Thermally Reduced Graphene Oxide,” Presented at the 2009 MRS Fall Meeting, Boston, MA, November 30-December 4, 2009. (Oral)\n\n\nS. Mao*, G. H. Lu, K. H. Yu, and J. H. Chen, ” Detection of Protein via Gold Nanoparticle-Antigen Conjugates in Carbon Nanotube Field Effect Transistors,” Presented at the 2009 MRS Fall Meeting, Boston, MA, November 30-December 4, 2009. (Poster)\n\n\nG. H. Lu*, J. H. Chen, and L. E. Ocola, “Low-Temperature Reduced Graphene Oxide for Gas Detection Applications,” Presented at the 2009 ASME International Conference & Exhibition, Lake Buena Vista, Florida, November 13-19, 2009. (Oral)\n\n\nK. H. Yu*, G. H. Lu, and J. H. Chen, “Photodetectors Based on Hybrid Nanostructures of CdSe Quantum Dots Supported on Multiwalled Carbon Nanotubes,” Presented at the 2008 ASME International Conference & Exhibition, Lake Buena Vista, Florida, November 13-19, 2009. (Oral)\n\n\nS. Mao, G. H. Lu*, and J. H. Chen, “Decoration of Carbon Nanotubes with Electrosprayed Nanocrystals via Electrostatic Force Directed Assembly”, Presented at the 27th AAAR Conference, Orlando, FL, October 20-24, 2008. (Poster)\n\n\nS. Mao, G. H. Lu*, and J. H. Chen, “Carbon-Nanotube-Assisted Transmission Electron Microscopy Characterization of Aerosol Nanoparticles”, Presented at the 27th AAAR Conference, Orlando, FL, October 20-24, 2008. (Poster)\n\n\nG. H. Lu, L. E. Ocola, and J. H. Chen*, “Novel Gas Sensors Fabricated Using Tin Oxide Nanocrystal-Carbon Nanotube Hybrid Structures,” Presented at the 2008 ASME International Conference on Integration and Commercialization of Micro and Nanosystems, Clear Water Bay, Kowloon, Hongkong, China, June 3-5, 2008.\n\n\nG. H. Lu*, J. H. Chen, and L. E. Ocola, “Miniaturized Room-temperature Gas Sensors with Hybrid Nanostructures of SnO2 Nanocrystals Decorated on Multiwalled Carbon Nanotubes”, Presented at the 2008 NSTI Nanotechnology Conference and Trade Show, Boston, MA, June 1-5, 2008. (Oral)\n\n\nG. H. Lu, L. Y. Zhu, P. X. Wang, J. H. Chen*, D. A. Dikin, R. S. Ruoff, Y. Yu, and Z. F. Ren, “Dry Assembly of Nanocrystals onto Vertically Aligned Carbon Nanotubes,” Presented at the 2007 ASME International Mechanical Engineering Congress and RD&D Expo (IMECE), Seattle, WA, November 11-15, 2007.\n\n\nG. H. Lu* and J. H. Chen, “Synthesis and Characterization of Doped Tin Oxide Nanocrystals for Gas Sensing Applications,” Presented at the 26th AAAR Conference, Reno, NV, September 24-28, 2007. (Poster)\n\n\nG. H. Lu*, S. M. Hebert, L. E. Ocola, and J. H. Chen, “Room Temperature Gas Sensing with SnO2 Nanocrystals Supported on MWCNTs”, Presented at University of Wisconsin-Milwaukee Laboratory for Surface Studies Summer Symposium 2007, Milwaukee, WI, August 28, 2007. (Oral, Best Oral Presentation Award)\n\n\nG. H. Lu, L. Y. Zhu, S. Hebert, E. Jen, L. Ocola, and J. H. Chen*, “Engineering Gas Sensors with Aerosol Nanocrystals,” Proceedings of 2007 ASME International Conference on Integration and Commercialization of Micro and Nanosystems, Sanya, Hainan, China, January 10-13, 2007.\n\n\nJ. H. Chen*, G. H. Lu, L. Y. Zhu, and C. Hirschmugl, “Controlled Assembly of Nanoparticles onto Carbon Nanotubes,” Abstracts of 2006 MRS Fall Meeting, Boston, MA, November 27-December 1, 2006.\n\n\nG. H. Lu, L. Y. Zhu, S. Hebert, L. E. Ocola and J. H. Chen*, “Novel Gas Sensors Based on Hybrid Nanostructures,” Abstracts of 2006 MRS Fall Meeting, Boston, MA, November 27-December 1, 2006.\n\n\nG. H. Lu*, K. L. Huebner, E. Jen, L. E. Ocola and J. H. Chen, “Miniaturized Gas Sensors Based on Pure and Doped Tin Oxide Nanocrystals,”Abstract accepted to ASME International Mechanical Engineering Congress and RD&D Expo (IMECE), Chicago, IL, November 5-10, 2006. (Oral)\n\n\nG. H. Lu* and J. H. Chen, “Coating Carbon Nanotubes with Aerosol Nanoparticles,” Presented at the 7th International Aerosol Conference, St. Paul, MN, September 10-15, 2006. (Poster)\n\n\nG. H. Lu*, L. Y. Zhu, C. Hirschmugl and J. H. Chen, “Controlled Assembly of Nanoparticles onto Carbon Nanotubes”, Presented at University of Wisconsin-Milwaukee Laboratory for Surface Studies Summer Symposium, Milwaukee, WI, August 18, 2006. (Oral)\n\n\nJ. H. Chen*, G. H. Lu, L. Y. Zhu, and C. J. Hirschmugl, “A Generic Approach to Coat Carbon Nanotubes with Nanoparticles for Energy Applications,” Presented at the ASME Energy Nanotechnology International Conference, Cambridge, MA, Jun. 26-28, 2006. (Oral)\n\n\nL. Y. Zhu, G. H. Lu, J. H. Chen, and C. J. Hirschmugl*, “Assembly of Silver Nanoparticles onto Multi-walled Carbon Nanotubes and Study of Nanoparticle Attachment Mechanism,” Presented at the 66th Annual Physical Electronics Conference, Princeton, NJ, June 18-21, 2006. (Oral)\n\n\nG. H. Lu*, K. L. Huebner, L. E. Ocola, M. Gajdardziska-Josifovska, and J. H. Chen, “Fabrication and Characterization of an Ultrasensitive Tin Oxide Nanoparticle Sensor,” Presented at Argonne National Laboratory Users Meetings, Argonne, IL, May 1-5, 2006. (Poster, Best Student Poster Award)\n\n\nG. H. Lu*, K. L. Huebner, L. E. Ocola, M. Gajdardziska-Josifovska, and J. H. Chen, “Ultrasensitive Nanoparticle Gas Sensors,” Presented at the 30th Annual Great Lakes Biomedical Conference Biomedical Sensors, Racine, WI, April 21, 2006. (Poser)\n\n\nG. H. Lu*, J. H. Chen and M. Gajdardziska-Josifovska, “Synthesis of Tin Oxide Nanoparticles Using a Mini-arc Plasma Source,” Presented at University of Wisconsin-Milwaukee Laboratory for Surface Studies Summer Symposium, Milwaukee, WI, August 19, 2005. (Poster)\n\n\nG. H. Lu*, J. H. Chen, and M. Gajdardziska-Josifovska , “Synthesis of Tin Oxide Nanoparticles Using a Mini-Arc Plasma Source,” Presented at 2005 Materials Research Society (MRS) Spring Meeting, San Francisco, CA, March 28-April 1, 2005. (Poster, Paper)\n\n\nJ. H. Chen*, G. H. Lu, E. Abu-Zahra, and M. Gajdardziska-Josifovska, “Arc Plasma Synthesis of Tin Oxide Nanoparticles,” Presented at 2004 ASME International Mechanical Engineering Congress and RD&D Expo (IMECE), Anaheim, CA, November 13-19, 2004. (Oral)\n\n\nG. H. Lu*, J. H. Chen and M. Gajdardziska-Josifovska, “Synthesis of Tin Oxide Nanoparticles for Fabrication of Functional Devices,” Presented at University of Wisconsin-Milwaukee Keulks Symposium on Nanotechnology, Milwaukee, WI, October 15, 2004. (Poster)\n\n\nJ. H. Chen*, G. H. Lu, and M. Gajdardziska-Josifovska, “Synthesis of Tin Oxide Nanoparticles Using a Commercial Arc Welder,” Presented at the 23rd American Association for Aerosol Research (AAAR) Annual Conference, Atlanta, GA, October 4-8, 2004. (Poster)\n\n\nJ. H. Chen*, G. H. Lu, E. Abu-Zahra and R. C. Flagan, “A Simple Arc Plasma Source for Nanoparticle Synthesis,” Presented at the 2004 Gordon Research Conference on Plasma Processing Science, Plymouth, NH, August 15-20, 2004. (Poster)\n\n\nG. H. Lu*, E. Abu-Zahra, J. H. Chen and M. Gajdardziska-Josifovska, “Arc Plasma Synthesis of Tin Oxide Nanoparticles,” Presented at University of Wisconsin-Milwaukee Laboratory for Surface Studies Summer Graduate Student Symposium, Milwaukee, WI, August 10, 2004. (Poster)\n\n\n(Back to top)"
  }
]